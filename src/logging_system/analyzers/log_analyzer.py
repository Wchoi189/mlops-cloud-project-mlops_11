"""
ë¡œê·¸ ë¶„ì„ ë„êµ¬
ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ íŒ¨í„´, ì˜¤ë¥˜, ì„±ëŠ¥ ì´ìŠˆë¥¼ ìë™ìœ¼ë¡œ íƒì§€
"""

import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict, Counter
import numpy as np

class LogAnalyzer:
    """ë¡œê·¸ íŒŒì¼ ë¶„ì„ê¸°"""
    
    def __init__(self, log_dir: str = 'logs'):
        self.log_dir = Path(log_dir)
        self.error_patterns = [
            (r'ERROR.*?ConnectionError', 'connection_error'),
            (r'ERROR.*?TimeoutError', 'timeout_error'),
            (r'ERROR.*?HTTP 404', 'not_found_error'),
            (r'ERROR.*?HTTP 500', 'server_error'),
            (r'ERROR.*?ValidationError', 'validation_error'),
            (r'CRITICAL', 'critical_error'),
            (r'API.*?failed', 'api_failure'),
            (r'Database.*?error', 'database_error')
        ]
        
        self.performance_thresholds = {
            'api_call': 5.0,  # 5ì´ˆ
            'data_processing': 30.0,  # 30ì´ˆ
            'database_operation': 2.0,  # 2ì´ˆ
            'file_operation': 1.0,  # 1ì´ˆ
            'validation': 10.0  # 10ì´ˆ
        }
    
    def analyze_error_logs(self, hours: int = 24) -> Dict[str, Any]:
        """ì—ëŸ¬ ë¡œê·¸ ë¶„ì„"""
        since = datetime.now() - timedelta(hours=hours)
        error_files = list(self.log_dir.glob('error/*.log')) + list(self.log_dir.glob('app/*.log'))
        
        error_analysis = {
            'analysis_period': f'Last {hours} hours',
            'analysis_time': datetime.now().isoformat(),
            'total_errors': 0,
            'error_types': {},
            'error_timeline': [],
            'frequent_errors': [],
            'critical_errors': [],
            'error_trend': 'stable'
        }
        
        all_errors = []
        
        for log_file in error_files:
            try:
                if not log_file.exists():
                    continue
                    
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        # ì‹œê°„ íŒŒì‹±
                        timestamp_match = re.search(r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', line)
                        if timestamp_match:
                            try:
                                log_time = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                                if log_time < since:
                                    continue
                            except ValueError:
                                continue
                        
                        # ì—ëŸ¬ íŒ¨í„´ ë§¤ì¹­
                        if 'ERROR' in line or 'CRITICAL' in line:
                            error_type = 'unknown_error'
                            for pattern, etype in self.error_patterns:
                                if re.search(pattern, line, re.IGNORECASE):
                                    error_type = etype
                                    break
                            
                            error_info = {
                                'file': str(log_file),
                                'line_num': line_num,
                                'timestamp': log_time.isoformat() if timestamp_match else None,
                                'type': error_type,
                                'message': line.strip(),
                                'severity': 'CRITICAL' if 'CRITICAL' in line else 'ERROR'
                            }
                            all_errors.append(error_info)
            
            except Exception as e:
                print(f"Error reading {log_file}: {e}")
                continue
        
        # ë¶„ì„ ê²°ê³¼ ì§‘ê³„
        error_analysis['total_errors'] = len(all_errors)
        
        # ì—ëŸ¬ ìœ í˜•ë³„ ì§‘ê³„
        error_type_counts = Counter(error['type'] for error in all_errors)
        error_analysis['error_types'] = dict(error_type_counts.most_common())
        
        # ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë¶„í¬
        hourly_errors = defaultdict(int)
        for error in all_errors:
            if error['timestamp']:
                hour = datetime.fromisoformat(error['timestamp']).strftime('%Y-%m-%d %H:00')
                hourly_errors[hour] += 1
        
        error_analysis['error_timeline'] = [
            {'hour': hour, 'count': count} 
            for hour, count in sorted(hourly_errors.items())
        ]
        
        # ë¹ˆë²ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€
        message_counts = Counter(error['message'][:100] for error in all_errors)
        error_analysis['frequent_errors'] = [
            {'message': msg, 'count': count}
            for msg, count in message_counts.most_common(10)
        ]
        
        # ì‹¬ê°í•œ ì—ëŸ¬
        critical_errors = [error for error in all_errors if error['severity'] == 'CRITICAL']
        error_analysis['critical_errors'] = critical_errors[-10:]  # ìµœê·¼ 10ê°œ
        
        # ì—ëŸ¬ íŠ¸ë Œë“œ ë¶„ì„
        if len(error_analysis['error_timeline']) >= 2:
            recent_errors = sum(item['count'] for item in error_analysis['error_timeline'][-6:])  # ìµœê·¼ 6ì‹œê°„
            earlier_errors = sum(item['count'] for item in error_analysis['error_timeline'][-12:-6])  # ì´ì „ 6ì‹œê°„
            
            if recent_errors > earlier_errors * 1.5:
                error_analysis['error_trend'] = 'increasing'
            elif recent_errors < earlier_errors * 0.5:
                error_analysis['error_trend'] = 'decreasing'
        
        return error_analysis
    
    def analyze_performance_logs(self, hours: int = 24) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„"""
        since = datetime.now() - timedelta(hours=hours)
        perf_files = list(self.log_dir.glob('performance/*.log')) + list(self.log_dir.glob('app/*.log'))
        
        performance_analysis = {
            'analysis_period': f'Last {hours} hours',
            'analysis_time': datetime.now().isoformat(),
            'total_operations': 0,
            'slow_operations': [],
            'component_stats': {},
            'performance_trend': 'stable'
        }
        
        all_operations = []
        
        for log_file in perf_files:
            try:
                if not log_file.exists():
                    continue
                    
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        # ì„±ëŠ¥ ë¡œê·¸ íŒŒì‹± (ë‹¤ì–‘í•œ íŒ¨í„´ ì§€ì›)
                        patterns = [
                            r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\] PERF ([\w\.]+)\s+\| (\w+) completed in ([\d\.]+)s',
                            r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\].*?(\w+) completed in ([\d\.]+)s',
                            r'(\w+) took ([\d\.]+) seconds'
                        ]
                        
                        for pattern in patterns:
                            perf_match = re.search(pattern, line)
                            if perf_match:
                                if len(perf_match.groups()) == 4:
                                    timestamp_str, component, operation, duration_str = perf_match.groups()
                                    try:
                                        log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                                        if log_time < since:
                                            continue
                                    except ValueError:
                                        continue
                                elif len(perf_match.groups()) == 3:
                                    timestamp_str, operation, duration_str = perf_match.groups()
                                    component = 'unknown'
                                    try:
                                        log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                                        if log_time < since:
                                            continue
                                    except ValueError:
                                        continue
                                elif len(perf_match.groups()) == 2:
                                    operation, duration_str = perf_match.groups()
                                    component = 'unknown'
                                    log_time = datetime.now()
                                else:
                                    continue
                                
                                try:
                                    duration = float(duration_str)
                                except ValueError:
                                    continue
                                
                                op_info = {
                                    'timestamp': log_time.isoformat(),
                                    'component': component,
                                    'operation': operation,
                                    'duration': duration
                                }
                                all_operations.append(op_info)
                                break
            
            except Exception as e:
                print(f"Error reading {log_file}: {e}")
                continue
        
        performance_analysis['total_operations'] = len(all_operations)
        
        # ëŠë¦° ì‘ì—… íƒì§€
        slow_ops = []
        for op in all_operations:
            operation_type = self._categorize_operation(op['operation'])
            threshold = self.performance_thresholds.get(operation_type, 10.0)
            
            if op['duration'] > threshold:
                slow_ops.append({
                    **op,
                    'threshold': threshold,
                    'slowness_factor': op['duration'] / threshold
                })
        
        performance_analysis['slow_operations'] = sorted(
            slow_ops, key=lambda x: x['slowness_factor'], reverse=True
        )[:20]
        
        # ì»´í¬ë„ŒíŠ¸ë³„ í†µê³„
        component_stats = defaultdict(lambda: {
            'count': 0, 'total_time': 0, 'avg_time': 0, 
            'max_time': 0, 'min_time': float('inf')
        })
        
        for op in all_operations:
            comp_stats = component_stats[op['component']]
            comp_stats['count'] += 1
            comp_stats['total_time'] += op['duration']
            comp_stats['max_time'] = max(comp_stats['max_time'], op['duration'])
            comp_stats['min_time'] = min(comp_stats['min_time'], op['duration'])
        
        for comp, stats in component_stats.items():
            if stats['count'] > 0:
                stats['avg_time'] = stats['total_time'] / stats['count']
                if stats['min_time'] == float('inf'):
                    stats['min_time'] = 0
        
        performance_analysis['component_stats'] = dict(component_stats)
        
        return performance_analysis
    
    def _categorize_operation(self, operation: str) -> str:
        """ì‘ì—… ìœ í˜• ë¶„ë¥˜"""
        operation_lower = operation.lower()
        
        if any(term in operation_lower for term in ['api', 'request', 'call', 'fetch']):
            return 'api_call'
        elif any(term in operation_lower for term in ['process', 'transform', 'clean', 'parse']):
            return 'data_processing'
        elif any(term in operation_lower for term in ['select', 'insert', 'update', 'delete', 'query']):
            return 'database_operation'
        elif any(term in operation_lower for term in ['read', 'write', 'save', 'load', 'file']):
            return 'file_operation'
        elif any(term in operation_lower for term in ['validate', 'check', 'verify']):
            return 'validation'
        else:
            return 'other'
    
    def generate_health_report(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±"""
        error_analysis = self.analyze_error_logs(24)
        perf_analysis = self.analyze_performance_logs(24)
        
        # ê±´ê°• ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
        health_score = 100
        issues = []
        recommendations = []
        
        # ì—ëŸ¬ ì ìˆ˜ (ìµœëŒ€ 40ì  ê°ì )
        error_rate = error_analysis['total_errors']
        if error_rate > 100:
            health_score -= 40
            issues.append('high_error_rate')
            recommendations.append("ì—ëŸ¬ ë°œìƒë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ì ê²€í•˜ê³  ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.")
        elif error_rate > 50:
            health_score -= 25
            issues.append('moderate_error_rate')
            recommendations.append("ì—ëŸ¬ ë°œìƒë¥ ì´ ì¦ê°€í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”.")
        elif error_rate > 20:
            health_score -= 10
            issues.append('some_errors')
        
        # ì‹¬ê°í•œ ì—ëŸ¬ (ìµœëŒ€ 30ì  ê°ì )
        critical_count = len(error_analysis['critical_errors'])
        if critical_count > 5:
            health_score -= 30
            issues.append('critical_errors')
            recommendations.append("ì‹¬ê°í•œ ì—ëŸ¬ê°€ ë‹¤ìˆ˜ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        elif critical_count > 0:
            health_score -= 15
            issues.append('some_critical_errors')
            recommendations.append("ì‹¬ê°í•œ ì—ëŸ¬ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì„±ëŠ¥ ì ìˆ˜ (ìµœëŒ€ 30ì  ê°ì )
        slow_ops = len(perf_analysis['slow_operations'])
        if slow_ops > 20:
            health_score -= 30
            issues.append('performance_issues')
            recommendations.append("ì„±ëŠ¥ ì €í•˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ëŠë¦° ì‘ì—…ë“¤ì„ ìµœì í™”í•˜ì„¸ìš”.")
        elif slow_ops > 10:
            health_score -= 15
            issues.append('some_slow_operations')
            recommendations.append("ì¼ë¶€ ì‘ì—…ì˜ ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # íŠ¸ë Œë“œ ë¶„ì„
        if error_analysis['error_trend'] == 'increasing':
            health_score -= 10
            issues.append('error_trend_increasing')
            recommendations.append("ì—ëŸ¬ ë°œìƒë¥ ì´ ì¦ê°€í•˜ëŠ” ì¶”ì„¸ì…ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ì„ ê°•í™”í•˜ì„¸ìš”.")
        
        # ê±´ê°• ë“±ê¸‰
        if health_score >= 90:
            grade = "ğŸŸ¢ Excellent"
        elif health_score >= 80:
            grade = "ğŸŸ¡ Good"
        elif health_score >= 70:
            grade = "ğŸŸ  Fair"
        elif health_score >= 60:
            grade = "ğŸŸ  Poor"
        else:
            grade = "ğŸ”´ Critical"
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.")
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬ ìœ í˜• ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if error_analysis['error_types']:
            top_error = list(error_analysis['error_types'].keys())[0]
            if top_error == 'connection_error':
                recommendations.append("ì—°ê²° ì˜¤ë¥˜ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœì™€ ì™¸ë¶€ ì„œë¹„ìŠ¤ë¥¼ ì ê²€í•˜ì„¸ìš”.")
            elif top_error == 'timeout_error':
                recommendations.append("íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ê°€ ë§ìŠµë‹ˆë‹¤. ìš”ì²­ ì‹œê°„ ì œí•œì„ ì¡°ì •í•˜ê±°ë‚˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ì„¸ìš”.")
            elif top_error == 'validation_error':
                recommendations.append("ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜ê°€ ë§ìŠµë‹ˆë‹¤. ì…ë ¥ ë°ì´í„° í’ˆì§ˆì„ í™•ì¸í•˜ì„¸ìš”.")
        
        return {
            'timestamp': datetime.now().isoformat(),
            'health_score': max(0, health_score),
            'grade': grade,
            'issues': issues,
            'recommendations': recommendations,
            'error_summary': {
                'total_errors': error_analysis['total_errors'],
                'critical_errors': critical_count,
                'top_error_types': list(error_analysis['error_types'].keys())[:3],
                'error_trend': error_analysis['error_trend']
            },
            'performance_summary': {
                'total_operations': perf_analysis['total_operations'],
                'slow_operations': slow_ops,
                'slowest_component': max(
                    perf_analysis['component_stats'].items(),
                    key=lambda x: x[1]['avg_time'],
                    default=('none', {'avg_time': 0})
                )[0]
            }
        }
    
    def analyze_api_usage(self, hours: int = 24) -> Dict[str, Any]:
        """API ì‚¬ìš© íŒ¨í„´ ë¶„ì„"""
        since = datetime.now() - timedelta(hours=hours)
        api_files = list(self.log_dir.glob('app/api_calls.log')) + list(self.log_dir.glob('app/*.log'))
        
        api_analysis = {
            'analysis_period': f'Last {hours} hours',
            'total_api_calls': 0,
            'successful_calls': 0,
            'failed_calls': 0,
            'api_endpoints': {},
            'response_times': [],
            'error_rates': {}
        }
        
        for log_file in api_files:
            try:
                if not log_file.exists():
                    continue
                    
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        # API í˜¸ì¶œ íŒ¨í„´ ë§¤ì¹­
                        api_match = re.search(
                            r'\[(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})\].*?API Call (\w+) (succeeded|failed).*?in ([\d\.]+)s',
                            line
                        )
                        
                        if api_match:
                            timestamp_str, endpoint, status, duration_str = api_match.groups()
                            try:
                                log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                                if log_time < since:
                                    continue
                            except ValueError:
                                continue
                            
                            api_analysis['total_api_calls'] += 1
                            
                            if status == 'succeeded':
                                api_analysis['successful_calls'] += 1
                            else:
                                api_analysis['failed_calls'] += 1
                            
                            # ì—”ë“œí¬ì¸íŠ¸ë³„ í†µê³„
                            if endpoint not in api_analysis['api_endpoints']:
                                api_analysis['api_endpoints'][endpoint] = {
                                    'total': 0, 'success': 0, 'failed': 0, 'avg_response_time': 0
                                }
                            
                            api_analysis['api_endpoints'][endpoint]['total'] += 1
                            if status == 'succeeded':
                                api_analysis['api_endpoints'][endpoint]['success'] += 1
                            else:
                                api_analysis['api_endpoints'][endpoint]['failed'] += 1
                            
                            try:
                                duration = float(duration_str)
                                api_analysis['response_times'].append(duration)
                            except ValueError:
                                pass
            
            except Exception as e:
                print(f"Error reading {log_file}: {e}")
                continue
        
        # í†µê³„ ê³„ì‚°
        if api_analysis['total_api_calls'] > 0:
            api_analysis['success_rate'] = (api_analysis['successful_calls'] / api_analysis['total_api_calls']) * 100
        else:
            api_analysis['success_rate'] = 0
        
        if api_analysis['response_times']:
            api_analysis['avg_response_time'] = np.mean(api_analysis['response_times'])
            api_analysis['median_response_time'] = np.median(api_analysis['response_times'])
            api_analysis['p95_response_time'] = np.percentile(api_analysis['response_times'], 95)
        
        return api_analysis

def generate_daily_log_report():
    """ì¼ì¼ ë¡œê·¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    analyzer = LogAnalyzer()
    
    print("ë¡œê·¸ ë¶„ì„ ì¤‘...")
    
    # ê±´ê°• ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±
    health_report = analyzer.generate_health_report()
    
    # ìƒì„¸ ë¶„ì„
    error_analysis = analyzer.analyze_error_logs(24)
    perf_analysis = analyzer.analyze_performance_logs(24)
    api_analysis = analyzer.analyze_api_usage(24)
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    report_dir = Path('logs/reports')
    report_dir.mkdir(exist_ok=True, parents=True)
    
    date_str = datetime.now().strftime('%Y%m%d')
    report_file = report_dir / f'daily_log_report_{date_str}.json'
    
    full_report = {
        'report_date': date_str,
        'generation_time': datetime.now().isoformat(),
        'health_report': health_report,
        'error_analysis': error_analysis,
        'performance_analysis': perf_analysis,
        'api_analysis': api_analysis
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(full_report, f, ensure_ascii=False, indent=2, default=str)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*60)
    print(f"ğŸ“Š ì¼ì¼ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ({date_str})")
    print("="*60)
    print(f"ğŸ¥ ì‹œìŠ¤í…œ ê±´ê°•ë„: {health_report['grade']} ({health_report['health_score']}/100)")
    print(f"âŒ ì´ ì—ëŸ¬ ìˆ˜: {error_analysis['total_errors']}")
    print(f"ğŸš¨ ì‹¬ê°í•œ ì—ëŸ¬: {len(error_analysis['critical_errors'])}")
    print(f"âš¡ ì´ ì‘ì—… ìˆ˜: {perf_analysis['total_operations']}")
    print(f"ğŸŒ ëŠë¦° ì‘ì—… ìˆ˜: {len(perf_analysis['slow_operations'])}")
    print(f"ğŸŒ API í˜¸ì¶œ ìˆ˜: {api_analysis['total_api_calls']}")
    print(f"âœ… API ì„±ê³µë¥ : {api_analysis.get('success_rate', 0):.1f}%")
    
    print(f"\nğŸ“‹ ì£¼ìš” ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(health_report['recommendations'][:3], 1):
        print(f"  {i}. {rec}")
    
    print(f"\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
    print("="*60)
    
    return full_report

if __name__ == "__main__":
    generate_daily_log_report()
