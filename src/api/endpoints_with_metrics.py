"""
Enhanced API endpoints with Prometheus metrics integration
Monitoring-ready endpoints for MLOps pipeline
"""


import logging
import time
from datetime import datetime
from functools import wraps
from typing import Any, Callable, Dict, Optional

import numpy as np
import pandas as pd
from fastapi import APIRouter, Depends, HTTPException

# Monitoring imports
try:
    from ..monitoring.metrics import metrics as mlops_metrics
    from ..monitoring.metrics import track_api_call, track_prediction_time

    HAS_MONITORING = True
except ImportError:
    HAS_MONITORING = False

    def track_prediction_time(
        model_name: str = "imdb_model", model_version: str = "1.0"
    ):
        def decorator(func: Callable) -> Callable:
            return func  # No-op decorator

        return decorator

    def track_api_call(endpoint: str, method: str = "GET"):
        def decorator(func: Callable) -> Callable:
            return func  # No-op decorator

        return decorator


from ..models.evaluator import ModelEvaluator
from .schemas import (
    BatchPredictionRequest,
    BatchPredictionResponse,
    HealthResponse,
    ModelInfo,
    PredictionRequest,
    PredictionResponse,
)

# Î°úÍπÖ ÏÑ§Ï†ï
logger = logging.getLogger(__name__)

# ÎùºÏö∞ÌÑ∞ ÏÉùÏÑ±
router = APIRouter()

# Ï†ÑÏó≠ Î™®Îç∏ ÌèâÍ∞ÄÍ∏∞ (Ïï± ÏãúÏûëÏãú Î°úÎìúÎê®)

model_evaluator: Optional[ModelEvaluator] = None


def get_model_evaluator() -> Optional[ModelEvaluator]:
    """
    Î™®Îç∏ ÌèâÍ∞ÄÍ∏∞ ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ (Graceful handling)
    CI/CD ÌôòÍ≤ΩÏóêÏÑú Î™®Îç∏Ïù¥ ÏóÜÏùÑ ÎïåÎèÑ ÎèôÏûëÌïòÎèÑÎ°ù Í∞úÏÑ†
    """
    global model_evaluator
    
    if model_evaluator is None:
        logger.warning("Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. CI/CD ÌôòÍ≤ΩÏù¥Í±∞ÎÇò Î™®Îç∏ ÌååÏùºÏù¥ ÏóÜÏùÑ Ïàò ÏûàÏäµÎãàÎã§.")
        return None
    
    return model_evaluator

def require_model_evaluator() -> ModelEvaluator:
    """
    Î™®Îç∏Ïù¥ ÌïÑÏàòÏù∏ ÏóîÎìúÌè¨Ïù∏Ìä∏Ïö© ÏùòÏ°¥ÏÑ± Ï£ºÏûÖ
    Î™®Îç∏Ïù¥ ÏóÜÏúºÎ©¥ 503 ÏóêÎü¨ Î∞òÌôò
    """
    evaluator = get_model_evaluator()
    if evaluator is None:
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Service Temporarily Unavailable",
                "message": "Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§",
                "details": "ÏÑúÎ≤ÑÎ•º Îã§Ïãú ÏãúÏûëÌïòÍ±∞ÎÇò Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï¥Ï£ºÏÑ∏Ïöî",
                "suggestions": [
                    "python scripts/train_model.py Ïã§Ìñâ",
                    "models/ ÎîîÎ†âÌÜ†Î¶¨Ïóê Î™®Îç∏ ÌååÏùº ÌôïÏù∏",
                    "ÏÑúÎ≤Ñ Ïû¨ÏãúÏûë ÏãúÎèÑ"
                ],
                "model_required": True,
                "status": "no_model_loaded"
            }
        )
    return evaluator

@router.post("/predict", response_model=PredictionResponse)
@track_prediction_time(model_name="imdb_model", model_version="1.0")
async def predict_movie_rating(request: PredictionRequest):
    """
    Îã®Ïùº ÏòÅÌôî ÌèâÏ†ê ÏòàÏ∏° (Legacy endpoint with monitoring)
    """
    start_time = time.time()

    try:
        logger.info(f"ÌèâÏ†ê ÏòàÏ∏° ÏöîÏ≤≠: {(request.text[:50] + '...') if request.text else 'No text provided'}")

        # Record API call if monitoring enabled
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict", status_code="200"
            ).inc()

        # ÏöîÏ≤≠ÏóêÏÑú ÏòÅÌôî Ï†ïÎ≥¥ Ï∂îÏ∂ú
        movie_data = {
            "startYear": getattr(request, "startYear", 2020),
            "runtimeMinutes": getattr(request, "runtimeMinutes", 120),
            "numVotes": getattr(request, "numVotes", 5000),
        }

        # Î™®Îç∏ ÌèâÍ∞ÄÍ∏∞ ÌôïÏù∏ (graceful handling)
        evaluator = get_model_evaluator()

        if evaluator is None:
            # Î™®Îç∏Ïù¥ ÏóÜÏùÑ Îïå fallback ÏòàÏ∏° + monitoring
            logger.warning("Î™®Îç∏ ÏóÜÏùå - fallback ÏòàÏ∏° Ï†úÍ≥µ (with monitoring)")

            # Record fallback usage
            if HAS_MONITORING:
                mlops_metrics.model_predictions_total.labels(
                    model_name="fallback_model", 
                    model_version="1.0", 
                    prediction_type="heuristic_fallback"
                ).inc()
            # Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã± Í∏∞Î∞ò ÏòàÏ∏°
            base_rating = 6.0  # Í∏∞Î≥∏ ÌèâÏ†ê
            
            # Ïó∞ÎèÑ Î≥¥Ï†ï
            year = movie_data.get("startYear", 2020)
            if year > 2010:
                base_rating += 0.3
            if year > 2020:
                base_rating += 0.1
                
            # Îü∞ÌÉÄÏûÑ Î≥¥Ï†ï
            runtime = movie_data.get("runtimeMinutes", 120)
            if 90 <= runtime <= 150:
                base_rating += 0.2
            elif runtime > 200:
                base_rating -= 0.1
                
            # Ìà¨ÌëúÏàò Î≥¥Ï†ï
            votes = movie_data.get("numVotes", 5000)
            if votes > 10000:
                base_rating += 0.2
            if votes > 50000:
                base_rating += 0.1

            # Î≤îÏúÑ Ï†úÌïú
            predicted_rating = min(max(base_rating, 1.0), 10.0)
            
            # Record fallback prediction metrics
            if HAS_MONITORING:
                mlops_metrics.record_prediction_rating(predicted_rating)
                # Record data validation (fallback mode)
                mlops_metrics.data_validation_errors_total.labels(
                    validation_type="model_unavailable", error_type="fallback_used"
                ).inc()
            # Í∞êÏ†ï Î∂ÑÎ•ò (ÌèâÏ†ê Í∏∞Î∞ò)
            sentiment = "positive" if predicted_rating >= 6.0 else "negative"
            confidence = 0.5  # ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ (fallbackÏù¥ÎØÄÎ°ú)

            # Record response time
            response_time = time.time() - start_time
            if HAS_MONITORING:
                mlops_metrics.http_request_duration_seconds.labels(
                    method="POST", endpoint="/predict"
                ).observe(response_time)

            logger.info(f"Fallback ÏòàÏ∏° ÏôÑÎ£å: {predicted_rating:.2f}/10 (ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s)")

            return PredictionResponse(
                text=request.text,
                sentiment=sentiment,
                confidence=confidence,
                timestamp=datetime.now().isoformat(),
                # Enhanced fields for graceful degradation
                predicted_rating=round(predicted_rating, 2),
                model_version="fallback-v1.0",
                features_used=["heuristic_fallback"],
                processing_time=response_time,
                metadata={
                    "model_status": "not_loaded",
                    "prediction_method": "heuristic_fallback",
                    "monitoring_enabled": HAS_MONITORING,
                    "warning": "Ïã§Ï†ú ML Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÄ ÏòàÏ∏°ÏûÖÎãàÎã§",
                    "note": "Î™®Îç∏ ÌõàÎ†® ÌõÑ Îçî Ï†ïÌôïÌïú ÏòàÏ∏°Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§"
                }
            )

        # Î™®Îç∏ ÏòàÏ∏° with monitoring
        if HAS_MONITORING:
            predicted_rating = track_prediction_time("imdb_model", "1.0")(
                evaluator.predict_single_movie
            )(movie_data)
        else:
            predicted_rating = evaluator.predict_single_movie(movie_data)

        # Record prediction metrics
        if HAS_MONITORING:
            mlops_metrics.record_prediction_rating(predicted_rating)
            mlops_metrics.model_predictions_total.labels(
                model_name="imdb_model", model_version="1.0", prediction_type="single"
            ).inc()

        # Í∞êÏ†ï Î∂ÑÎ•ò (ÌèâÏ†ê Í∏∞Î∞ò)
        sentiment = "positive" if predicted_rating >= 6.0 else "negative"
        confidence = min(0.95, max(0.55, predicted_rating / 10.0))

        # Record response time
        response_time = time.time() - start_time
        if HAS_MONITORING:
            mlops_metrics.http_request_duration_seconds.labels(
                method="POST", endpoint="/predict"
            ).observe(response_time)

        logger.info(
            f"ÏòàÏ∏° ÏôÑÎ£å: {predicted_rating:.2f}/10 (ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s)"
        )

        return PredictionResponse(
            text=request.text,
            sentiment=sentiment,
            confidence=confidence,
            timestamp=datetime.now().isoformat(),
            # Enhanced fields for graceful degradation
            predicted_rating=round(predicted_rating, 2),
            model_version="fallback-v1.0",
            features_used=["heuristic_fallback"],
            processing_time=0.001,
            metadata={
                "model_status": "not_loaded",
                "prediction_method": "heuristic_fallback",
                "warning": "Ïã§Ï†ú ML Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÄ ÏòàÏ∏°ÏûÖÎãàÎã§",
                "note": "Î™®Îç∏ ÌõàÎ†® ÌõÑ Îçî Ï†ïÌôïÌïú ÏòàÏ∏°Ïù¥ Í∞ÄÎä•Ìï©ÎãàÎã§"
            }
        )

    except Exception as e:
        # Record error metrics
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict", status_code="500"
            ).inc()

        logger.error(f"ÏòàÏ∏° Ïò§Î•ò: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ÏòàÏ∏° Ïã§Ìå®: {str(e)}")


@router.post("/predict/movie", response_model=Dict[str, Any])
@track_api_call(endpoint="/predict/movie", method="POST")
async def predict_movie_with_features(
    movie_data: Dict[str, Any], evaluator: ModelEvaluator = Depends(get_model_evaluator)
):
    """
    ÏòÅÌôî ÌîºÏ≤òÎ•º ÏßÅÏ†ë ÏûÖÎ†•Î∞õÏïÑ ÌèâÏ†ê ÏòàÏ∏° (Enhanced with monitoring)
    """
    start_time = time.time()

    try:
        logger.info(f"ÏòÅÌôî ÌèâÏ†ê ÏòàÏ∏°: {movie_data}")

        # Record API call
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict/movie", status_code="200"
            ).inc()

        # ÌïÑÏàò ÌîºÏ≤ò ÌôïÏù∏
        required_features = evaluator.get_feature_names()
        logger.info(f"Î™®Îç∏ ÌïÑÏöî ÌîºÏ≤ò: {required_features}")

        # ÏòàÏ∏° Ïã§Ìñâ with monitoring
        if HAS_MONITORING:
            predicted_rating = track_prediction_time("imdb_model", "1.0")(
                evaluator.predict_single_movie
            )(movie_data)
        else:
            predicted_rating = evaluator.predict_single_movie(movie_data)

        # Record business mlops_metrics
        if HAS_MONITORING:
            mlops_metrics.record_prediction_rating(predicted_rating)
            mlops_metrics.model_predictions_total.labels(
                model_name="imdb_model",
                model_version="1.0",
                prediction_type="movie_features",
            ).inc()

            # Record feature usage
            for feature in required_features:
                if feature in movie_data:
                    mlops_metrics.data_validation_errors_total.labels(
                        validation_type="feature_present", error_type="none"
                    ).inc()

        # ÏùëÎãµ ÏÉùÏÑ±
        response = {
            "title": movie_data.get("title", "Unknown Movie"),
            "predicted_rating": round(predicted_rating, 2),
            "rating_out_of_10": f"{predicted_rating:.1f}/10",
            "features_used": {
                k: v for k, v in movie_data.items() if k in required_features
            },
            "model_features": required_features,
            "timestamp": datetime.now().isoformat(),
        }

        # Record response time
        response_time = time.time() - start_time
        if HAS_MONITORING:
            mlops_metrics.http_request_duration_seconds.labels(
                method="POST", endpoint="/predict/movie"
            ).observe(response_time)

        logger.info(
            f"ÏòàÏ∏° Í≤∞Í≥º: {predicted_rating:.2f}/10 (ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s)"
        )
        return response

    except Exception as e:
        # Record error mlops_metrics
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict/movie", status_code="500"
            ).inc()

        logger.error(f"ÏòÅÌôî ÏòàÏ∏° Ïò§Î•ò: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ÏòàÏ∏° Ïã§Ìå®: {str(e)}")

@router.post("/predict/batch", response_model=BatchPredictionResponse)
@track_api_call(endpoint="/predict/batch", method="POST")
async def predict_batch_movies(request: BatchPredictionRequest):
    """
    Ïó¨Îü¨ ÏòÅÌôî Î∞∞Ïπò ÏòàÏ∏° (Enhanced with monitoring + graceful degradation)
    """
    start_time = time.time()
    batch_size = len(request.texts)

    try:
        logger.info(f"Î∞∞Ïπò ÏòàÏ∏° ÏöîÏ≤≠: {batch_size}Í∞ú ÏòÅÌôî")

        # Record API call
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict/batch", status_code="200"
            ).inc()

        if batch_size > 100:
            raise HTTPException(
                status_code=400, detail="Ìïú Î≤àÏóê ÏµúÎåÄ 100Í∞úÍπåÏßÄ Ï≤òÎ¶¨ Í∞ÄÎä•Ìï©ÎãàÎã§."
            )

        # Check model availability (graceful handling)
        evaluator = get_model_evaluator()
        predictions = []
        successful_predictions = 0
        failed_predictions = 0
        fallback_predictions = 0

        # Process batch with monitoring
        for i, text in enumerate(request.texts):
            try:
                # Í∞ÑÎã®Ìïú ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî ÌÖçÏä§Ìä∏ ÌååÏã± ÌïÑÏöî)
                movie_data = {
                    "startYear": 2020 - (i % 20),  # 2000-2020 Î≤îÏúÑ
                    "runtimeMinutes": 90 + (i % 60),  # 90-150Î∂Ñ Î≤îÏúÑ
                    "numVotes": 1000 + (i * 100),  # Îã§ÏñëÌïú Ïù∏Í∏∞ÎèÑ
                }

                if evaluator is None:
                    # Î™®Îç∏Ïù¥ ÏóÜÏùÑ Îïå fallback ÏòàÏ∏° + monitoring
                    base_rating = 6.0  # Í∏∞Î≥∏ ÌèâÏ†ê
                    
                    # Ïó∞ÎèÑ Î≥¥Ï†ï
                    year = movie_data.get("startYear", 2020)
                    if year > 2010:
                        base_rating += 0.2
                    if year > 2015:
                        base_rating += 0.1
                        
                    # Îü∞ÌÉÄÏûÑ Î≥¥Ï†ï
                    runtime = movie_data.get("runtimeMinutes", 120)
                    if 90 <= runtime <= 150:
                        base_rating += 0.1
                        
                    # Ìà¨ÌëúÏàò Î≥¥Ï†ï
                    votes = movie_data.get("numVotes", 1000)
                    if votes > 5000:
                        base_rating += 0.1
                        
                    # Í∞úÎ≥Ñ ÏòÅÌôîÎßàÎã§ ÏïΩÍ∞ÑÏùò Î≥ÄÌôî Ï∂îÍ∞Ä
                    import random
                    base_rating += random.uniform(-0.3, 0.3)
                    
                    # Î≤îÏúÑ Ï†úÌïú
                    predicted_rating = min(max(base_rating, 1.0), 10.0)
                    
                    sentiment = "positive" if predicted_rating >= 6.0 else "negative"
                    confidence = 0.5  # ÎÇÆÏùÄ Ïã†Î¢∞ÎèÑ

                    predictions.append(
                        PredictionResponse(
                            text=text,
                            sentiment=sentiment,
                            confidence=confidence,
                            timestamp=datetime.now().isoformat(),
                            # Enhanced fields for fallback
                            predicted_rating=round(predicted_rating, 2),
                            model_version="fallback-v1.0",
                            features_used=["heuristic_fallback"],
                            processing_time=0.001,
                            metadata={
                                "model_status": "not_loaded",
                                "prediction_method": "heuristic_fallback",
                                "batch_index": i,
                                "monitoring_enabled": HAS_MONITORING,
                                "warning": "Ïã§Ï†ú ML Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏßÄ ÏïäÏùÄ ÏòàÏ∏°ÏûÖÎãàÎã§"
                            }
                        )
                    )

                    fallback_predictions += 1

                    # Record fallback prediction metrics
                    if HAS_MONITORING:
                        mlops_metrics.record_prediction_rating(predicted_rating)
                        mlops_metrics.model_predictions_total.labels(
                            model_name="fallback_model", 
                            model_version="1.0", 
                            prediction_type="batch_fallback"
                        ).inc()

                else:
                    # Î™®Îç∏Ïù¥ ÏûàÏùÑ Îïå Ï†ïÏÉÅ ÏòàÏ∏° (Í∏∞Ï°¥ monitoring Î°úÏßÅ)
                    if HAS_MONITORING:
                        predicted_rating = track_prediction_time("imdb_model", "1.0")(
                            evaluator.predict_single_movie
                        )(movie_data)
                    else:
                        predicted_rating = evaluator.predict_single_movie(movie_data)
                        
                    sentiment = "positive" if predicted_rating >= 6.0 else "negative"
                    confidence = min(0.95, max(0.55, predicted_rating / 10.0))

                    predictions.append(
                        PredictionResponse(
                            text=text,
                            sentiment=sentiment,
                            confidence=confidence,
                            timestamp=datetime.now().isoformat(),
                            # Enhanced fields for normal prediction
                            predicted_rating=round(predicted_rating, 2),
                            model_version="1.0.0",
                            features_used=evaluator.get_feature_names(),
                            processing_time=0.05,
                            metadata={
                                "model_status": "loaded",
                                "prediction_method": "ml_model",
                                "batch_index": i,
                                "monitoring_enabled": HAS_MONITORING,
                                "model_type": evaluator.model_type
                            }
                        )
                    )

                    successful_predictions += 1

                    # Record individual prediction
                    if HAS_MONITORING:
                        mlops_metrics.record_prediction_rating(predicted_rating)

            except Exception as e:
                logger.warning(f"Í∞úÎ≥Ñ ÏòàÏ∏° Ïã§Ìå® ({text[:30]}...): {e}")
                failed_predictions += 1

                # Ïã§Ìå®Ìïú Í≤ΩÏö∞ Í∏∞Î≥∏Í∞í (error fallback)
                predictions.append(
                    PredictionResponse(
                        text=text,
                        sentiment="neutral",
                        confidence=0.3,
                        timestamp=datetime.now().isoformat(),
                        # Enhanced fields for error case
                        predicted_rating=5.0,  # Ï§ëÍ∞ÑÍ∞í
                        model_version="error-fallback",
                        features_used=["error_fallback"],
                        processing_time=0.001,
                        metadata={
                            "model_status": "error",
                            "prediction_method": "error_fallback",
                            "batch_index": i,
                            "monitoring_enabled": HAS_MONITORING,
                            "error": str(e),
                            "warning": "Í∞úÎ≥Ñ ÏòàÏ∏° Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§"
                        }
                    )
                )

        # Record batch metrics
        if HAS_MONITORING:
            # Record successful ML predictions
            if successful_predictions > 0:
                mlops_metrics.model_predictions_total.labels(
                    model_name="imdb_model", 
                    model_version="1.0", 
                    prediction_type="batch"
                ).inc(successful_predictions)

            # Record fallback predictions
            if fallback_predictions > 0:
                mlops_metrics.model_predictions_total.labels(
                    model_name="fallback_model", 
                    model_version="1.0", 
                    prediction_type="batch_fallback"
                ).inc(fallback_predictions)

            # Record failed predictions
            if failed_predictions > 0:
                mlops_metrics.model_predictions_total.labels(
                    model_name="error_model",
                    model_version="1.0",
                    prediction_type="batch_failed",
                ).inc(failed_predictions)

        # Record response time
        response_time = time.time() - start_time
        if HAS_MONITORING:
            mlops_metrics.http_request_duration_seconds.labels(
                method="POST", endpoint="/predict/batch"
            ).observe(response_time)

        logger.info(
            f"Î∞∞Ïπò ÏòàÏ∏° ÏôÑÎ£å: {successful_predictions}Í∞ú MLÏÑ±Í≥µ, {fallback_predictions}Í∞ú fallback, {failed_predictions}Í∞ú Ïã§Ìå® (ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s)"
        )

        return BatchPredictionResponse(
            predictions=predictions, 
            total_count=len(predictions)
        )

    except HTTPException:
        raise
    except Exception as e:
        # Record error metrics
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST", endpoint="/predict/batch", status_code="500"
            ).inc()

        logger.error(f"Î∞∞Ïπò ÏòàÏ∏° Ïò§Î•ò: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Î∞∞Ïπò ÏòàÏ∏° Ïã§Ìå®: {str(e)}")
@router.get("/model/info")  # Remove response_model=ModelInfo to allow flexible response
async def get_model_info():
    """Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå (Enhanced with monitoring + graceful handling)"""
    start_time = time.time()

    try:
        # Record API call
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET", endpoint="/model/info", status_code="200"
            ).inc()

        # Check model availability (graceful handling)
        evaluator = get_model_evaluator()
        
        if evaluator is None:
            # Model not available - provide graceful response with monitoring
            logger.warning("Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå - Î™®Îç∏ ÏóÜÏùå (with monitoring)")
            
            # Record model unavailability
            if HAS_MONITORING:
                mlops_metrics.data_validation_errors_total.labels(
                    validation_type="model_info_check", error_type="model_not_available"
                ).inc()
            
            response = {
                "name": "IMDB Rating Predictor",
                "version": "not_loaded",
                "status": "model_not_available",
                "description": "IMDB ÏòÅÌôî ÌèâÏ†ê ÏòàÏ∏° Î™®Îç∏ (ÌòÑÏû¨ Î°úÎìúÎêòÏßÄ ÏïäÏùå)",
                "model_loaded": False,
                "created_at": datetime.now().isoformat(),
                "metrics": {
                    "features": 0,
                    "model_loaded": False,
                    "scaler_loaded": False,
                    "monitoring_enabled": HAS_MONITORING,
                },
                "expected_features": ["startYear", "runtimeMinutes", "numVotes"],
                "fallback_available": True,
                "suggestions": [
                    "Î™®Îç∏ ÌõàÎ†®: python scripts/train_model.py",
                    "Î™®Îç∏ ÌååÏùº ÌôïÏù∏: ls models/",
                    "ÏÑúÎ≤Ñ Ïû¨ÏãúÏûë Í≥†Î†§"
                ],
                "warning": "ÌòÑÏû¨ heuristic fallback Î™®ÎìúÏóêÏÑú ÎèôÏûë Ï§ëÏûÖÎãàÎã§",
                "monitoring_status": "enabled" if HAS_MONITORING else "disabled"
            }
            
            # Record response time for fallback case
            response_time = time.time() - start_time
            if HAS_MONITORING:
                mlops_metrics.http_request_duration_seconds.labels(
                    method="GET", endpoint="/model/info"
                ).observe(response_time)
            
            logger.info(f"Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå ÏôÑÎ£å (fallback mode) - ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s")
            return response
        
        # Model is available - normal flow with all monitoring
        try:
            model_info = evaluator.get_model_info()
            
            # Record successful model info retrieval
            if HAS_MONITORING:
                mlops_metrics.model_predictions_total.labels(
                    model_name="imdb_model", 
                    model_version="1.0", 
                    prediction_type="model_info_query"
                ).inc()
            
            response = {
                "name": model_info.get("model_type", "Unknown"),
                "version": "1.0.0",
                "status": "loaded",
                "description": f"IMDB ÏòÅÌôî ÌèâÏ†ê ÏòàÏ∏° Î™®Îç∏ ({model_info.get('model_type', 'Unknown')})",
                "created_at": datetime.now().isoformat(),
                "model_loaded": True,
                "metrics": {
                    "features": model_info.get("n_features", 0),
                    "model_loaded": model_info.get("model_loaded", False),
                    "scaler_loaded": model_info.get("scaler_loaded", False),
                    "monitoring_enabled": HAS_MONITORING,
                },
                "feature_names": model_info.get("feature_names", []),
                "fallback_available": False,
                "model_type": model_info.get("model_type", "Unknown"),
                "monitoring_status": "enabled" if HAS_MONITORING else "disabled"
            }
            
        except Exception as model_error:
            logger.error(f"Î™®Îç∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ïò§Î•ò: {model_error}")
            
            # Record model info extraction error
            if HAS_MONITORING:
                mlops_metrics.data_validation_errors_total.labels(
                    validation_type="model_info_extraction", error_type="extraction_failed"
                ).inc()
            
            # Fallback response when model exists but info extraction fails
            response = {
                "name": "IMDB Rating Predictor",
                "version": "1.0.0",
                "status": "loaded_with_errors",
                "description": "IMDB ÏòÅÌôî ÌèâÏ†ê ÏòàÏ∏° Î™®Îç∏ (Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë Ïò§Î•ò)",
                "created_at": datetime.now().isoformat(),
                "model_loaded": True,
                "metrics": {
                    "features": 3,  # Default expected
                    "model_loaded": True,
                    "scaler_loaded": False,  # Unknown due to error
                    "monitoring_enabled": HAS_MONITORING,
                },
                "expected_features": ["startYear", "runtimeMinutes", "numVotes"],
                "fallback_available": True,
                "error": str(model_error),
                "warning": "Î™®Îç∏ Ï†ïÎ≥¥ Ï∂îÏ∂ú Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏßÄÎßå ÏòàÏ∏°ÏùÄ Í∞ÄÎä•Ìï† Ïàò ÏûàÏäµÎãàÎã§",
                "monitoring_status": "enabled" if HAS_MONITORING else "disabled"
            }

        # Record response time
        response_time = time.time() - start_time
        if HAS_MONITORING:
            mlops_metrics.http_request_duration_seconds.labels(
                method="GET", endpoint="/model/info"
            ).observe(response_time)

        logger.info(f"Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå ÏôÑÎ£å - ÏùëÎãµÏãúÍ∞Ñ: {response_time:.3f}s")
        return response

    except HTTPException:
        raise
    except Exception as e:
        # Record error metrics
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET", endpoint="/model/info", status_code="500"
            ).inc()

        logger.error(f"Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ïò§Î•ò: {str(e)}")
        
        # Return error response instead of raising exception (graceful)
        return {
            "name": "IMDB Rating Predictor",
            "version": "error",
            "status": "error",
            "description": "Î™®Îç∏ Ï†ïÎ≥¥ Ï°∞Ìöå Ï§ë Ïò§Î•ò Î∞úÏÉù",
            "created_at": datetime.now().isoformat(),
            "model_loaded": False,
            "error": str(e),
            "fallback_available": True,
            "suggestions": [
                "ÏÑúÎ≤Ñ Ïû¨ÏãúÏûë ÏãúÎèÑ",
                "Î°úÍ∑∏ ÌôïÏù∏",
                "Î™®Îç∏ ÌååÏùº ÏÉÅÌÉú Ï†êÍ≤Ä"
            ],
            "monitoring_status": "enabled" if HAS_MONITORING else "disabled"
        }
@router.get("/health", response_model=HealthResponse)
@track_api_call(endpoint="/health", method="GET")
async def health_check():
    """API ÏÉÅÌÉú ÌôïÏù∏ (Enhanced with monitoring + graceful degradation)"""
    start_time = time.time()

    try:
        evaluator = get_model_evaluator()
        model_loaded = evaluator is not None
        
        # ÏÉÅÌÉú Í≤∞Ï†ï
        if model_loaded:
            status = "healthy"
            details = "Î™®Îì† ÏÑúÎπÑÏä§Í∞Ä Ï†ïÏÉÅ ÎèôÏûë Ï§ë"
        else:
            status = "degraded"
            details = "Î™®Îç∏Ïù¥ Î°úÎìúÎêòÏßÄ ÏïäÏïòÏßÄÎßå APIÎäî ÎèôÏûë Í∞ÄÎä• (fallback Î™®Îìú)"
        
        # Record API call
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET", endpoint="/health", status_code="200"
            ).inc()

            # Update active users (simple example)
            mlops_metrics.set_active_users(1)

        # Create response object
        response = HealthResponse(
            status=status,
            timestamp=datetime.now(),
            version="1.1.0",  # Updated version to match monitoring version
            model_loaded=model_loaded,
            details=details,
            capabilities={
                "prediction": True,  # fallback Í∞ÄÎä•ÌïòÎØÄÎ°ú Ìï≠ÏÉÅ true
                "model_info": True,
                "health_check": True,
                "fallback_prediction": not model_loaded
            }
        )

        # Record response time
        response_time = time.time() - start_time
        if HAS_MONITORING:
            mlops_metrics.http_request_duration_seconds.labels(
                method="GET", endpoint="/health"
            ).observe(response_time)

        return response

    except Exception as e:
        # Record error metrics
        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET", endpoint="/health", status_code="500"
            ).inc()

        logger.error(f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ïò§Î•ò: {str(e)}")
        return HealthResponse(
            status="unhealthy",
            timestamp=datetime.now(),
            version="1.1.0",
            model_loaded=False,
            details=f"Ìó¨Ïä§Ï≤¥ÌÅ¨ Ï§ë Ïò§Î•ò Î∞úÏÉù: {str(e)}",
            capabilities={
                "prediction": False,
                "model_info": False,
                "health_check": True,
                "fallback_prediction": False
            }
        )
    

# ÏÉàÎ°úÏö¥ Î™®ÎãàÌÑ∞ÎßÅ Ï†ÑÏö© ÏóîÎìúÌè¨Ïù∏Ìä∏Îì§
@router.get("/monitoring/predictions/stats")
@track_api_call(endpoint="/monitoring/predictions/stats", method="GET")
async def get_prediction_stats():
    """ÏòàÏ∏° ÌÜµÍ≥Ñ Ï°∞Ìöå"""
    if not HAS_MONITORING:
        return {"error": "Monitoring not available"}

    try:
        # Simulate getting prediction statistics
        stats = {
            "total_predictions": 1000,  # This would come from metrics
            "average_rating": 7.2,
            "predictions_per_hour": 150,
            "model_accuracy": 0.69,
            "response_time_ms": 250,
            "error_rate": 0.01,
            "timestamp": datetime.now().isoformat(),
        }

        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET",
                endpoint="/monitoring/predictions/stats",
                status_code="200",
            ).inc()

        return stats

    except Exception as e:
        logger.error(f"Prediction stats error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/monitoring/data-drift/check")
@track_api_call(endpoint="/monitoring/data-drift/check", method="POST")
async def check_data_drift(data: Dict[str, Any]):
    """Îç∞Ïù¥ÌÑ∞ ÎìúÎ¶¨ÌîÑÌä∏ Ï≤¥ÌÅ¨"""
    if not HAS_MONITORING:
        return {"error": "Monitoring not available"}

    try:
        # Simulate data drift detection
        features = data.get("features", {})
        drift_scores = {}

        for feature_name, feature_value in features.items():
            # Simple drift simulation
            drift_score = abs(hash(str(feature_value)) % 100) / 1000.0
            drift_scores[feature_name] = drift_score

            # Record drift metrics
            if HAS_MONITORING:
                mlops_metrics.record_data_drift(feature_name, drift_score, "imdb_model")

        # Overall drift status
        max_drift = max(drift_scores.values()) if drift_scores else 0
        drift_status = (
            "high" if max_drift > 0.1 else "low" if max_drift > 0.05 else "normal"
        )

        response = {
            "drift_status": drift_status,
            "max_drift_score": max_drift,
            "feature_drift_scores": drift_scores,
            "threshold": 0.1,
            "timestamp": datetime.now().isoformat(),
        }

        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST",
                endpoint="/monitoring/data-drift/check",
                status_code="200",
            ).inc()

        return response

    except Exception as e:
        logger.error(f"Data drift check error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/monitoring/alerts/active")
@track_api_call(endpoint="/monitoring/alerts/active", method="GET")
async def get_active_alerts():
    """ÌôúÏÑ± ÏïåÎ¶º Ï°∞Ìöå"""
    try:
        # Simulate active alerts (in real scenario, this would query AlertManager)
        alerts = [
            {
                "id": "alert_001",
                "severity": "warning",
                "message": "API response time above threshold",
                "timestamp": datetime.now().isoformat(),
                "status": "firing",
            },
            {
                "id": "alert_002",
                "severity": "info",
                "message": "Model accuracy within normal range",
                "timestamp": datetime.now().isoformat(),
                "status": "resolved",
            },
        ]

        active_alerts = [alert for alert in alerts if alert["status"] == "firing"]

        response = {
            "active_alerts": active_alerts,
            "total_active": len(active_alerts),
            "last_updated": datetime.now().isoformat(),
        }

        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="GET", endpoint="/monitoring/alerts/active", status_code="200"
            ).inc()

        return response

    except Exception as e:
        logger.error(f"Active alerts error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/monitoring/model/update-accuracy")
@track_api_call(endpoint="/monitoring/model/update-accuracy", method="POST")
async def update_model_accuracy(accuracy_data: Dict[str, Any]):
    """Î™®Îç∏ Ï†ïÌôïÎèÑ ÏóÖÎç∞Ïù¥Ìä∏"""
    if not HAS_MONITORING:
        return {"error": "Monitoring not available"}

    try:
        accuracy = accuracy_data.get("accuracy", 0.0)
        model_name = accuracy_data.get("model_name", "imdb_model")
        model_version = accuracy_data.get("model_version", "1.0")

        # Validate accuracy
        if not 0 <= accuracy <= 1:
            raise HTTPException(
                status_code=400, detail="Accuracy must be between 0 and 1"
            )

        # Record accuracy metric
        if HAS_MONITORING:
            mlops_metrics.record_model_accuracy(accuracy, model_name, model_version)

        response = {
            "message": "Model accuracy updated",
            "model_name": model_name,
            "model_version": model_version,
            "accuracy": accuracy,
            "timestamp": datetime.now().isoformat(),
        }

        if HAS_MONITORING:
            mlops_metrics.http_requests_total.labels(
                method="POST",
                endpoint="/monitoring/model/update-accuracy",
                status_code="200",
            ).inc()

        logger.info(
            f"Model accuracy updated: {model_name} v{model_version} = {accuracy}"
        )
        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Update accuracy error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Enhanced load_model_at_startup function (for both endpoints.py and endpoints_with_metrics.py)

def load_model_at_startup():
    """
    Ïï± ÏãúÏûëÏãú Î™®Îç∏ Î°úÎìú (Enhanced with graceful handling)
    """
    global model_evaluator
    
    try:
        from pathlib import Path
        
        logger.info("üîç Î™®Îç∏ Î°úÎìú ÏãúÎèÑ Ï§ë...")
        
        # Î™®Îç∏ ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏
        models_dir = Path("models")
        if not models_dir.exists():
            logger.warning("‚ö†Ô∏è models ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏäµÎãàÎã§ - CI/CD ÌôòÍ≤ΩÏùº Ïàò ÏûàÏäµÎãàÎã§")
            logger.info("üìù fallback Î™®ÎìúÎ°ú ÎèôÏûëÌï©ÎãàÎã§")
            model_evaluator = None
            return False
        
        # Î™®Îç∏ ÌååÏùº ÌôïÏù∏
        model_files = list(models_dir.glob("*forest*.joblib"))
        if not model_files:
            logger.warning("‚ö†Ô∏è Ï†ÄÏû•Îêú Î™®Îç∏ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§ - CI/CD ÌôòÍ≤ΩÏùº Ïàò ÏûàÏäµÎãàÎã§")
            logger.info("üìù heuristic fallback Î™®ÎìúÎ°ú ÎèôÏûëÌï©ÎãàÎã§")
            logger.info("üí° Î™®Îç∏ ÌõàÎ†® Î∞©Î≤ï: python scripts/train_model.py")
            model_evaluator = None
            return False
        
        # Î™®Îç∏ Î°úÎìú ÏãúÎèÑ
        latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
        logger.info(f"üì¶ Î™®Îç∏ Î°úÎìú ÏãúÎèÑ: {latest_model}")
        
        from ..models.evaluator import ModelEvaluator
        model_evaluator = ModelEvaluator()
        model_evaluator.load_model(str(latest_model))
        
        logger.info("‚úÖ Î™®Îç∏ Î°úÎìú ÏÑ±Í≥µ!")
        logger.info(f"   Î™®Îç∏ ÌÉÄÏûÖ: {model_evaluator.model_type}")
        logger.info(f"   ÌîºÏ≤ò: {model_evaluator.get_feature_names()}")
        
        # Record successful model loading (for monitoring version)
        try:
            # This will work in endpoints_with_metrics.py
            if HAS_MONITORING and 'mlops_metrics' in globals():
                mlops_metrics.record_model_accuracy(0.69, "imdb_model", "1.0")
                mlops_metrics.mlflow_experiments_total.labels(
                    experiment_name="model_loading"
                ).inc()
                mlops_metrics.mlflow_runs_total.labels(
                    experiment_name="model_loading", status="success"
                ).inc()
        except:
            # Silently fail if monitoring not available (for regular endpoints.py)
            pass
        
        return True
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Î™®Îç∏ Î°úÎìú Ïã§Ìå®: {e}")
        logger.info("üìù APIÎäî fallback Î™®ÎìúÎ°ú Í≥ÑÏÜç ÎèôÏûëÌï©ÎãàÎã§")
        logger.info("üí° Î¨∏Ï†ú Ìï¥Í≤∞ Î∞©Î≤ï:")
        logger.info("   1. python scripts/train_model.py Ïã§Ìñâ")
        logger.info("   2. models/ ÎîîÎ†âÌÜ†Î¶¨ ÌôïÏù∏")
        logger.info("   3. ÏÑúÎ≤Ñ Ïû¨ÏãúÏûë")
        
        # Record failed model loading (for monitoring version)
        try:
            # This will work in endpoints_with_metrics.py
            if HAS_MONITORING and 'mlops_metrics' in globals():
                mlops_metrics.mlflow_runs_total.labels(
                    experiment_name="model_loading", status="failed"
                ).inc()
        except:
            # Silently fail if monitoring not available (for regular endpoints.py)
            pass
        
        model_evaluator = None
        return False
