"""
icecream, tqdm, rich í†µí•©ì´ í¬í•¨ëœ í–¥ìƒëœ íŠ¸ë ˆì´ë„ˆ
ë” ë‚˜ì€ ë””ë²„ê¹…, ì§„í–‰ë¥  ì¶”ì , ì‹œê°ì  í”¼ë“œë°±
"""

import logging
import warnings
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

import joblib
import mlflow
import mlflow.sklearn
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")

# í–¥ìƒëœ ìœ í‹¸ë¦¬í‹°
from ..utils.enhanced import (
    HAS_ICECREAM,
    HAS_RICH,
    HAS_TQDM,
    EnhancedLogger,
    ProgressTracker,
    display_table,
    enhanced_print,
    ic,
    tools,
)


class EnhancedMovieRatingTrainer:
    """
    ë” ë‚˜ì€ UXë¥¼ ì œê³µí•˜ëŠ” MovieRatingTrainerì˜ í–¥ìƒëœ ë²„ì „
    ê¸°ëŠ¥: ì§„í–‰ë¥  í‘œì‹œì¤„, ë” ë‚˜ì€ ë””ë²„ê¹…, í’ë¶€í•œ ì¶œë ¥
    """

    # í•µì‹¬ ê¸°ëŠ¥ (ì›ë³¸ê³¼ ë™ì¼)
    BASE_FEATURES = ["startYear", "runtimeMinutes", "numVotes"]
    TARGET_COLUMN = "averageRating"

    def __init__(self, experiment_name: str = "enhanced_imdb_movie_rating"):
        self.experiment_name = experiment_name
        self.model = None
        self.scaler = None
        self.feature_names = []
        self.models_dir = Path("models")
        self.models_dir.mkdir(exist_ok=True)

        # í–¥ìƒëœ êµ¬ì„± ìš”ì†Œ
        self.logger = EnhancedLogger("í–¥ìƒëœíŠ¸ë ˆì´ë„ˆ")
        self.progress = ProgressTracker()

        # í–¥ìƒëœ ë¡œê¹…ê³¼ í•¨ê»˜ MLflow ì„¤ì •
        try:
            mlflow.set_experiment(self.experiment_name)
            self.logger.success(f"MLflow ì‹¤í—˜ ì„¤ì •: {self.experiment_name}")
        except Exception as e:
            self.logger.error(f"MLflow ì„¤ì • ì‹¤íŒ¨: {e}")
            ic(e)

    def load_data(
        self, data_path: str = "data/processed/movies_with_ratings.csv"
    ) -> pd.DataFrame:
        """í–¥ìƒëœ ì§„í–‰ë¥  ì¶”ì ìœ¼ë¡œ ë°ì´í„° ë¡œë“œ"""
        self.logger.info("ì˜í™” ë°ì´í„° ë¡œë”© ì¤‘...")
        ic(data_path)

        if not Path(data_path).exists():
            self.logger.error(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")

        # ì§„í–‰ë¥  í‘œì‹œì™€ í•¨ê»˜ ë¡œë“œ
        with self.progress.progress_context("ë°ì´í„° ë¡œë”©") as progress:
            task = progress.add_task("CSV ì½ëŠ” ì¤‘...", total=100)
            df = pd.read_csv(data_path)
            progress.update(task, advance=100)

        self.logger.success(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ ì˜í™”")
        ic(df.shape, df.columns.tolist())

        # ë°ì´í„° ìš”ì•½ í…Œì´ë¸” í‘œì‹œ
        display_table(
            "ë°ì´í„° ìš”ì•½",
            ["ì§€í‘œ", "ê°’"],
            [
                ["ì´ ì˜í™” ìˆ˜", f"{len(df):,}"],
                ["ì»¬ëŸ¼ ìˆ˜", str(len(df.columns))],
                ["ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{df.memory_usage().sum() / 1024**2:.1f} MB"],
                ["ê²°ì¸¡ê°’", str(df.isnull().sum().sum())],
                ["í‰ê·  í‰ì ", f"{df[self.TARGET_COLUMN].mean():.2f}"],
                [
                    "í‰ì  ë²”ìœ„",
                    f"{df[self.TARGET_COLUMN].min():.1f} - {df[self.TARGET_COLUMN].max():.1f}",
                ],
            ],
        )

        return df

    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """ë””ë²„ê¹…ì´ í¬í•¨ëœ í–¥ìƒëœ í”¼ì²˜ ì¤€ë¹„"""
        self.logger.info("í”¼ì²˜ ì¤€ë¹„ ì¤‘...")
        ic("í”¼ì²˜ ì¤€ë¹„ ì‹œì‘")

        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ í™•ì¸
        available_features = [col for col in self.BASE_FEATURES if col in df.columns]
        missing_features = [col for col in self.BASE_FEATURES if col not in df.columns]

        ic(available_features, missing_features)

        if not available_features:
            self.logger.error(
                f"í•„ìš”í•œ í”¼ì²˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! í•„ìš”: {self.BASE_FEATURES}"
            )
            raise ValueError(f"í•„ìˆ˜ í”¼ì²˜ ëˆ„ë½: {self.BASE_FEATURES}")

        if missing_features:
            self.logger.warning(f"ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")

        # ì§„í–‰ë¥ ê³¼ í•¨ê»˜ í”¼ì²˜ ì²˜ë¦¬
        self.logger.info("í”¼ì²˜ ì²˜ë¦¬ ì¤‘...")
        X = df[available_features].copy()

        # í”¼ì²˜ í†µê³„ì˜ í–¥ìƒëœ ë””ë²„ê¹…
        if HAS_ICECREAM:
            for feature in available_features:
                ic(feature)
                ic(X[feature].describe())
                ic(X[feature].isnull().sum())

        # ì§„í–‰ë¥  ì¶”ì ê³¼ í•¨ê»˜ ê²°ì¸¡ê°’ ì±„ìš°ê¸°
        with self.progress.progress_context("ê²°ì¸¡ê°’ ì±„ìš°ê¸°") as progress:
            task = progress.add_task("ì²˜ë¦¬ ì¤‘...", total=len(available_features))
            for feature in available_features:
                median_val = X[feature].median()
                X[feature] = X[feature].fillna(median_val)
                progress.update(task, advance=1)
                ic(f"{feature}ì˜ ê²°ì¸¡ê°’ì„ {median_val}ë¡œ ì±„ì›€")

        # íƒ€ê²Ÿ ë³€ìˆ˜
        y = df[self.TARGET_COLUMN].values
        ic(y.shape, y.min(), y.max(), y.mean())

        # í”¼ì²˜ ì´ë¦„ ì €ì¥
        self.feature_names = available_features

        self.logger.success(
            f"í”¼ì²˜ ì¤€ë¹„ ì™„ë£Œ: {len(X)}ê°œ ìƒ˜í”Œ, {len(available_features)}ê°œ í”¼ì²˜"
        )

        # í”¼ì²˜ ìƒê´€ê´€ê³„ ë¶„ì„ (í–¥ìƒëœ ì¶œë ¥)
        if HAS_RICH:
            correlations = []
            for feature in available_features:
                corr = np.corrcoef(X[feature], y)[0, 1]
                correlations.append([feature, f"{corr:.3f}"])

            display_table("í”¼ì²˜-íƒ€ê²Ÿ ìƒê´€ê´€ê³„", ["í”¼ì²˜", "ìƒê´€ê´€ê³„"], correlations)

        return X.values, y

    def train_model(
        self, X: np.ndarray, y: np.ndarray, model_type: str = "random_forest"
    ) -> Dict[str, float]:
        """ì§„í–‰ë¥ ê³¼ ë””ë²„ê¹…ì´ í¬í•¨ëœ í–¥ìƒëœ ëª¨ë¸ í›ˆë ¨"""
        self.logger.info(f"{model_type} ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        ic(X.shape, y.shape, model_type)

        # í–¥ìƒëœ ë¡œê¹…ê³¼ í•¨ê»˜ ë°ì´í„° ë¶„í• 
        self.logger.info("ë°ì´í„° ë¶„í•  ì¤‘...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        ic(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

        # ì§„í–‰ë¥ ê³¼ í•¨ê»˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§

        self.logger.info("í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì¤‘...")
        from sklearn.preprocessing import StandardScaler

        self.scaler = StandardScaler()

        with self.progress.progress_context("í”¼ì²˜ ìŠ¤ì¼€ì¼ë§") as progress:
            task = progress.add_task("ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ… ì¤‘...", total=100)
            X_train_scaled = self.scaler.fit_transform(X_train)
            progress.update(task, advance=50)
            X_test_scaled = self.scaler.transform(X_test)
            progress.update(task, advance=50)

        ic("í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")

        # í–¥ìƒëœ ì¶œë ¥ê³¼ í•¨ê»˜ ëª¨ë¸ ì„ íƒ
        self.logger.info(f"{model_type} ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        if model_type == "random_forest":
            self.model = RandomForestRegressor(
                n_estimators=100, random_state=42, n_jobs=-1  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
            )
        elif model_type == "linear_regression":
            self.model = LinearRegression()
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")

        ic(self.model.get_params())

        # ì§„í–‰ë¥  ì¶”ì ê³¼ í•¨ê»˜ í›ˆë ¨
        self.logger.info("ëª¨ë¸ í›ˆë ¨ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        start_time = datetime.now()

        # RandomForestì˜ ê²½ìš° n_estimatorsë¥¼ ì‚¬ìš©í•˜ì—¬ ì§„í–‰ë¥  ì¶”ì  ê°€ëŠ¥
        if model_type == "random_forest" and HAS_TQDM:
            # RandomForestìš© ì‚¬ìš©ì ì •ì˜ ì§„í–‰ë¥  ì¶”ì 
            estimators_list = []
            for i in self.progress.track(range(100), "ì¶”ì •ê¸° í›ˆë ¨"):
                temp_model = RandomForestRegressor(
                    n_estimators=1, random_state=42 + i, warm_start=False
                )
                temp_model.fit(X_train_scaled, y_train)
                estimators_list.extend(temp_model.estimators_)

            # ëª¨ë“  ì¶”ì •ê¸° ê²°í•©
            self.model.estimators_ = estimators_list
            self.model.n_estimators = len(estimators_list)
        else:
            self.model.fit(X_train_scaled, y_train)

        training_time = (datetime.now() - start_time).total_seconds()
        self.logger.success(f"í›ˆë ¨ì´ {training_time:.2f}ì´ˆ ë§Œì— ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤")
        ic(training_time)

        # í–¥ìƒëœ ì˜ˆì¸¡ ë° í‰ê°€
        self.logger.info("ëª¨ë¸ í‰ê°€ ì¤‘...")
        y_pred = self.model.predict(X_test_scaled)

        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            "rmse": np.sqrt(mean_squared_error(y_test, y_pred)),
            "mae": mean_absolute_error(y_test, y_pred),
            "r2_score": r2_score(y_test, y_pred),
        }

        ic(metrics)

        # í–¥ìƒëœ ë©”íŠ¸ë¦­ í‘œì‹œ
        display_table(
            "ëª¨ë¸ ì„±ëŠ¥",
            ["ë©”íŠ¸ë¦­", "ê°’", "í•´ì„"],
            [
                ["RMSE", f"{metrics['rmse']:.4f}", "ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ"],
                ["MAE", f"{metrics['mae']:.4f}", "ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ"],
                ["RÂ² ì ìˆ˜", f"{metrics['r2_score']:.4f}", "ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (ìµœëŒ€ 1.0)"],
                ["í›ˆë ¨ ì‹œê°„", f"{training_time:.2f}ì´ˆ", "íš¨ìœ¨ì„± ì¸¡ì •"],
            ],
        )

        # í”¼ì²˜ ì¤‘ìš”ë„ (RandomForestìš©)
        if hasattr(self.model, "feature_importances_"):
            importances = []
            for i, importance in enumerate(self.model.feature_importances_):
                importances.append([self.feature_names[i], f"{importance:.4f}"])

            display_table("í”¼ì²˜ ì¤‘ìš”ë„", ["í”¼ì²˜", "ì¤‘ìš”ë„"], importances)
            ic(dict(zip(self.feature_names, self.model.feature_importances_)))

        # í–¥ìƒëœ ì˜¤ë¥˜ ì²˜ë¦¬ì™€ í•¨ê»˜ MLflow ë¡œê¹…
        try:
            with mlflow.start_run():
                # ë§¤ê°œë³€ìˆ˜
                mlflow.log_param("model_type", model_type)
                mlflow.log_param("features", self.feature_names)
                mlflow.log_param("n_features", len(self.feature_names))
                mlflow.log_param("training_time", training_time)

                # ë©”íŠ¸ë¦­
                for metric_name, metric_value in metrics.items():
                    mlflow.log_metric(metric_name, metric_value)

                # ëª¨ë¸ ë¡œê¹…
                input_example = pd.DataFrame(
                    X_train_scaled[:5], columns=self.feature_names
                )

                mlflow.sklearn.log_model(
                    self.model,
                    "model",
                    input_example=input_example,
                    registered_model_name=f"{model_type}_movie_rating_enhanced",
                )

                self.logger.success("MLflow ë¡œê¹… ì™„ë£Œ")

        except Exception as e:
            self.logger.warning(f"MLflow ë¡œê¹… ì‹¤íŒ¨: {e}")
            ic(e)

        return metrics

    def save_model(self) -> Dict[str, str]:
        """ì§„í–‰ë¥ ê³¼ ê²€ì¦ì´ í¬í•¨ëœ í–¥ìƒëœ ëª¨ë¸ ì €ì¥"""
        if self.model is None:
            self.logger.error("ì €ì¥í•  í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤!")
            raise ValueError("í›ˆë ¨ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_filename = (
            f"enhanced_{type(self.model).__name__.lower()}_{timestamp}.joblib"
        )
        scaler_filename = f"enhanced_scaler_{timestamp}.joblib"

        model_path = self.models_dir / model_filename
        scaler_path = self.models_dir / scaler_filename

        # í–¥ìƒëœ ëª¨ë¸ ì •ë³´
        model_info = {
            "model": self.model,
            "feature_names": self.feature_names,
            "model_type": type(self.model).__name__,
            "timestamp": timestamp,
            "enhanced": True,
            "version": "2.0",
        }

        # ì§„í–‰ë¥  ì¶”ì ê³¼ í•¨ê»˜ ì €ì¥
        with self.progress.progress_context("ëª¨ë¸ ì €ì¥") as progress:
            task = progress.add_task("íŒŒì¼ ì €ì¥ ì¤‘...", total=100)

            joblib.dump(model_info, model_path)
            progress.update(task, advance=50)
            ic(f"ëª¨ë¸ ì €ì¥ë¨: {model_path}")

            if self.scaler:
                joblib.dump(self.scaler, scaler_path)
            progress.update(task, advance=50)
            ic(f"ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ë¨: {scaler_path}")

        self.logger.success(f"ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ê²€ì¦ í™•ì¸
        try:
            test_load = joblib.load(model_path)
            self.logger.success("ëª¨ë¸ íŒŒì¼ ê²€ì¦ í†µê³¼")
            ic("ëª¨ë¸ ì €ì¥ ê²€ì¦ ì„±ê³µ")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {e}")
            ic(e)

        return {
            "model_path": str(model_path),
            "scaler_path": str(scaler_path) if self.scaler else None,
            "feature_names": self.feature_names,
            "timestamp": timestamp,
        }


def enhanced_training_pipeline(
    data_path: str = "data/processed/movies_with_ratings.csv",
    model_type: str = "random_forest",
):
    """ì™„ì „í•œ UX ê°œì„ ì´ í¬í•¨ëœ í–¥ìƒëœ í›ˆë ¨ íŒŒì´í”„ë¼ì¸"""

    logger = EnhancedLogger("íŒŒì´í”„ë¼ì¸")
    logger.info("ğŸš€ í–¥ìƒëœ MLOps í›ˆë ¨ íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    try:
        # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”
        trainer = EnhancedMovieRatingTrainer()

        # ë°ì´í„° ë¡œë“œ
        df = trainer.load_data(data_path)

        # í”¼ì²˜ ì¤€ë¹„
        X, y = trainer.prepare_features(df)

        # ëª¨ë¸ í›ˆë ¨
        metrics = trainer.train_model(X, y, model_type=model_type)

        # ëª¨ë¸ ì €ì¥
        model_info = trainer.save_model()

        # ìµœì¢… ìš”ì•½
        enhanced_print(
            "\nğŸ‰ [bold green]í›ˆë ¨ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤![/bold green]"
        )

        display_table(
            "íŒŒì´í”„ë¼ì¸ ê²°ê³¼",
            ["êµ¬ì„±ìš”ì†Œ", "ìƒíƒœ", "ì„¸ë¶€ì‚¬í•­"],
            [
                ["ë°ì´í„° ë¡œë”©", "âœ… ì„±ê³µ", f"{len(df):,}ê°œ ì˜í™”"],
                ["í”¼ì²˜ ì¤€ë¹„", "âœ… ì„±ê³µ", f"{len(trainer.feature_names)}ê°œ í”¼ì²˜"],
                ["ëª¨ë¸ í›ˆë ¨", "âœ… ì„±ê³µ", f"RMSE: {metrics['rmse']:.4f}"],
                ["ëª¨ë¸ ì €ì¥", "âœ… ì„±ê³µ", model_info["model_path"]],
                ["MLflow ë¡œê¹…", "âœ… ì„±ê³µ", trainer.experiment_name],
            ],
        )

        logger.success("ëª¨ë“  íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return model_info

    except Exception as e:
        logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        ic(e)
        raise


def enhanced_model_comparison(
    data_path: str = "data/processed/movies_with_ratings.csv",
):
    """ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµë¥¼ ìœ„í•œ í–¥ìƒëœ í•¨ìˆ˜"""
    logger = EnhancedLogger("ëª¨ë¸ë¹„êµ")
    logger.info("ğŸ” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")

    models_to_compare = ["random_forest", "linear_regression"]
    comparison_results = []

    for model_type in logger.progress.track(models_to_compare, "ëª¨ë¸ ë¹„êµ"):
        try:
            logger.info(f"{model_type} ëª¨ë¸ í›ˆë ¨ ì¤‘...")

            # ê° ëª¨ë¸ì— ëŒ€í•´ í›ˆë ¨ ì‹¤í–‰
            trainer = EnhancedMovieRatingTrainer(
                experiment_name=f"model_comparison_{model_type}"
            )

            df = trainer.load_data(data_path)
            X, y = trainer.prepare_features(df)
            metrics = trainer.train_model(X, y, model_type=model_type)

            # ê²°ê³¼ ì €ì¥
            comparison_results.append(
                [
                    model_type.replace("_", " ").title(),
                    f"{metrics['rmse']:.4f}",
                    f"{metrics['mae']:.4f}",
                    f"{metrics['r2_score']:.4f}",
                ]
            )

            logger.success(f"{model_type} ì™„ë£Œ")

        except Exception as e:
            logger.error(f"{model_type} ì‹¤íŒ¨: {e}")
            comparison_results.append(
                [model_type.replace("_", " ").title(), "ì‹¤íŒ¨", "ì‹¤íŒ¨", "ì‹¤íŒ¨"]
            )

    # ë¹„êµ ê²°ê³¼ í‘œì‹œ
    display_table(
        "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", ["ëª¨ë¸", "RMSE", "MAE", "RÂ² ì ìˆ˜"], comparison_results
    )

    logger.success("ëª¨ë¸ ë¹„êµ ì™„ë£Œ!")
    return comparison_results


def enhanced_hyperparameter_tuning(
    data_path: str = "data/processed/movies_with_ratings.csv",
):
    """í–¥ìƒëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""
    logger = EnhancedLogger("í•˜ì´í¼íŒŒë¼ë¯¸í„°íŠœë‹")
    logger.info("ğŸ¯ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")

    # RandomForest í•˜ì´í¼íŒŒë¼ë¯¸í„° ê·¸ë¦¬ë“œ
    param_combinations = [
        {"n_estimators": 50, "max_depth": 10},
        {"n_estimators": 100, "max_depth": 15},
        {"n_estimators": 150, "max_depth": 20},
        {"n_estimators": 200, "max_depth": None},
    ]

    tuning_results = []
    best_score = float("inf")
    best_params = None

    # ë°ì´í„° ì¤€ë¹„ (í•œ ë²ˆë§Œ)
    trainer = EnhancedMovieRatingTrainer(experiment_name="hyperparameter_tuning")
    df = trainer.load_data(data_path)
    X, y = trainer.prepare_features(df)

    for i, params in enumerate(
        trainer.progress.track(param_combinations, "í•˜ì´í¼íŒŒë¼ë¯¸í„° í…ŒìŠ¤íŠ¸")
    ):
        try:
            logger.info(f"ë§¤ê°œë³€ìˆ˜ ì¡°í•© {i+1}/{len(param_combinations)} í…ŒìŠ¤íŠ¸ ì¤‘...")
            ic(params)

            # ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ë¡œ í›ˆë ¨
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import StandardScaler

            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42
            )

            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)

            # ë§¤ê°œë³€ìˆ˜ë¡œ ëª¨ë¸ ìƒì„±
            model = RandomForestRegressor(random_state=42, **params)
            model.fit(X_train_scaled, y_train)

            # í‰ê°€
            y_pred = model.predict(X_test_scaled)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)

            # ê²°ê³¼ ì €ì¥
            tuning_results.append(
                [
                    f"n_est={params['n_estimators']}, depth={params['max_depth']}",
                    f"{rmse:.4f}",
                    f"{mae:.4f}",
                    f"{r2:.4f}",
                ]
            )

            # ìµœê³  ì ìˆ˜ ì¶”ì 
            if rmse < best_score:
                best_score = rmse
                best_params = params

            ic(f"RMSE: {rmse:.4f}")

        except Exception as e:
            logger.error(f"ë§¤ê°œë³€ìˆ˜ ì¡°í•© ì‹¤íŒ¨: {e}")
            tuning_results.append(
                [
                    f"n_est={params['n_estimators']}, depth={params['max_depth']}",
                    "ì‹¤íŒ¨",
                    "ì‹¤íŒ¨",
                    "ì‹¤íŒ¨",
                ]
            )

    # íŠœë‹ ê²°ê³¼ í‘œì‹œ
    display_table(
        "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê²°ê³¼",
        ["ë§¤ê°œë³€ìˆ˜", "RMSE", "MAE", "RÂ² ì ìˆ˜"],
        tuning_results,
    )

    if best_params:
        logger.success(f"ìµœì  ë§¤ê°œë³€ìˆ˜: {best_params} (RMSE: {best_score:.4f})")

        ic(best_params, best_score)

        # ìµœì  ë§¤ê°œë³€ìˆ˜ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨
        logger.info("ìµœì  ë§¤ê°œë³€ìˆ˜ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
        final_trainer = EnhancedMovieRatingTrainer(experiment_name="best_model_final")

        # ìµœì  ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì¬ì •ì˜
        final_trainer.model = RandomForestRegressor(random_state=42, **best_params)

        # ìµœì¢… í›ˆë ¨ ë° ì €ì¥
        final_metrics = final_trainer.train_model(X, y, model_type="random_forest")
        final_model_info = final_trainer.save_model()

        enhanced_print(f"\nğŸ† [bold green]ìµœì  ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ![/bold green]")
        enhanced_print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: {final_model_info['model_path']}")

        return {
            "best_params": best_params,
            "best_score": best_score,
            "model_info": final_model_info,
            "tuning_results": tuning_results,
        }
    else:
        logger.error("ìœ íš¨í•œ ë§¤ê°œë³€ìˆ˜ ì¡°í•©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return None


def enhanced_data_analysis(data_path: str = "data/processed/movies_with_ratings.csv"):
    """í–¥ìƒëœ ë°ì´í„° ë¶„ì„ ë° ì‹œê°í™”"""
    logger = EnhancedLogger("ë°ì´í„°ë¶„ì„")
    logger.info("ğŸ“Š í–¥ìƒëœ ë°ì´í„° ë¶„ì„ ì‹œì‘")

    try:
        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        logger.success(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê°œ í–‰")

        # ê¸°ë³¸ í†µê³„
        numeric_columns = df.select_dtypes(include=[np.number]).columns

        # ê¸°ë³¸ í†µê³„ í…Œì´ë¸”
        basic_stats = []
        for col in numeric_columns:
            basic_stats.append(
                [
                    col,
                    f"{df[col].mean():.2f}",
                    f"{df[col].std():.2f}",
                    f"{df[col].min():.2f}",
                    f"{df[col].max():.2f}",
                    str(df[col].isnull().sum()),
                ]
            )

        display_table(
            "ê¸°ë³¸ í†µê³„",
            ["ì»¬ëŸ¼", "í‰ê· ", "í‘œì¤€í¸ì°¨", "ìµœì†Œê°’", "ìµœëŒ€ê°’", "ê²°ì¸¡ê°’"],
            basic_stats,
        )

        # í‰ì  ë¶„í¬ ë¶„ì„
        rating_col = "averageRating"
        if rating_col in df.columns:
            rating_stats = []

            # í‰ì  êµ¬ê°„ë³„ ë¶„í¬
            bins = [0, 5, 6, 7, 8, 9, 10]
            labels = [
                "ë§¤ìš° ë‚®ìŒ(0-5)",
                "ë‚®ìŒ(5-6)",
                "ë³´í†µ(6-7)",
                "ì¢‹ìŒ(7-8)",
                "ë§¤ìš° ì¢‹ìŒ(8-9)",
                "ìµœê³ (9-10)",
            ]

            df["rating_category"] = pd.cut(
                df[rating_col], bins=bins, labels=labels, include_lowest=True
            )

            for category in labels:
                count = (df["rating_category"] == category).sum()
                percentage = (count / len(df)) * 100
                rating_stats.append([category, str(count), f"{percentage:.1f}%"])

            display_table("í‰ì  ë¶„í¬", ["í‰ì  êµ¬ê°„", "ì˜í™” ìˆ˜", "ë¹„ìœ¨"], rating_stats)

        # ì—°ë„ë³„ ë¶„ì„
        year_col = "startYear"
        if year_col in df.columns:
            # ìµœê·¼ 20ë…„ ë°ì´í„° ë¶„ì„
            recent_years = df[df[year_col] >= 2000]

            if len(recent_years) > 0:
                year_analysis = []

                # 5ë…„ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                year_groups = [
                    (2000, 2004, "2000-2004"),
                    (2005, 2009, "2005-2009"),
                    (2010, 2014, "2010-2014"),
                    (2015, 2019, "2015-2019"),
                    (2020, 2024, "2020-2024"),
                ]

                for start_year, end_year, label in year_groups:
                    group_data = recent_years[
                        (recent_years[year_col] >= start_year)
                        & (recent_years[year_col] <= end_year)
                    ]

                    if len(group_data) > 0:
                        avg_rating = group_data[rating_col].mean()
                        movie_count = len(group_data)
                        year_analysis.append(
                            [label, str(movie_count), f"{avg_rating:.2f}"]
                        )

                display_table(
                    "ì—°ë„ë³„ ë¶„ì„ (2000ë…„ ì´í›„)",
                    ["ê¸°ê°„", "ì˜í™” ìˆ˜", "í‰ê·  í‰ì "],
                    year_analysis,
                )

        # ì¥ë¥´ ë¶„ì„ (ì¥ë¥´ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
        genre_columns = [col for col in df.columns if col.startswith("genre_")]
        if genre_columns:
            genre_analysis = []

            for genre_col in genre_columns[:10]:  # ìƒìœ„ 10ê°œ ì¥ë¥´ë§Œ
                genre_name = genre_col.replace("genre_", "").title()
                genre_movies = df[df[genre_col] == 1]

                if len(genre_movies) > 0:
                    avg_rating = genre_movies[rating_col].mean()
                    movie_count = len(genre_movies)
                    genre_analysis.append(
                        [genre_name, str(movie_count), f"{avg_rating:.2f}"]
                    )

            # í‰ì  ìˆœìœ¼ë¡œ ì •ë ¬
            genre_analysis.sort(key=lambda x: float(x[2]), reverse=True)

            display_table(
                "ì¥ë¥´ë³„ ë¶„ì„ (ìƒìœ„ 10ê°œ)",
                ["ì¥ë¥´", "ì˜í™” ìˆ˜", "í‰ê·  í‰ì "],
                genre_analysis[:10],
            )

        # ìƒê´€ê´€ê³„ ë¶„ì„
        if len(numeric_columns) > 1:
            correlation_data = []
            target_col = rating_col

            for col in numeric_columns:
                if col != target_col and col in df.columns:
                    corr = df[col].corr(df[target_col])
                    if not np.isnan(corr):
                        correlation_data.append([col, f"{corr:.3f}"])

            # ìƒê´€ê´€ê³„ ì ˆëŒ“ê°’ìœ¼ë¡œ ì •ë ¬
            correlation_data.sort(key=lambda x: abs(float(x[1])), reverse=True)

            display_table(
                f"{target_col}ê³¼ì˜ ìƒê´€ê´€ê³„", ["í”¼ì²˜", "ìƒê´€ê³„ìˆ˜"], correlation_data
            )

        logger.success("ë°ì´í„° ë¶„ì„ ì™„ë£Œ!")

        return {
            "total_movies": len(df),
            "numeric_columns": list(numeric_columns),
            "genre_columns": genre_columns,
            "analysis_complete": True,
        }

    except Exception as e:
        logger.error(f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
        ic(e)
        return None


# CLI ëª…ë ¹ì–´ í•¨ìˆ˜ë“¤
def enhanced_quick_train():
    """ë¹ ë¥¸ í›ˆë ¨ì„ ìœ„í•œ ê°„ì†Œí™”ëœ í•¨ìˆ˜"""
    logger = EnhancedLogger("ë¹ ë¥¸í›ˆë ¨")
    logger.info("âš¡ ë¹ ë¥¸ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")

    try:
        # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ë¹ ë¥¸ í›ˆë ¨
        model_info = enhanced_training_pipeline(model_type="random_forest")

        enhanced_print("âš¡ [bold green]ë¹ ë¥¸ í›ˆë ¨ ì™„ë£Œ![/bold green]")
        return model_info

    except Exception as e:
        logger.error(f"ë¹ ë¥¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
        return None


def enhanced_full_pipeline():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    logger = EnhancedLogger("ì „ì²´íŒŒì´í”„ë¼ì¸")
    logger.info("ğŸš€ ì „ì²´ MLOps íŒŒì´í”„ë¼ì¸ ì‹œì‘")

    results = {}

    try:
        # 1. ë°ì´í„° ë¶„ì„
        logger.info("1ï¸âƒ£ ë°ì´í„° ë¶„ì„ ë‹¨ê³„")
        analysis_results = enhanced_data_analysis()
        results["data_analysis"] = analysis_results

        # 2. ëª¨ë¸ ë¹„êµ
        logger.info("2ï¸âƒ£ ëª¨ë¸ ë¹„êµ ë‹¨ê³„")
        comparison_results = enhanced_model_comparison()
        results["model_comparison"] = comparison_results

        # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        logger.info("3ï¸âƒ£ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë‹¨ê³„")
        tuning_results = enhanced_hyperparameter_tuning()
        results["hyperparameter_tuning"] = tuning_results

        # ìµœì¢… ìš”ì•½
        enhanced_print("\nğŸ‰ [bold green]ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ![/bold green]")

        display_table(
            "íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìš”ì•½",
            ["ë‹¨ê³„", "ìƒíƒœ", "ê²°ê³¼"],
            [
                [
                    "ë°ì´í„° ë¶„ì„",
                    "âœ… ì™„ë£Œ",
                    (
                        f"{analysis_results['total_movies']:,}ê°œ ì˜í™” ë¶„ì„"
                        if analysis_results
                        else "ì‹¤íŒ¨"
                    ),
                ],
                [
                    "ëª¨ë¸ ë¹„êµ",
                    "âœ… ì™„ë£Œ",
                    (
                        f"{len(comparison_results)}ê°œ ëª¨ë¸ ë¹„êµ"
                        if comparison_results
                        else "ì‹¤íŒ¨"
                    ),
                ],
                [
                    "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹",
                    "âœ… ì™„ë£Œ",
                    (
                        f"ìµœì  RMSE: {tuning_results['best_score']:.4f}"
                        if tuning_results
                        else "ì‹¤íŒ¨"
                    ),
                ],
            ],
        )

        logger.success("ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        return results

    except Exception as e:
        logger.error(f"ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        ic(e)
        return results


def enhanced_model_info(model_path: str):
    """ì €ì¥ëœ ëª¨ë¸ ì •ë³´ í‘œì‹œ"""
    logger = EnhancedLogger("ëª¨ë¸ì •ë³´")

    try:
        if not Path(model_path).exists():
            logger.error(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            return None

        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(model_path)

        if isinstance(model_data, dict):
            # í–¥ìƒëœ ëª¨ë¸ í˜•ì‹
            model_info_data = []

            for key, value in model_data.items():
                if key == "model":
                    model_info_data.append(["ëª¨ë¸ íƒ€ì…", type(value).__name__])
                elif key == "feature_names":
                    model_info_data.append(["í”¼ì²˜ ìˆ˜", str(len(value))])
                    model_info_data.append(["í”¼ì²˜ ëª©ë¡", ", ".join(value)])
                else:
                    model_info_data.append([key, str(value)])

            display_table("ëª¨ë¸ ì •ë³´", ["ì†ì„±", "ê°’"], model_info_data)

            # ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ (ìˆëŠ” ê²½ìš°)
            if "model" in model_data and hasattr(model_data["model"], "get_params"):
                params = model_data["model"].get_params()
                param_data = [[k, str(v)] for k, v in params.items()]

                display_table(
                    "ëª¨ë¸ ë§¤ê°œë³€ìˆ˜", ["ë§¤ê°œë³€ìˆ˜", "ê°’"], param_data[:10]
                )  # ìƒìœ„ 10ê°œë§Œ

        else:
            # ê¸°ë³¸ ëª¨ë¸ í˜•ì‹
            logger.info(f"ê¸°ë³¸ ëª¨ë¸ í˜•ì‹: {type(model_data).__name__}")
            ic(type(model_data))

        logger.success("ëª¨ë¸ ì •ë³´ í‘œì‹œ ì™„ë£Œ")
        return model_data

    except Exception as e:
        logger.error(f"ëª¨ë¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        ic(e)
        return None


def enhanced_predict_single(
    title: str,
    year: int = 2020,
    runtime: int = 120,
    votes: int = 5000,
    model_path: str = None,
):
    """ë‹¨ì¼ ì˜í™” ì˜ˆì¸¡"""
    logger = EnhancedLogger("ë‹¨ì¼ì˜ˆì¸¡")
    logger.info(f"ì˜í™” '{title}' í‰ì  ì˜ˆì¸¡ ì¤‘...")

    try:
        # ëª¨ë¸ ê²½ë¡œ ìë™ ì°¾ê¸°
        if model_path is None:
            models_dir = Path("models")
            model_files = list(models_dir.glob("enhanced_*.joblib"))

            if not model_files:
                logger.error("ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return None

            # ê°€ì¥ ìµœê·¼ ëª¨ë¸ ì‚¬ìš©
            model_path = max(model_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ìµœê·¼ ëª¨ë¸ ì‚¬ìš©: {model_path.name}")

        # ëª¨ë¸ ë¡œë“œ
        model_data = joblib.load(model_path)

        if isinstance(model_data, dict):
            model = model_data["model"]
            feature_names = model_data.get(
                "feature_names", ["startYear", "runtimeMinutes", "numVotes"]
            )
        else:
            model = model_data
            feature_names = ["startYear", "runtimeMinutes", "numVotes"]

        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        input_data = {"startYear": year, "runtimeMinutes": runtime, "numVotes": votes}

        # í”¼ì²˜ ë²¡í„° ìƒì„±
        feature_vector = []
        for feature in feature_names:
            if feature in input_data:
                feature_vector.append(input_data[feature])
            else:
                feature_vector.append(0)  # ê¸°ë³¸ê°’

        feature_vector = np.array(feature_vector).reshape(1, -1)

        # ìŠ¤ì¼€ì¼ëŸ¬ ì ìš© (ìˆëŠ” ê²½ìš°)
        scaler_path = (
            str(model_path)
            .replace("enhanced_randomforest", "enhanced_scaler")
            .replace("enhanced_linear", "enhanced_scaler")
        )
        if Path(scaler_path).exists():
            scaler = joblib.load(scaler_path)
            feature_vector = scaler.transform(feature_vector)
            logger.info("ìŠ¤ì¼€ì¼ëŸ¬ ì ìš©ë¨")

        # ì˜ˆì¸¡
        prediction = model.predict(feature_vector)[0]
        prediction = max(1.0, min(10.0, prediction))  # 1-10 ë²”ìœ„ë¡œ ì œí•œ

        # ê²°ê³¼ í‘œì‹œ
        display_table(
            f"'{title}' ì˜ˆì¸¡ ê²°ê³¼",
            ["ì†ì„±", "ê°’"],
            [
                ["ì˜í™” ì œëª©", title],
                ["ê°œë´‰ ì—°ë„", str(year)],
                ["ìƒì˜ ì‹œê°„", f"{runtime}ë¶„"],
                ["íˆ¬í‘œ ìˆ˜", f"{votes:,}"],
                ["ì˜ˆì¸¡ í‰ì ", f"{prediction:.2f}/10"],
                ["ì‚¬ìš©ëœ ëª¨ë¸", type(model).__name__],
                ["í”¼ì²˜ ìˆ˜", str(len(feature_names))],
            ],
        )

        # ì˜ˆì¸¡ ì‹ ë¢°ë„ í‘œì‹œ (RandomForestì¸ ê²½ìš°)
        if hasattr(model, "estimators_"):
            # ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ êµ¬í•´ì„œ ë¶„ì‚° ê³„ì‚°
            tree_predictions = []
            for estimator in model.estimators_[:10]:  # ì²˜ìŒ 10ê°œ íŠ¸ë¦¬ë§Œ
                tree_pred = estimator.predict(feature_vector)[0]
                tree_predictions.append(tree_pred)

            prediction_std = np.std(tree_predictions)
            confidence = max(0, 100 - (prediction_std * 50))  # ê°„ë‹¨í•œ ì‹ ë¢°ë„ ê³„ì‚°

            logger.info(f"ì˜ˆì¸¡ ì‹ ë¢°ë„: {confidence:.1f}%")
            ic(prediction_std, confidence)

        logger.success(f"ì˜ˆì¸¡ ì™„ë£Œ: {prediction:.2f}/10")
        ic(title, year, runtime, votes, prediction)

        return {
            "title": title,
            "prediction": prediction,
            "input_features": input_data,
            "model_type": type(model).__name__,
        }

    except Exception as e:
        logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        ic(e)
        return None


def enhanced_batch_predict(csv_path: str, output_path: str = None):
    """ë°°ì¹˜ ì˜ˆì¸¡ (CSV íŒŒì¼)"""
    logger = EnhancedLogger("ë°°ì¹˜ì˜ˆì¸¡")
    logger.info(f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹œì‘: {csv_path}")

    try:
        # ì…ë ¥ íŒŒì¼ í™•ì¸
        if not Path(csv_path).exists():
            logger.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_path}")
            return None

        # ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(csv_path)
        logger.success(f"ì…ë ¥ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ í–‰")

        # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
        required_cols = ["title", "startYear", "runtimeMinutes", "numVotes"]
        missing_cols = [col for col in required_cols if col not in df.columns]

        if missing_cols:
            logger.error(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing_cols}")
            return None

        # ëª¨ë¸ ë¡œë“œ
        models_dir = Path("models")
        model_files = list(models_dir.glob("enhanced_*.joblib"))

        if not model_files:
            logger.error("ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return None

        model_path = max(model_files, key=lambda x: x.stat().st_mtime)
        model_data = joblib.load(model_path)

        if isinstance(model_data, dict):
            model = model_data["model"]
            feature_names = model_data.get(
                "feature_names", ["startYear", "runtimeMinutes", "numVotes"]
            )
        else:
            model = model_data
            feature_names = ["startYear", "runtimeMinutes", "numVotes"]

        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {type(model).__name__}")

        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        scaler = None
        scaler_path = (
            str(model_path)
            .replace("enhanced_randomforest", "enhanced_scaler")
            .replace("enhanced_linear", "enhanced_scaler")
        )
        if Path(scaler_path).exists():
            scaler = joblib.load(scaler_path)
            logger.info("ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ ì™„ë£Œ")

        # ë°°ì¹˜ ì˜ˆì¸¡
        predictions = []

        for idx, row in self.progress.track(df.iterrows(), "ì˜ˆì¸¡ ì¤‘", total=len(df)):
            try:
                # í”¼ì²˜ ë²¡í„° ìƒì„±
                feature_vector = []
                for feature in feature_names:
                    if feature in row:
                        feature_vector.append(row[feature])
                    else:
                        feature_vector.append(0)

                feature_vector = np.array(feature_vector).reshape(1, -1)

                # ìŠ¤ì¼€ì¼ë§ ì ìš©
                if scaler:
                    feature_vector = scaler.transform(feature_vector)

                # ì˜ˆì¸¡
                prediction = model.predict(feature_vector)[0]
                prediction = max(1.0, min(10.0, prediction))
                predictions.append(prediction)

            except Exception as e:
                logger.warning(f"í–‰ {idx} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                predictions.append(np.nan)

        # ê²°ê³¼ ì¶”ê°€
        df["predicted_rating"] = predictions

        # ì¶œë ¥ íŒŒì¼ ì €ì¥
        if output_path is None:
            output_path = csv_path.replace(".csv", "_predictions.csv")

        df.to_csv(output_path, index=False)

        # ê²°ê³¼ ìš”ì•½
        valid_predictions = df["predicted_rating"].dropna()

        display_table(
            "ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼",
            ["ì§€í‘œ", "ê°’"],
            [
                ["ì´ ì…ë ¥ í–‰", str(len(df))],
                ["ì„±ê³µí•œ ì˜ˆì¸¡", str(len(valid_predictions))],
                ["ì‹¤íŒ¨í•œ ì˜ˆì¸¡", str(len(df) - len(valid_predictions))],
                ["í‰ê·  ì˜ˆì¸¡ í‰ì ", f"{valid_predictions.mean():.2f}"],
                [
                    "ì˜ˆì¸¡ ë²”ìœ„",
                    f"{valid_predictions.min():.2f} - {valid_predictions.max():.2f}",
                ],
                ["ì¶œë ¥ íŒŒì¼", output_path],
            ],
        )

        logger.success(f"ë°°ì¹˜ ì˜ˆì¸¡ ì™„ë£Œ: {output_path}")
        return output_path

    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
        ic(e)
        return None


# í–¥ìƒëœ CLI í•¨ìˆ˜ ë”•ì…”ë„ˆë¦¬ ì—…ë°ì´íŠ¸
ENHANCED_CLI_FUNCTIONS = {
    # ê¸°ë³¸ í›ˆë ¨ í•¨ìˆ˜ë“¤
    "train": enhanced_training_pipeline,
    "quick_train": enhanced_quick_train,
    "full_pipeline": enhanced_full_pipeline,
    # ë¶„ì„ ë° ë¹„êµ
    "analyze": enhanced_data_analysis,
    "compare": enhanced_model_comparison,
    "tune": enhanced_hyperparameter_tuning,
    # ì˜ˆì¸¡ í•¨ìˆ˜ë“¤
    "predict": enhanced_predict_single,
    "batch_predict": enhanced_batch_predict,
    # ëª¨ë¸ ê´€ë¦¬
    "model_info": enhanced_model_info,
    # ìœ í‹¸ë¦¬í‹°
    "demo": demo_enhanced_features,
    "help": show_enhanced_help,
}


def show_enhanced_help():
    """í–¥ìƒëœ ë„ì›€ë§ í‘œì‹œ"""
    logger = EnhancedLogger("ë„ì›€ë§")

    enhanced_print("\nğŸ¬ [bold blue]Enhanced IMDB Movie Rating Trainer[/bold blue]")
    enhanced_print("í–¥ìƒëœ UXì™€ ë””ë²„ê¹… ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” MLOps ë„êµ¬\n")

    display_table(
        "ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´",
        ["ëª…ë ¹ì–´", "ì„¤ëª…", "ì˜ˆì œ"],
        [
            [
                "train",
                "ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨",
                "python trainer.py train --model_type=random_forest",
            ],
            ["quick_train", "ë¹ ë¥¸ í›ˆë ¨ (ê¸°ë³¸ ì„¤ì •)", "python trainer.py quick_train"],
            [
                "full_pipeline",
                "ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰",
                "python trainer.py full_pipeline",
            ],
            ["analyze", "ë°ì´í„° ë¶„ì„", "python trainer.py analyze"],
            ["compare", "ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", "python trainer.py compare"],
            ["tune", "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹", "python trainer.py tune"],
            [
                "predict",
                "ë‹¨ì¼ ì˜í™” ì˜ˆì¸¡",
                "python trainer.py predict --title='ì˜í™”ì œëª©' --year=2020",
            ],
            [
                "batch_predict",
                "ë°°ì¹˜ ì˜ˆì¸¡",
                "python trainer.py batch_predict --csv_path=movies.csv",
            ],
            [
                "model_info",
                "ëª¨ë¸ ì •ë³´ í‘œì‹œ",
                "python trainer.py model_info --model_path=models/model.joblib",
            ],
            ["demo", "ê¸°ëŠ¥ ë°ëª¨", "python trainer.py demo"],
        ],
    )

    enhanced_print("\nğŸ“Š [bold green]ì£¼ìš” ê¸°ëŠ¥:[/bold green]")
    enhanced_print("â€¢ ğŸ› í–¥ìƒëœ ë””ë²„ê¹… (icecream)")
    enhanced_print("â€¢ ğŸ“Š ì§„í–‰ë¥  í‘œì‹œ (tqdm, rich)")
    enhanced_print("â€¢ ğŸ¨ ì•„ë¦„ë‹¤ìš´ ì¶œë ¥ (rich)")
    enhanced_print("â€¢ ğŸ”¥ CLI ì¸í„°í˜ì´ìŠ¤ (fire)")
    enhanced_print("â€¢ ğŸ“ˆ MLflow í†µí•©")
    enhanced_print("â€¢ ğŸ” ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
    enhanced_print("â€¢ ğŸ“‹ ìƒì„¸í•œ ëª¨ë¸ ë¹„êµ")


def enhanced_system_check():
    """ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸"""
    logger = EnhancedLogger("ì‹œìŠ¤í…œì²´í¬")
    logger.info("ì‹œìŠ¤í…œ í™˜ê²½ í™•ì¸ ì¤‘...")

    checks = []

    # Python ë²„ì „ í™•ì¸
    python_version = sys.version.split()[0]
    python_ok = python_version >= "3.8"
    checks.append(["Python ë²„ì „", python_version, "âœ…" if python_ok else "âŒ"])

    # í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = {
        "pandas": "ë°ì´í„° ì²˜ë¦¬",
        "numpy": "ìˆ˜ì¹˜ ê³„ì‚°",
        "scikit-learn": "ë¨¸ì‹ ëŸ¬ë‹",
        "mlflow": "ì‹¤í—˜ ì¶”ì ",
        "joblib": "ëª¨ë¸ ì €ì¥",
    }

    for package, description in required_packages.items():
        try:
            __import__(package)
            checks.append([f"{package}", description, "âœ…"])
        except ImportError:
            checks.append([f"{package}", description, "âŒ"])

    # í–¥ìƒëœ íŒ¨í‚¤ì§€ í™•ì¸
    enhanced_packages = {
        "icecream": "ë””ë²„ê¹…",
        "tqdm": "ì§„í–‰ë¥  í‘œì‹œ",
        "rich": "ì•„ë¦„ë‹¤ìš´ ì¶œë ¥",
        "fire": "CLI ì¸í„°í˜ì´ìŠ¤",
    }

    for package, description in enhanced_packages.items():
        try:
            __import__(package)
            checks.append([f"{package} (í–¥ìƒ)", description, "âœ…"])
        except ImportError:
            checks.append([f"{package} (í–¥ìƒ)", description, "âš ï¸"])

    # ë””ë ‰í† ë¦¬ í™•ì¸
    directories = ["models", "data", "logs"]
    for directory in directories:
        dir_path = Path(directory)
        exists = dir_path.exists()
        checks.append([f"{directory}/ ë””ë ‰í† ë¦¬", "íŒŒì¼ ì €ì¥", "âœ…" if exists else "âŒ"])

        if not exists:
            logger.info(f"{directory} ë””ë ‰í† ë¦¬ ìƒì„± ì¤‘...")
            dir_path.mkdir(exist_ok=True)

    display_table("ì‹œìŠ¤í…œ í™˜ê²½ ì²´í¬", ["êµ¬ì„±ìš”ì†Œ", "ì„¤ëª…", "ìƒíƒœ"], checks)

    # ê¶Œì¥ì‚¬í•­
    missing_enhanced = [
        pkg
        for pkg in enhanced_packages.keys()
        if not any(check[0].startswith(pkg) and check[2] == "âœ…" for check in checks)
    ]

    if missing_enhanced:
        enhanced_print(f"\nğŸ’¡ [yellow]ê¶Œì¥ì‚¬í•­:[/yellow]")
        enhanced_print(f"í–¥ìƒëœ ê¸°ëŠ¥ì„ ìœ„í•´ ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
        enhanced_print(f"pip install {' '.join(missing_enhanced)}")

    logger.success("ì‹œìŠ¤í…œ ì²´í¬ ì™„ë£Œ")


def enhanced_cleanup():
    """ì˜¤ë˜ëœ ëª¨ë¸ íŒŒì¼ ì •ë¦¬"""
    logger = EnhancedLogger("ì •ë¦¬")
    logger.info("ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬ ì¤‘...")

    try:
        models_dir = Path("models")
        if not models_dir.exists():
            logger.info("models ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # ëª¨ë“  ëª¨ë¸ íŒŒì¼ ì°¾ê¸°
        model_files = list(models_dir.glob("*.joblib")) + list(models_dir.glob("*.pkl"))

        if not model_files:
            logger.info("ì •ë¦¬í•  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return

        # íŒŒì¼ì„ ìˆ˜ì • ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬
        model_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)

        # ìµœì‹  5ê°œ íŒŒì¼ ìœ ì§€, ë‚˜ë¨¸ì§€ ì‚­ì œ
        keep_count = 5
        files_to_delete = model_files[keep_count:]

        if not files_to_delete:
            logger.info(f"ëª¨ë“  íŒŒì¼ì´ ìµœì‹ ì…ë‹ˆë‹¤ ({len(model_files)}ê°œ íŒŒì¼)")
            return

        # ì‚­ì œí•  íŒŒì¼ ëª©ë¡ í‘œì‹œ
        delete_info = []
        total_size = 0

        for file_path in files_to_delete:
            size = file_path.stat().st_size
            total_size += size
            modified = datetime.fromtimestamp(file_path.stat().st_mtime)
            delete_info.append(
                [
                    file_path.name,
                    f"{size / 1024**2:.1f} MB",
                    modified.strftime("%Y-%m-%d %H:%M"),
                ]
            )

        display_table(
            f"ì‚­ì œí•  íŒŒì¼ ({len(files_to_delete)}ê°œ)",
            ["íŒŒì¼ëª…", "í¬ê¸°", "ìˆ˜ì •ì¼"],
            delete_info,
        )

        # ì‚¬ìš©ì í™•ì¸ (CLIì—ì„œ)
        enhanced_print(f"\nì´ {total_size / 1024**2:.1f} MBë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        # íŒŒì¼ ì‚­ì œ
        deleted_count = 0
        for file_path in self.progress.track(files_to_delete, "íŒŒì¼ ì‚­ì œ"):
            try:
                file_path.unlink()
                deleted_count += 1
                ic(f"ì‚­ì œë¨: {file_path.name}")
            except Exception as e:
                logger.warning(f"ì‚­ì œ ì‹¤íŒ¨: {file_path.name} - {e}")

        logger.success(
            f"{deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ ({total_size / 1024**2:.1f} MB ì ˆì•½)"
        )

        # ë‚¨ì€ íŒŒì¼ í‘œì‹œ
        remaining_files = model_files[:keep_count]
        if remaining_files:
            remaining_info = []
            for file_path in remaining_files:
                size = file_path.stat().st_size
                modified = datetime.fromtimestamp(file_path.stat().st_mtime)
                remaining_info.append(
                    [
                        file_path.name,
                        f"{size / 1024**2:.1f} MB",
                        modified.strftime("%Y-%m-%d %H:%M"),
                    ]
                )

            display_table(
                f"ìœ ì§€ëœ íŒŒì¼ ({len(remaining_files)}ê°œ)",
                ["íŒŒì¼ëª…", "í¬ê¸°", "ìˆ˜ì •ì¼"],
                remaining_info,
            )

    except Exception as e:
        logger.error(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        ic(e)


def enhanced_export_results(output_dir: str = "results"):
    """ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
    logger = EnhancedLogger("ê²°ê³¼ë‚´ë³´ë‚´ê¸°")
    logger.info(f"ê²°ê³¼ë¥¼ {output_dir}ì— ë‚´ë³´ë‚´ëŠ” ì¤‘...")

    try:
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)

        # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
        models_dir = Path("models")
        model_files = list(models_dir.glob("enhanced_*.joblib"))

        if not model_files:
            logger.warning("ë‚´ë³´ë‚¼ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤")
            return

        # ìµœì‹  ëª¨ë¸ ì„ íƒ
        latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
        model_data = joblib.load(latest_model)

        # ëª¨ë¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        if isinstance(model_data, dict):
            export_data = {
                "model_type": model_data.get("model_type", "Unknown"),
                "feature_names": model_data.get("feature_names", []),
                "timestamp": model_data.get("timestamp", "Unknown"),
                "version": model_data.get("version", "1.0"),
                "enhanced": model_data.get("enhanced", False),
            }

            # ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€
            if "model" in model_data and hasattr(model_data["model"], "get_params"):
                export_data["model_params"] = model_data["model"].get_params()

            # JSON íŒŒì¼ë¡œ ì €ì¥
            import json

            json_path = output_path / "model_info.json"
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(export_data, f, indent=2, ensure_ascii=False, default=str)

            logger.success(f"ëª¨ë¸ ì •ë³´ ì €ì¥: {json_path}")

        # ì‹¤í–‰ ìš”ì•½ ìƒì„±
        summary_data = {
            "execution_time": datetime.now().isoformat(),
            "python_version": sys.version.split()[0],
            "enhanced_features": {
                "icecream": HAS_ICECREAM,
                "tqdm": HAS_TQDM,
                "rich": HAS_RICH,
                "fire": HAS_FIRE,
            },
            "model_file": latest_model.name,
            "total_models": len(model_files),
        }

        summary_path = output_path / "execution_summary.json"
        with open(summary_path, "w", encoding="utf-8") as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)

        logger.success(f"ì‹¤í–‰ ìš”ì•½ ì €ì¥: {summary_path}")

        # README íŒŒì¼ ìƒì„±
        readme_content = f"""# MLOps IMDB Movie Rating - ì‹¤í–‰ ê²°ê³¼

    ## ëª¨ë¸ ì •ë³´
    - **ëª¨ë¸ íƒ€ì…**: {export_data.get('model_type', 'Unknown')}
    - **ìƒì„± ì‹œê°„**: {export_data.get('timestamp', 'Unknown')}
    - **í”¼ì²˜ ìˆ˜**: {len(export_data.get('feature_names', []))}
    - **í–¥ìƒëœ ë²„ì „**: {export_data.get('enhanced', False)}

    ## í”¼ì²˜ ëª©ë¡
    {chr(10).join(f"- {feature}" for feature in export_data.get('feature_names', []))}

    ## íŒŒì¼ ëª©ë¡
    - `model_info.json`: ëª¨ë¸ ìƒì„¸ ì •ë³´
    - `execution_summary.json`: ì‹¤í–‰ í™˜ê²½ ìš”ì•½
    - `README.md`: ì´ íŒŒì¼

    ## ì‚¬ìš©ë²•
    ```python
    import joblib
    model_data = joblib.load('{latest_model.name}')
    model = model_data['model']
    feature_names = model_data['feature_names']
    ```

    ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
    """

        readme_path = output_path / "README.md"
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(readme_content)

        logger.success(f"README ìƒì„±: {readme_path}")

        # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
        display_table(
            "ë‚´ë³´ë‚´ê¸° ê²°ê³¼",
            ["íŒŒì¼", "ì„¤ëª…", "í¬ê¸°"],
            [
                [
                    "model_info.json",
                    "ëª¨ë¸ ìƒì„¸ ì •ë³´",
                    f"{json_path.stat().st_size} bytes",
                ],
                [
                    "execution_summary.json",
                    "ì‹¤í–‰ í™˜ê²½ ìš”ì•½",
                    f"{summary_path.stat().st_size} bytes",
                ],
                ["README.md", "ì‚¬ìš©ë²• ì•ˆë‚´", f"{readme_path.stat().st_size} bytes"],
            ],
        )

        logger.success(f"ëª¨ë“  ê²°ê³¼ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
        return str(output_path)

    except Exception as e:
        logger.error(f"ê²°ê³¼ ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
        ic(e)
        return None


# CLI í•¨ìˆ˜ ë”•ì…”ë„ˆë¦¬ì— ìƒˆ í•¨ìˆ˜ë“¤ ì¶”ê°€
ENHANCED_CLI_FUNCTIONS.update(
    {
        "system_check": enhanced_system_check,
        "cleanup": enhanced_cleanup,
        "export": enhanced_export_results,
    }
)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if HAS_FIRE:
        enhanced_print("ğŸ¬ [bold blue]Enhanced IMDB Movie Rating Trainer[/bold blue]")
        enhanced_print("í–¥ìƒëœ MLOps ë„êµ¬ ì‹œì‘\n")

        # ì‹œìŠ¤í…œ ì²´í¬ ì‹¤í–‰
        enhanced_system_check()

        fire.Fire(ENHANCED_CLI_FUNCTIONS)
    else:
        print("âŒ Fireë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í–¥ìƒëœ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install fire icecream tqdm rich")

        # ê¸°ë³¸ í›ˆë ¨ ì‹¤í–‰
        enhanced_training_pipeline()


if __name__ == "__main__":
    main()


## Summary of what's left:

# The code is now **complete**. Here's what we've covered:

# ### Main Components (ì´ ~500ì¤„):
# 1. **EnhancedMovieRatingTrainer í´ë˜ìŠ¤** (~200ì¤„)
#    - ë°ì´í„° ë¡œë”©, í”¼ì²˜ ì¤€ë¹„, ëª¨ë¸ í›ˆë ¨, ì €ì¥
# 2. **íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤** (~150ì¤„)
#    - ì „ì²´ íŒŒì´í”„ë¼ì¸, ëª¨ë¸ ë¹„êµ, í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# 3. **ì˜ˆì¸¡ í•¨ìˆ˜ë“¤** (~100ì¤„)
#    - ë‹¨ì¼ ì˜ˆì¸¡, ë°°ì¹˜ ì˜ˆì¸¡
# 4. **ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤** (~50ì¤„)
#    - ì‹œìŠ¤í…œ ì²´í¬, ì •ë¦¬, ë‚´ë³´ë‚´ê¸°, CLI ì„¤ì •

# ### Key Features Added:
# - âœ… **icecream** ë””ë²„ê¹…
# - âœ… **tqdm/rich** ì§„í–‰ë¥  í‘œì‹œ
# - âœ… **fire** CLI ì¸í„°í˜ì´ìŠ¤
# - âœ… **rich** ì•„ë¦„ë‹¤ìš´ ì¶œë ¥
# - âœ… í–¥ìƒëœ ì—ëŸ¬ ì²˜ë¦¬
# - âœ… ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# - âœ… ëª¨ë¸ ë¹„êµ ë° ë¶„ì„
# - âœ… ë°°ì¹˜ ì˜ˆì¸¡ ê¸°ëŠ¥

# The code is now **production-ready** with enhanced UX! Would you like me to:
# 1. **Create a summary/overview** of all functions?
# 2. **Split into multiple files** for better organization?
# 3. **Add specific features** you'd like to see?

# ğŸ“ File Structure & Organization
# enhanced_trainer.py
# â”œâ”€â”€ ğŸ“¦ Imports & Setup
# â”œâ”€â”€ ğŸ—ï¸ EnhancedMovieRatingTrainer í´ë˜ìŠ¤ (~200ì¤„)
# â”‚   â”œâ”€â”€ __init__()
# â”‚   â”œâ”€â”€ load_data()
# â”‚   â”œâ”€â”€ prepare_features()
# â”‚   â”œâ”€â”€ train_model()
# â”‚   â””â”€â”€ save_model()
# â”œâ”€â”€ ğŸ”§ Pipeline Functions (í´ë˜ìŠ¤ ì™¸ë¶€, ~150ì¤„)
# â”‚   â”œâ”€â”€ enhanced_training_pipeline()
# â”‚   â”œâ”€â”€ enhanced_model_comparison()
# â”‚   â”œâ”€â”€ enhanced_hyperparameter_tuning()
# â”‚   â””â”€â”€ enhanced_data_analysis()
# â”œâ”€â”€ ğŸ¯ Prediction Functions (í´ë˜ìŠ¤ ì™¸ë¶€, ~100ì¤„)
# â”‚   â”œâ”€â”€ enhanced_predict_single()
# â”‚   â””â”€â”€ enhanced_batch_predict()
# â”œâ”€â”€ ğŸ› ï¸ Utility Functions (í´ë˜ìŠ¤ ì™¸ë¶€, ~50ì¤„)
# â”‚   â”œâ”€â”€ enhanced_system_check()
# â”‚   â”œâ”€â”€ enhanced_cleanup()
# â”‚   â”œâ”€â”€ enhanced_export_results()
# â”‚   â””â”€â”€ show_enhanced_help()
# â”œâ”€â”€ ğŸ“‹ CLI Configuration
# â”‚   â””â”€â”€ ENHANCED_CLI_FUNCTIONS = {...}
# â””â”€â”€ ğŸš€ main()
