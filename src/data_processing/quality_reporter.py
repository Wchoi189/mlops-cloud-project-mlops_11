# src/data_processing/quality_reporter.py
"""
ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œ
"""

import json
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_processing.quality_validator import DataQualityValidator, AnomalyDetector, DataCleaner
from data.file_formats import DataFileManager

class QualityReporter:
    """ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.validator = DataQualityValidator()
        self.anomaly_detector = AnomalyDetector()
        self.data_cleaner = DataCleaner()
        self.file_manager = DataFileManager()
        self.logger = logging.getLogger(__name__)
        
        # ê¸°ì¤€ì„  ë¡œë“œ ì‹œë„
        baseline_file = Path("data/quality_baseline.json")
        if baseline_file.exists():
            self.anomaly_detector.load_baseline(str(baseline_file))
    
    def generate_daily_report(self, date_str: Optional[str] = None) -> Dict[str, Any]:
        """ì¼ê°„ í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        
        self.logger.info(f"ì¼ê°„ í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘: {date_str}")
        
        # í•´ë‹¹ ë‚ ì§œ ë°ì´í„° ë¡œë“œ
        daily_files = self._find_daily_files(date_str)
        if not daily_files:
            return {'status': 'no_data', 'date': date_str}
        
        all_movies = []
        collection_stats = {}
        
        for file_path in daily_files:
            try:
                data = self.file_manager.load_data(file_path)
                if data and isinstance(data, dict):
                    if 'movies' in data:
                        all_movies.extend(data['movies'])
                    if 'collection_info' in data:
                        collection_stats.update(data['collection_info'])
            except Exception as e:
                self.logger.error(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")
                continue
        
        if not all_movies:
            return {'status': 'no_movies', 'date': date_str}
        
        # í’ˆì§ˆ ë¶„ì„ ì‹¤í–‰
        batch_results = self.validator.validate_batch_data(all_movies)
        anomaly_results = self.anomaly_detector.detect_rating_anomalies(all_movies)
        popularity_anomalies = self.anomaly_detector.detect_popularity_anomalies(all_movies)
        collection_anomalies = self.anomaly_detector.detect_collection_anomalies(collection_stats)
        
        # ë°ì´í„° ì •ì œ ë¶„ì„
        cleaned_movies, cleaning_report = self.data_cleaner.apply_batch_cleaning(all_movies)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = {
            'report_date': date_str,
            'generation_time': datetime.now().isoformat(),
            'data_summary': {
                'total_files_processed': len(daily_files),
                'total_movies_analyzed': len(all_movies),
                'unique_movies': len(set(m.get('id') for m in all_movies if m.get('id'))),
                'collection_stats': collection_stats
            },
            'quality_summary': {
                'overall_quality_score': self._calculate_overall_quality(batch_results),
                'valid_rate': batch_results['valid_movies'] / batch_results['total_movies'] * 100 if batch_results['total_movies'] > 0 else 0,
                'quality_distribution': batch_results['quality_distribution'],
                'common_issues': batch_results['common_issues'],
                'recommendations': batch_results['recommendations']
            },
            'anomaly_analysis': {
                'rating_anomalies': anomaly_results.get('anomalies', []),
                'popularity_anomalies': popularity_anomalies.get('anomalies', []),
                'collection_anomalies': collection_anomalies.get('anomalies', []),
                'total_anomalies': len(anomaly_results.get('anomalies', [])) + 
                                  len(popularity_anomalies.get('anomalies', [])) + 
                                  len(collection_anomalies.get('anomalies', []))
            },
            'cleaning_analysis': {
                'cleaning_report': cleaning_report,
                'cleanable_movies': len(cleaned_movies),
                'cleaning_success_rate': (cleaning_report['successfully_cleaned'] / cleaning_report['total_processed'] * 100) if cleaning_report['total_processed'] > 0 else 0
            },
            'data_health': self._assess_data_health(batch_results, anomaly_results, popularity_anomalies, collection_anomalies)
        }
        
        # ê¸°ì¤€ì„  ì—…ë°ì´íŠ¸
        self.anomaly_detector.update_baseline(all_movies, collection_stats)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        self._save_report(report, date_str)
        
        return report
    
    def generate_weekly_summary(self, week_start_date: str) -> Dict[str, Any]:
        """ì£¼ê°„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        self.logger.info(f"ì£¼ê°„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±: {week_start_date}")
        
        # ì£¼ê°„ ë‚ ì§œ ë²”ìœ„ ê³„ì‚°
        start_dt = datetime.strptime(week_start_date, '%Y%m%d')
        week_dates = [(start_dt + timedelta(days=i)).strftime('%Y%m%d') for i in range(7)]
        
        weekly_data = {
            'week_start': week_start_date,
            'daily_reports': [],
            'weekly_summary': {
                'total_movies': 0,
                'total_valid': 0,
                'average_quality_score': 0,
                'weekly_trends': {},
                'issues_summary': {}
            }
        }
        
        daily_scores = []
        all_issues = []
        
        for date_str in week_dates:
            daily_report = self.generate_daily_report(date_str)
            if daily_report.get('status') in ['no_data', 'no_movies']:
                continue
            
            weekly_data['daily_reports'].append(daily_report)
            weekly_data['weekly_summary']['total_movies'] += daily_report['data_summary']['total_movies_analyzed']
            weekly_data['weekly_summary']['total_valid'] += daily_report['quality_summary']['valid_rate'] * daily_report['data_summary']['total_movies_analyzed'] / 100
            
            daily_scores.append(daily_report['quality_summary']['overall_quality_score'])
            all_issues.extend(daily_report['quality_summary']['common_issues'].keys())
        
        if daily_scores:
            weekly_data['weekly_summary']['average_quality_score'] = sum(daily_scores) / len(daily_scores)
            
            # ì£¼ê°„ íŠ¸ë Œë“œ ë¶„ì„
            weekly_data['weekly_summary']['weekly_trends'] = {
                'quality_trend': 'improving' if daily_scores[-1] > daily_scores[0] else 'declining' if daily_scores[-1] < daily_scores[0] else 'stable',
                'score_variance': max(daily_scores) - min(daily_scores),
                'best_day': week_dates[daily_scores.index(max(daily_scores))],
                'worst_day': week_dates[daily_scores.index(min(daily_scores))]
            }
            
            # ì´ìŠˆ ìš”ì•½
            from collections import Counter
            issue_counts = Counter(all_issues)
            weekly_data['weekly_summary']['issues_summary'] = dict(issue_counts.most_common(5))
        
        # ì£¼ê°„ ë¦¬í¬íŠ¸ ì €ì¥
        self._save_weekly_report(weekly_data, week_start_date)
        
        return weekly_data
    
    def generate_quality_dashboard_data(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        self.logger.info("í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±")
        
        # ìµœê·¼ 7ì¼ ë°ì´í„° ìˆ˜ì§‘
        recent_dates = [(datetime.now() - timedelta(days=i)).strftime('%Y%m%d') for i in range(7)]
        
        dashboard_data = {
            'last_updated': datetime.now().isoformat(),
            'time_series': {
                'dates': [],
                'quality_scores': [],
                'movie_counts': [],
                'anomaly_counts': []
            },
            'current_status': {
                'latest_quality_score': 0,
                'total_anomalies': 0,
                'data_health_grade': 'Unknown',
                'trending_issues': []
            },
            'statistics': {
                'total_movies_processed': 0,
                'average_quality_score': 0,
                'anomaly_detection_rate': 0,
                'data_cleaning_effectiveness': 0
            }
        }
        
        scores = []
        counts = []
        anomaly_counts = []
        all_issues = []
        
        for date_str in reversed(recent_dates):  # ë‚ ì§œ ìˆœìœ¼ë¡œ ì •ë ¬
            daily_report = self.generate_daily_report(date_str)
            if daily_report.get('status') in ['no_data', 'no_movies']:
                continue
            
            dashboard_data['time_series']['dates'].append(date_str)
            
            quality_score = daily_report['quality_summary']['overall_quality_score']
            movie_count = daily_report['data_summary']['total_movies_analyzed']
            anomaly_count = daily_report['anomaly_analysis']['total_anomalies']
            
            dashboard_data['time_series']['quality_scores'].append(quality_score)
            dashboard_data['time_series']['movie_counts'].append(movie_count)
            dashboard_data['time_series']['anomaly_counts'].append(anomaly_count)
            
            scores.append(quality_score)
            counts.append(movie_count)
            anomaly_counts.append(anomaly_count)
            all_issues.extend(daily_report['quality_summary']['common_issues'].keys())
        
        if scores:
            # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
            dashboard_data['current_status'].update({
                'latest_quality_score': scores[-1],
                'total_anomalies': sum(anomaly_counts),
                'data_health_grade': self._get_health_grade(scores[-1]),
                'trending_issues': list(Counter(all_issues).most_common(3))
            })
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            dashboard_data['statistics'].update({
                'total_movies_processed': sum(counts),
                'average_quality_score': sum(scores) / len(scores),
                'anomaly_detection_rate': sum(anomaly_counts) / sum(counts) * 100 if sum(counts) > 0 else 0,
                'data_cleaning_effectiveness': 85.0  # ì„ì‹œê°’, ì‹¤ì œë¡œëŠ” ì •ì œ ì„±ê³µë¥  ê³„ì‚°
            })
        
        # ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥
        self._save_dashboard_data(dashboard_data)
        
        return dashboard_data
    
    def _calculate_overall_quality(self, batch_results: Dict[str, Any]) -> float:
        """ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        quality_dist = batch_results['quality_distribution']
        total = batch_results['total_movies']
        
        if total == 0:
            return 0
        
        weighted_score = (
            quality_dist['excellent'] * 95 +
            quality_dist['good'] * 85 +
            quality_dist['fair'] * 75 +
            quality_dist['poor'] * 50
        ) / total
        
        return round(weighted_score, 2)
    
    def _assess_data_health(self, batch_results: Dict[str, Any], rating_anomalies: Dict[str, Any], 
                           popularity_anomalies: Dict[str, Any], collection_anomalies: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ê±´ê°•ë„ í‰ê°€"""
        health_score = 100
        issues = []
        
        # í’ˆì§ˆ ë¶ˆëŸ‰ë¥  ì²´í¬
        if batch_results['total_movies'] > 0:
            invalid_rate = batch_results['invalid_movies'] / batch_results['total_movies']
            if invalid_rate > 0.2:  # 20% ì´ìƒ
                health_score -= 30
                issues.append("high_invalid_rate")
            elif invalid_rate > 0.1:  # 10% ì´ìƒ
                health_score -= 15
                issues.append("moderate_invalid_rate")
        
        # ì´ìƒ íƒì§€ ê²°ê³¼ ì²´í¬
        total_anomalies = (len(rating_anomalies.get('anomalies', [])) + 
                          len(popularity_anomalies.get('anomalies', [])) + 
                          len(collection_anomalies.get('anomalies', [])))
        
        if total_anomalies > 2:
            health_score -= 25
            issues.append("multiple_anomalies")
        elif total_anomalies > 0:
            health_score -= 10
            issues.append("minor_anomalies")
        
        # ë°ì´í„° ì™„ì „ì„± ì²´í¬
        if batch_results['total_movies'] < 50:  # ë„ˆë¬´ ì ì€ ë°ì´í„°
            health_score -= 20
            issues.append("insufficient_data")
        
        # ê±´ê°•ë„ ë“±ê¸‰ ê²°ì •
        if health_score >= 90:
            grade = "ğŸŸ¢ Excellent"
        elif health_score >= 80:
            grade = "ğŸŸ¡ Good"
        elif health_score >= 70:
            grade = "ğŸŸ  Fair"
        else:
            grade = "ğŸ”´ Poor"
        
        return {
            'health_score': max(0, health_score),
            'grade': grade,
            'issues': issues,
            'recommendations': self._generate_health_recommendations(issues)
        }
    
    def _generate_health_recommendations(self, issues: List[str]) -> List[str]:
        """ê±´ê°•ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if "high_invalid_rate" in issues:
            recommendations.append("ë°ì´í„° í’ˆì§ˆì´ ë§¤ìš° ë‚®ìŠµë‹ˆë‹¤. ìˆ˜ì§‘ ë¡œì§ê³¼ í•„í„°ë§ ê·œì¹™ì„ ì ê²€í•˜ì„¸ìš”.")
        
        if "multiple_anomalies" in issues:
            recommendations.append("ë‹¤ìˆ˜ì˜ ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì ê²€í•˜ì„¸ìš”.")
        
        if "insufficient_data" in issues:
            recommendations.append("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. ìˆ˜ì§‘ ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ê±°ë‚˜ ìˆ˜ì§‘ ë¹ˆë„ë¥¼ ë†’ì´ì„¸ìš”.")
        
        if not issues:
            recommendations.append("ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. í˜„ì¬ ìˆ˜ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”.")
        
        return recommendations
    
    def _get_health_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ê±´ê°•ë„ ë“±ê¸‰"""
        if score >= 90:
            return "ğŸŸ¢ Excellent"
        elif score >= 80:
            return "ğŸŸ¡ Good"
        elif score >= 70:
            return "ğŸŸ  Fair"
        else:
            return "ğŸ”´ Poor"
    
    def _find_daily_files(self, date_str: str) -> List[Path]:
        """í•´ë‹¹ ë‚ ì§œ ë°ì´í„° íŒŒì¼ ì°¾ê¸°"""
        data_dir = Path('data/raw/movies')
        pattern = f"*{date_str}*.json"
        
        files = []
        for subdir in ['daily', 'trending', 'genre']:
            subdir_path = data_dir / subdir
            if subdir_path.exists():
                files.extend(subdir_path.glob(pattern))
        
        return files
    
    def _save_report(self, report: Dict[str, Any], date_str: str):
        """ì¼ê°„ ë¦¬í¬íŠ¸ ì €ì¥"""
        report_dir = Path('data/raw/metadata/quality_reports')
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"daily_quality_report_{date_str}.json"
        success = self.file_manager.save_json(report, report_file)
        
        if success:
            # ìµœì‹  ë¦¬í¬íŠ¸ ë§í¬ ìƒì„±
            latest_link = report_dir / "latest_quality_report.json"
            if latest_link.exists():
                latest_link.unlink()
            
            # Windowsì—ì„œëŠ” ì‹¬ë³¼ë¦­ ë§í¬ ëŒ€ì‹  ë³µì‚¬
            import shutil
            shutil.copy2(report_file, latest_link)
            
            self.logger.info(f"ì¼ê°„ í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_file}")
    
    def _save_weekly_report(self, weekly_data: Dict[str, Any], week_start: str):
        """ì£¼ê°„ ë¦¬í¬íŠ¸ ì €ì¥"""
        report_dir = Path('data/raw/metadata/quality_reports/weekly')
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"weekly_quality_summary_{week_start}.json"
        self.file_manager.save_json(weekly_data, report_file)
        self.logger.info(f"ì£¼ê°„ í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_file}")
    
    def _save_dashboard_data(self, dashboard_data: Dict[str, Any]):
        """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥"""
        dashboard_dir = Path('data/processed/dashboard')
        dashboard_dir.mkdir(parents=True, exist_ok=True)
        
        dashboard_file = dashboard_dir / "quality_dashboard.json"
        self.file_manager.save_json(dashboard_data, dashboard_file)
        self.logger.info(f"í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {dashboard_file}")

def generate_daily_quality_report(date_str: Optional[str] = None):
    """ì¼ê°„ í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    reporter = QualityReporter()
    return reporter.generate_daily_report(date_str)

def generate_quality_dashboard():
    """í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ìƒì„± í¸ì˜ í•¨ìˆ˜"""
    reporter = QualityReporter()
    return reporter.generate_quality_dashboard_data()
