"""
TMDB ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG
ì£¼ê°„ ë‹¨ìœ„ë¡œ ì¥ë¥´ë³„, í‰ì ë³„ ì˜í™” ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìˆ˜ì§‘
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago
import sys

sys.path.append('/opt/airflow/src')

default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'catchup': False
}

dag = DAG(
    'tmdb_weekly_comprehensive',
    default_args=default_args,
    description='TMDB ì£¼ê°„ ì¢…í•© ë°ì´í„° ìˆ˜ì§‘',
    schedule_interval='0 3 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ
    max_active_runs=1,
    tags=['tmdb', 'data-collection', 'weekly', 'comprehensive'],
)

# ì£¼ìš” ì¥ë¥´ ì •ì˜
MAJOR_GENRES = {
    28: "ì•¡ì…˜",
    35: "ì½”ë¯¸ë””", 
    18: "ë“œë¼ë§ˆ",
    27: "ê³µí¬",
    10749: "ë¡œë§¨ìŠ¤",
    878: "SF",
    53: "ìŠ¤ë¦´ëŸ¬",
    16: "ì• ë‹ˆë©”ì´ì…˜"
}

def collect_genre_movies(genre_id, genre_name, **context):
    """ì¥ë¥´ë³„ ì˜í™” ìˆ˜ì§‘"""
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # ì¥ë¥´ë³„ ì˜í™” ìˆ˜ì§‘ (15í˜ì´ì§€)
        all_movies = []
        for page in range(1, 16):
            response = connector.get_movies_by_genre(genre_id, page)
            if response and 'results' in response:
                all_movies.extend(response['results'])
            else:
                break
        
        # ì¤‘ë³µ ì œê±°
        unique_movies = []
        seen_ids = set()
        for movie in all_movies:
            if movie.get('id') not in seen_ids:
                unique_movies.append(movie)
                seen_ids.add(movie.get('id'))
        
        collection_stats = {
            'collection_type': 'weekly_genre',
            'genre_id': genre_id,
            'genre_name': genre_name,
            'collection_date': context['ds'],
            'total_collected': len(unique_movies),
            'pages_processed': 15,
            'week_number': datetime.strptime(context['ds'], '%Y-%m-%d').isocalendar()[1]
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/genre')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f"{genre_name.lower()}_{context['ds_nodash']}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': unique_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… {genre_name} ì¥ë¥´ ì˜í™” {len(unique_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collection_stats
        
    except Exception as e:
        print(f"âŒ {genre_name} ì¥ë¥´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        connector.close()

def collect_top_rated_movies(**context):
    """í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘"""
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘
        all_movies = []
        for page in range(1, 21):  # 20í˜ì´ì§€
            response = connector.get_top_rated_movies(page)
            if response and 'results' in response:
                # í‰ì  7.5 ì´ìƒë§Œ í•„í„°ë§
                high_rated = [m for m in response['results'] if m.get('vote_average', 0) >= 7.5]
                all_movies.extend(high_rated)
            else:
                break
        
        collection_stats = {
            'collection_type': 'weekly_top_rated',
            'collection_date': context['ds'],
            'total_collected': len(all_movies),
            'min_rating': 7.5,
            'pages_processed': 20
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/weekly')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f"top_rated_{context['ds_nodash']}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': all_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… í‰ì  ë†’ì€ ì˜í™” {len(all_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collection_stats
        
    except Exception as e:
        print(f"âŒ í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        connector.close()

def consolidate_weekly_data(**context):
    """ì£¼ê°„ ìˆ˜ì§‘ ë°ì´í„° í†µí•©"""
    import json
    from pathlib import Path
    
    # ëª¨ë“  ì£¼ê°„ ìˆ˜ì§‘ íŒŒì¼ í†µí•©
    data_sources = [
        ('/opt/airflow/data/raw/movies/genre', 'ì¥ë¥´ë³„'),
        ('/opt/airflow/data/raw/movies/weekly', 'í‰ì ë³„')
    ]
    
    all_movies = []
    collection_summary = {
        'consolidation_date': context['ds'],
        'week_number': datetime.strptime(context['ds'], '%Y-%m-%d').isocalendar()[1],
        'sources_processed': [],
        'total_unique_movies': 0,
        'by_category': {}
    }
    
    seen_ids = set()
    
    for data_dir, category in data_sources:
        data_path = Path(data_dir)
        if data_path.exists():
            category_count = 0
            for file_path in data_path.glob(f"*{context['ds_nodash']}.json"):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    movies = data.get('movies', [])
                    
                    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
                    for movie in movies:
                        if movie.get('id') not in seen_ids:
                            all_movies.append(movie)
                            seen_ids.add(movie.get('id'))
                            category_count += 1
                
                collection_summary['sources_processed'].append(str(file_path))
            
            collection_summary['by_category'][category] = category_count
    
    collection_summary['total_unique_movies'] = len(all_movies)
    
    # í†µí•© ë°ì´í„° ì €ì¥
    consolidated_dir = Path('/opt/airflow/data/processed/weekly')
    consolidated_dir.mkdir(parents=True, exist_ok=True)
    
    week_number = collection_summary['week_number']
    output_file = consolidated_dir / f"consolidated_week_{week_number}_{context['ds_nodash']}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'movies': all_movies,
            'consolidation_info': collection_summary
        }, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“Š ì£¼ê°„ ë°ì´í„° í†µí•© ì™„ë£Œ")
    print(f"ğŸ¬ ì´ ê³ ìœ  ì˜í™”: {len(all_movies)}ê°œ")
    for category, count in collection_summary['by_category'].items():
        print(f"  {category}: {count}ê°œ")
    print(f"ğŸ“ í†µí•© íŒŒì¼: {output_file}")
    
    return collection_summary

def generate_weekly_report(**context):
    """ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±"""
    import json
    from pathlib import Path
    
    # í†µí•© ë°ì´í„° ë¡œë“œ
    week_number = datetime.strptime(context['ds'], '%Y-%m-%d').isocalendar()[1]
    consolidated_file = Path(f'/opt/airflow/data/processed/weekly/consolidated_week_{week_number}_{context["ds_nodash"]}.json')
    
    if not consolidated_file.exists():
        raise FileNotFoundError(f"í†µí•© íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {consolidated_file}")
    
    with open(consolidated_file, 'r', encoding='utf-8') as f:
        consolidated_data = json.load(f)
    
    movies = consolidated_data.get('movies', [])
    consolidation_info = consolidated_data.get('consolidation_info', {})
    
    # ë¶„ì„ ìˆ˜í–‰
    def analyze_genre_distribution(movies):
        genre_counts = {}
        for movie in movies:
            genres = movie.get('genre_ids', [])
            for genre_id in genres:
                genre_counts[genre_id] = genre_counts.get(genre_id, 0) + 1
        return dict(sorted(genre_counts.items(), key=lambda x: x[1], reverse=True))
    
    def analyze_rating_distribution(movies):
        rating_ranges = {
            '9.0-10.0': 0, '8.0-8.9': 0, '7.0-7.9': 0,
            '6.0-6.9': 0, '5.0-5.9': 0, '0.0-4.9': 0
        }
        
        for movie in movies:
            rating = movie.get('vote_average', 0)
            if rating >= 9.0:
                rating_ranges['9.0-10.0'] += 1
            elif rating >= 8.0:
                rating_ranges['8.0-8.9'] += 1
            elif rating >= 7.0:
                rating_ranges['7.0-7.9'] += 1
            elif rating >= 6.0:
                rating_ranges['6.0-6.9'] += 1
            elif rating >= 5.0:
                rating_ranges['5.0-5.9'] += 1
            else:
                rating_ranges['0.0-4.9'] += 1
        
        return rating_ranges
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = {
        'report_type': 'weekly',
        'week_number': week_number,
        'generation_time': datetime.now().isoformat(),
        'data_summary': consolidation_info,
        'analysis': {
            'genre_distribution': analyze_genre_distribution(movies),
            'rating_distribution': analyze_rating_distribution(movies),
            'top_movies': sorted(movies, key=lambda x: x.get('popularity', 0), reverse=True)[:10]
        }
    }
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_dir = Path('/opt/airflow/data/raw/metadata/weekly_reports')
    report_dir.mkdir(parents=True, exist_ok=True)
    
    report_file = report_dir / f"weekly_report_W{week_number}_{context['ds_nodash']}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"\nğŸ“‹ ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    print(f"ğŸ“ ë¦¬í¬íŠ¸ íŒŒì¼: {report_file}")
    
    return report

# ì‹œì‘ íƒœìŠ¤í¬
start_task = DummyOperator(
    task_id='start_weekly_collection',
    dag=dag
)

# ì¥ë¥´ë³„ ìˆ˜ì§‘ íƒœìŠ¤í¬ë“¤
genre_tasks = []
for genre_id, genre_name in MAJOR_GENRES.items():
    task = PythonOperator(
        task_id=f'collect_{genre_name.lower()}_movies',
        python_callable=collect_genre_movies,
        op_kwargs={'genre_id': genre_id, 'genre_name': genre_name},
        dag=dag
    )
    genre_tasks.append(task)

# í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘ íƒœìŠ¤í¬
top_rated_task = PythonOperator(
    task_id='collect_top_rated_movies',
    python_callable=collect_top_rated_movies,
    dag=dag
)

# ë°ì´í„° í†µí•© íƒœìŠ¤í¬
consolidate_task = PythonOperator(
    task_id='consolidate_weekly_data',
    python_callable=consolidate_weekly_data,
    dag=dag
)

# ë¦¬í¬íŠ¸ ìƒì„± íƒœìŠ¤í¬
report_task = PythonOperator(
    task_id='generate_weekly_report',
    python_callable=generate_weekly_report,
    dag=dag
)

# ì™„ë£Œ íƒœìŠ¤í¬
end_task = DummyOperator(
    task_id='end_weekly_collection',
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
start_task >> [*genre_tasks, top_rated_task] >> consolidate_task >> report_task >> end_task
