"""
TMDB ë°ì´í„° ìˆ˜ì§‘ DAG
ë§¤ì¼ ìë™ìœ¼ë¡œ ì˜í™” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì›Œí¬í”Œë¡œìš°
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append('/opt/airflow/src')

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'tmdb_daily_collection',
    default_args=default_args,
    description='TMDB ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    max_active_runs=1,
    tags=['tmdb', 'data-collection', 'daily'],
)

def collect_popular_movies(**context):
    """ì¸ê¸° ì˜í™” ë°ì´í„° ìˆ˜ì§‘"""
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    # API ì»¤ë„¥í„° ìƒì„±
    connector = TMDBAPIConnector()
    
    try:
        # ì¸ê¸° ì˜í™” ìˆ˜ì§‘ (ìµœì‹  5í˜ì´ì§€)
        all_movies = []
        for page in range(1, 6):
            response = connector.get_popular_movies(page)
            if response and 'results' in response:
                all_movies.extend(response['results'])
        
        # ìˆ˜ì§‘ í†µê³„
        collection_stats = {
            'collection_type': 'daily_popular',
            'collection_date': context['ds'],
            'total_collected': len(all_movies),
            'pages_processed': 5,
            'start_time': context['ts'],
            'dag_run_id': context['dag_run'].run_id
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/daily')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f"popular_movies_{context['ds_nodash']}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': all_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… ì¸ê¸° ì˜í™” {len(all_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}")
        
        # XComì— í†µê³„ ì •ë³´ ì €ì¥ (ë‹¤ìŒ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©)
        return collection_stats
        
    except Exception as e:
        print(f"âŒ ì¸ê¸° ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        connector.close()

def collect_trending_movies(**context):
    """íŠ¸ë Œë”© ì˜í™” ë°ì´í„° ìˆ˜ì§‘"""
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘
        trending_response = connector.get_trending_movies('day')
        trending_movies = trending_response.get('results', []) if trending_response else []
        
        collection_stats = {
            'collection_type': 'daily_trending',
            'collection_date': context['ds'],
            'total_collected': len(trending_movies),
            'time_window': 'day',
            'dag_run_id': context['dag_run'].run_id
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/trending')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f"trending_movies_{context['ds_nodash']}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': trending_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… íŠ¸ë Œë”© ì˜í™” {len(trending_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return collection_stats
        
    except Exception as e:
        print(f"âŒ íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        connector.close()

def validate_collected_data(**context):
    """ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    from data_processing.quality_validator import DataQualityValidator
    import json
    from pathlib import Path
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° íŒŒì¼ë“¤ í™•ì¸
    data_files = [
        f"/opt/airflow/data/raw/movies/daily/popular_movies_{context['ds_nodash']}.json",
        f"/opt/airflow/data/raw/movies/trending/trending_movies_{context['ds_nodash']}.json"
    ]
    
    validator = DataQualityValidator()
    validation_results = {
        'validation_date': context['ds'],
        'files_validated': [],
        'overall_quality_score': 0,
        'total_movies_validated': 0,
        'total_movies_passed': 0
    }
    
    all_movies = []
    
    for file_path in data_files:
        if Path(file_path).exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                movies = data.get('movies', [])
                all_movies.extend(movies)
                
                validation_results['files_validated'].append({
                    'file': file_path,
                    'movie_count': len(movies)
                })
    
    if all_movies:
        # ë°°ì¹˜ ê²€ì¦ ì‹¤í–‰
        batch_results = validator.validate_batch_data(all_movies)
        
        validation_results.update({
            'total_movies_validated': batch_results['total_movies'],
            'total_movies_passed': batch_results['valid_movies'],
            'validation_rate': (batch_results['valid_movies'] / batch_results['total_movies'] * 100) if batch_results['total_movies'] > 0 else 0,
            'quality_distribution': batch_results['quality_distribution'],
            'common_issues': batch_results['common_issues']
        })
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        report_dir = Path('/opt/airflow/data/raw/metadata/quality_reports')
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f"validation_report_{context['ds_nodash']}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {validation_results['validation_rate']:.1f}% í†µê³¼")
        print(f"ğŸ“Š ê²€ì¦ ë³´ê³ ì„œ: {report_file}")
        
        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
        if validation_results['validation_rate'] < 80:
            print(f"âš ï¸ ë°ì´í„° í’ˆì§ˆ ê²½ê³ : ê²€ì¦ í†µê³¼ìœ¨ì´ {validation_results['validation_rate']:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.")
        
        return validation_results
    
    else:
        raise ValueError("ê²€ì¦í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

def generate_daily_summary(**context):
    """ì¼ì¼ ìˆ˜ì§‘ ìš”ì•½ ìƒì„±"""
    task_instance = context['task_instance']
    
    # ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ê²°ê³¼ ìˆ˜ì§‘
    popular_stats = task_instance.xcom_pull(task_ids='collect_popular_movies')
    trending_stats = task_instance.xcom_pull(task_ids='collect_trending_movies')
    validation_results = task_instance.xcom_pull(task_ids='validate_collected_data')
    
    # ì¼ì¼ ìš”ì•½ ìƒì„±
    daily_summary = {
        'summary_date': context['ds'],
        'dag_run_id': context['dag_run'].run_id,
        'collection_summary': {
            'popular_movies': popular_stats.get('total_collected', 0) if popular_stats else 0,
            'trending_movies': trending_stats.get('total_collected', 0) if trending_stats else 0,
            'total_collected': 0
        },
        'quality_summary': validation_results if validation_results else {},
        'execution_summary': {
            'start_time': context['dag_run'].start_date.isoformat() if context['dag_run'].start_date else None,
            'end_time': datetime.now().isoformat(),
            'duration_minutes': 0
        }
    }
    
    # ì´ ìˆ˜ì§‘ëŸ‰ ê³„ì‚°
    daily_summary['collection_summary']['total_collected'] = (
        daily_summary['collection_summary']['popular_movies'] +
        daily_summary['collection_summary']['trending_movies']
    )
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    if context['dag_run'].start_date:
        duration = datetime.now() - context['dag_run'].start_date
        daily_summary['execution_summary']['duration_minutes'] = duration.total_seconds() / 60
    
    # ìš”ì•½ ì €ì¥
    summary_dir = Path('/opt/airflow/data/raw/metadata/daily_summaries')
    summary_dir.mkdir(parents=True, exist_ok=True)
    
    summary_file = summary_dir / f"daily_summary_{context['ds_nodash']}.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(daily_summary, f, ensure_ascii=False, indent=2, default=str)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“ˆ TMDB ì¼ì¼ ìˆ˜ì§‘ ìš”ì•½")
    print("="*50)
    print(f"ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {context['ds']}")
    print(f"ğŸ¬ ì¸ê¸° ì˜í™”: {daily_summary['collection_summary']['popular_movies']}ê°œ")
    print(f"ğŸ”¥ íŠ¸ë Œë”© ì˜í™”: {daily_summary['collection_summary']['trending_movies']}ê°œ")
    print(f"ğŸ“Š ì´ ìˆ˜ì§‘ëŸ‰: {daily_summary['collection_summary']['total_collected']}ê°œ")
    
    if validation_results:
        print(f"âœ… í’ˆì§ˆ ê²€ì¦: {validation_results.get('validation_rate', 0):.1f}% í†µê³¼")
    
    print(f"â±ï¸ ì‹¤í–‰ ì‹œê°„: {daily_summary['execution_summary']['duration_minutes']:.1f}ë¶„")
    print(f"ğŸ“ ìš”ì•½ ë³´ê³ ì„œ: {summary_file}")
    print("="*50)
    
    return daily_summary

def cleanup_old_data(**context):
    """ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬"""
    from pathlib import Path
    import os
    from datetime import datetime, timedelta
    
    # 30ì¼ ì´ìƒ ëœ íŒŒì¼ ì •ë¦¬
    cutoff_date = datetime.now() - timedelta(days=30)
    
    cleanup_dirs = [
        '/opt/airflow/data/raw/movies/daily',
        '/opt/airflow/data/raw/movies/trending',
        '/opt/airflow/data/raw/metadata/quality_reports'
    ]
    
    cleaned_files = 0
    
    for dir_path in cleanup_dirs:
        dir_obj = Path(dir_path)
        if dir_obj.exists():
            for file_path in dir_obj.glob('*.json'):
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)
                if file_mtime < cutoff_date:
                    try:
                        file_path.unlink()
                        cleaned_files += 1
                        print(f"ì •ë¦¬ë¨: {file_path}")
                    except Exception as e:
                        print(f"ì •ë¦¬ ì‹¤íŒ¨: {file_path} - {e}")
    
    print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {cleaned_files}ê°œ íŒŒì¼ ì‚­ì œ")
    return {'cleaned_files': cleaned_files}

# íƒœìŠ¤í¬ ì •ì˜
collect_popular_task = PythonOperator(
    task_id='collect_popular_movies',
    python_callable=collect_popular_movies,
    dag=dag,
    doc_md="""
    ## ì¸ê¸° ì˜í™” ìˆ˜ì§‘
    
    TMDB APIì—ì„œ ì¸ê¸° ì˜í™” ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ìµœì‹  5í˜ì´ì§€ (ì•½ 100ê°œ ì˜í™”)
    - JSON í˜•íƒœë¡œ ì €ì¥
    - ìˆ˜ì§‘ ë©”íƒ€ë°ì´í„° í¬í•¨
    """
)

collect_trending_task = PythonOperator(
    task_id='collect_trending_movies',
    python_callable=collect_trending_movies,
    dag=dag,
    doc_md="""
    ## íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘
    
    ë‹¹ì¼ íŠ¸ë Œë”© ì˜í™” ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ì¼ê°„ íŠ¸ë Œë”© ì˜í™”
    - ì‹¤ì‹œê°„ ì¸ê¸° ë°˜ì˜
    """
)

validate_data_task = PythonOperator(
    task_id='validate_collected_data',
    python_callable=validate_collected_data,
    dag=dag,
    doc_md="""
    ## ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    
    ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
    - í•„ìˆ˜ í•„ë“œ í™•ì¸
    - ë°ì´í„° íƒ€ì… ê²€ì¦
    - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©
    - í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
    """
)

generate_summary_task = PythonOperator(
    task_id='generate_daily_summary',
    python_callable=generate_daily_summary,
    dag=dag,
    doc_md="""
    ## ì¼ì¼ ìš”ì•½ ìƒì„±
    
    í•˜ë£¨ ì „ì²´ ìˆ˜ì§‘ ì‘ì—…ì˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ í†µê³„
    - í’ˆì§ˆ ì§€í‘œ
    - ì‹¤í–‰ ì‹œê°„
    - ì¢…í•© ë³´ê³ ì„œ
    """
)

cleanup_task = PythonOperator(
    task_id='cleanup_old_data',
    python_callable=cleanup_old_data,
    dag=dag,
    doc_md="""
    ## ì˜¤ë˜ëœ ë°ì´í„° ì •ë¦¬
    
    30ì¼ ì´ìƒ ëœ ë°ì´í„° íŒŒì¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤.
    - ë””ìŠ¤í¬ ê³µê°„ í™•ë³´
    - ì„±ëŠ¥ ìµœì í™”
    """
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
[collect_popular_task, collect_trending_task] >> validate_data_task >> generate_summary_task >> cleanup_task
