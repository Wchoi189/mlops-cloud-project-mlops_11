#!/usr/bin/env python3
"""
ì¢…í•© ë¡œê·¸ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ë¡œê·¸ ë¶„ì„, ì •ë¦¬, ëª¨ë‹ˆí„°ë§ì„ í†µí•© ê´€ë¦¬
"""

import sys
import argparse
import json
import os
from pathlib import Path
from datetime import datetime, timedelta
import subprocess

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / 'src'))

from logging_system.analyzers.log_analyzer import LogAnalyzer, generate_daily_log_report

class LogManager:
    """ì¢…í•© ë¡œê·¸ ê´€ë¦¬ì"""
    
    def __init__(self, log_dir='logs'):
        self.log_dir = Path(log_dir)
        self.analyzer = LogAnalyzer(log_dir)
    
    def clean_old_logs(self, days=30):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        cutoff_date = datetime.now() - timedelta(days=days)
        cleaned_files = []
        total_size_saved = 0
        
        print(f"ğŸ§¹ {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì¤‘...")
        
        for log_file in self.log_dir.rglob('*.log'):
            try:
                if log_file.stat().st_mtime < cutoff_date.timestamp():
                    file_size = log_file.stat().st_size
                    log_file.unlink()
                    cleaned_files.append(str(log_file))
                    total_size_saved += file_size
            except Exception as e:
                print(f"íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨ {log_file}: {e}")
        
        print(f"âœ… ì •ë¦¬ ì™„ë£Œ: {len(cleaned_files)}ê°œ íŒŒì¼, {total_size_saved/1024/1024:.1f}MB ì ˆì•½")
        return cleaned_files
    
    def compress_logs(self, days=7):
        """ì˜¤ë˜ëœ ë¡œê·¸ íŒŒì¼ ì••ì¶•"""
        import gzip
        import shutil
        
        cutoff_date = datetime.now() - timedelta(days=days)
        compressed_files = []
        
        print(f"ğŸ“¦ {days}ì¼ ì´ì „ ë¡œê·¸ íŒŒì¼ ì••ì¶• ì¤‘...")
        
        for log_file in self.log_dir.rglob('*.log'):
            try:
                if (log_file.stat().st_mtime < cutoff_date.timestamp() and 
                    not log_file.name.endswith('.gz')):
                    
                    compressed_path = log_file.with_suffix(log_file.suffix + '.gz')
                    
                    with open(log_file, 'rb') as f_in:
                        with gzip.open(compressed_path, 'wb') as f_out:
                            shutil.copyfileobj(f_in, f_out)
                    
                    log_file.unlink()
                    compressed_files.append(str(compressed_path))
                    
            except Exception as e:
                print(f"ì••ì¶• ì‹¤íŒ¨ {log_file}: {e}")
        
        print(f"âœ… ì••ì¶• ì™„ë£Œ: {len(compressed_files)}ê°œ íŒŒì¼")
        return compressed_files
    
    def archive_logs(self, year=None, month=None):
        """ë¡œê·¸ íŒŒì¼ ì•„ì¹´ì´ë¸Œ"""
        if year is None:
            year = datetime.now().year
        if month is None:
            month = datetime.now().month
        
        archive_dir = self.log_dir / 'archive' / str(year) / f"{month:02d}"
        archive_dir.mkdir(parents=True, exist_ok=True)
        
        archived_files = []
        
        print(f"ğŸ“š {year}ë…„ {month}ì›” ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì¤‘...")
        
        # í•´ë‹¹ ì›”ì˜ ë¡œê·¸ íŒŒì¼ë“¤ ì°¾ê¸°
        for log_file in self.log_dir.rglob('*.log*'):
            try:
                file_date = datetime.fromtimestamp(log_file.stat().st_mtime)
                if file_date.year == year and file_date.month == month:
                    if 'archive' not in str(log_file):  # ì´ë¯¸ ì•„ì¹´ì´ë¸Œëœ íŒŒì¼ ì œì™¸
                        dest_file = archive_dir / log_file.name
                        log_file.rename(dest_file)
                        archived_files.append(str(dest_file))
            except Exception as e:
                print(f"ì•„ì¹´ì´ë¸Œ ì‹¤íŒ¨ {log_file}: {e}")
        
        print(f"âœ… ì•„ì¹´ì´ë¸Œ ì™„ë£Œ: {len(archived_files)}ê°œ íŒŒì¼ â†’ {archive_dir}")
        return archived_files
    
    def get_disk_usage(self):
        """ë¡œê·¸ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ë¶„ì„"""
        usage_stats = {}
        total_size = 0
        
        for subdir in ['app', 'error', 'system', 'audit', 'performance', 'reports', 'archive']:
            subdir_path = self.log_dir / subdir
            if subdir_path.exists():
                size = sum(f.stat().st_size for f in subdir_path.rglob('*') if f.is_file())
                file_count = len(list(subdir_path.rglob('*')))
                usage_stats[subdir] = {
                    'size_bytes': size,
                    'size_mb': size / 1024 / 1024,
                    'file_count': file_count
                }
                total_size += size
        
        usage_stats['total'] = {
            'size_bytes': total_size,
            'size_mb': total_size / 1024 / 1024,
            'size_gb': total_size / 1024 / 1024 / 1024
        }
        
        return usage_stats
    
    def monitor_log_growth(self, hours=24):
        """ë¡œê·¸ ì¦ê°€ìœ¨ ëª¨ë‹ˆí„°ë§"""
        cutoff_time = datetime.now() - timedelta(hours=hours)
        growth_stats = {}
        
        for subdir in ['app', 'error', 'system', 'performance']:
            subdir_path = self.log_dir / subdir
            if subdir_path.exists():
                recent_size = 0
                recent_files = 0
                
                for log_file in subdir_path.rglob('*.log'):
                    try:
                        if datetime.fromtimestamp(log_file.stat().st_mtime) > cutoff_time:
                            recent_size += log_file.stat().st_size
                            recent_files += 1
                    except Exception:
                        continue
                
                growth_stats[subdir] = {
                    'recent_size_mb': recent_size / 1024 / 1024,
                    'recent_files': recent_files,
                    'growth_rate_mb_per_hour': (recent_size / 1024 / 1024) / hours
                }
        
        return growth_stats
    
    def generate_maintenance_report(self):
        """ë¡œê·¸ ìœ ì§€ë³´ìˆ˜ ë¦¬í¬íŠ¸ ìƒì„±"""
        usage_stats = self.get_disk_usage()
        growth_stats = self.monitor_log_growth(24)
        
        report = {
            'report_date': datetime.now().isoformat(),
            'disk_usage': usage_stats,
            'growth_analysis': growth_stats,
            'maintenance_recommendations': []
        }
        
        # ìœ ì§€ë³´ìˆ˜ ê¶Œì¥ì‚¬í•­ ìƒì„±
        total_size_gb = usage_stats['total']['size_gb']
        if total_size_gb > 5:
            report['maintenance_recommendations'].append(
                f"ë¡œê·¸ ì´ í¬ê¸°ê°€ {total_size_gb:.1f}GBì…ë‹ˆë‹¤. ì•„ì¹´ì´ë¸Œë¥¼ ê³ ë ¤í•˜ì„¸ìš”."
            )
        
        # ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë¡œê·¸ ê°ì§€
        for subdir, stats in growth_stats.items():
            if stats['growth_rate_mb_per_hour'] > 10:
                report['maintenance_recommendations'].append(
                    f"{subdir} ë¡œê·¸ê°€ ì‹œê°„ë‹¹ {stats['growth_rate_mb_per_hour']:.1f}MBë¡œ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤."
                )
        
        if not report['maintenance_recommendations']:
            report['maintenance_recommendations'].append("ë¡œê·¸ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤.")
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        report_file = self.log_dir / 'reports' / f"maintenance_report_{datetime.now().strftime('%Y%m%d')}.json"
        report_file.parent.mkdir(exist_ok=True, parents=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2, default=str)
        
        return report
    
    def export_logs(self, start_date, end_date, output_file):
        """íŠ¹ì • ê¸°ê°„ ë¡œê·¸ ë‚´ë³´ë‚´ê¸°"""
        import tarfile
        
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        export_files = []
        
        for log_file in self.log_dir.rglob('*.log*'):
            try:
                file_date = datetime.fromtimestamp(log_file.stat().st_mtime)
                if start_dt <= file_date <= end_dt:
                    export_files.append(log_file)
            except Exception:
                continue
        
        with tarfile.open(output_file, 'w:gz') as tar:
            for log_file in export_files:
                tar.add(log_file, arcname=log_file.relative_to(self.log_dir))
        
        print(f"âœ… {len(export_files)}ê°œ íŒŒì¼ì„ {output_file}ë¡œ ë‚´ë³´ëƒˆìŠµë‹ˆë‹¤.")
        return export_files

def analyze_logs(args):
    """ë¡œê·¸ ë¶„ì„ ì‹¤í–‰"""
    analyzer = LogAnalyzer()
    
    if args.type == 'errors':
        result = analyzer.analyze_error_logs(args.hours)
        print(f"\n=== ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ {args.hours}ì‹œê°„) ===")
        print(f"ì´ ì—ëŸ¬ ìˆ˜: {result['total_errors']}")
        print(f"ì—ëŸ¬ ìœ í˜•: {result['error_types']}")
        print(f"ì—ëŸ¬ íŠ¸ë Œë“œ: {result['error_trend']}")
        
    elif args.type == 'performance':
        result = analyzer.analyze_performance_logs(args.hours)
        print(f"\n=== ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ {args.hours}ì‹œê°„) ===")
        print(f"ì´ ì‘ì—… ìˆ˜: {result['total_operations']}")
        print(f"ëŠë¦° ì‘ì—… ìˆ˜: {len(result['slow_operations'])}")
        
        if result['component_stats']:
            print("\nì»´í¬ë„ŒíŠ¸ë³„ í‰ê·  ì‹¤í–‰ ì‹œê°„:")
            for comp, stats in result['component_stats'].items():
                print(f"  {comp}: {stats['avg_time']:.3f}s (ì´ {stats['count']}íšŒ)")
        
    elif args.type == 'health':
        result = analyzer.generate_health_report()
        print(f"\n=== ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ===")
        print(f"ê±´ê°•ë„: {result['grade']} ({result['health_score']}/100)")
        print(f"ê°ì§€ëœ ì´ìŠˆ: {result['issues']}")
        print("\nê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(result['recommendations'], 1):
            print(f"  {i}. {rec}")
            
    elif args.type == 'api':
        result = analyzer.analyze_api_usage(args.hours)
        print(f"\n=== API ì‚¬ìš© ë¶„ì„ (ìµœê·¼ {args.hours}ì‹œê°„) ===")
        print(f"ì´ API í˜¸ì¶œ: {result['total_api_calls']}")
        print(f"ì„±ê³µë¥ : {result.get('success_rate', 0):.1f}%")
        if 'avg_response_time' in result:
            print(f"í‰ê·  ì‘ë‹µ ì‹œê°„: {result['avg_response_time']:.3f}s")
        
    elif args.type == 'daily':
        result = generate_daily_log_report()
        print("ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, default=str, ensure_ascii=False)
        print(f"\nê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def manage_logs(args):
    """ë¡œê·¸ ê´€ë¦¬ ì‹¤í–‰"""
    manager = LogManager()
    
    if args.action == 'clean':
        manager.clean_old_logs(args.days)
    elif args.action == 'compress':
        manager.compress_logs(args.days)
    elif args.action == 'archive':
        manager.archive_logs()
    elif args.action == 'usage':
        usage = manager.get_disk_usage()
        print("\n=== ë¡œê·¸ ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ===")
        for subdir, stats in usage.items():
            if subdir != 'total':
                print(f"{subdir:15}: {stats['file_count']:4d}ê°œ íŒŒì¼, {stats['size_mb']:8.1f}MB")
        print("-" * 40)
        print(f"{'ì´ê³„':15}: {usage['total']['size_mb']:8.1f}MB ({usage['total']['size_gb']:.2f}GB)")
    elif args.action == 'growth':
        growth = manager.monitor_log_growth(24)
        print("\n=== ë¡œê·¸ ì¦ê°€ìœ¨ (24ì‹œê°„) ===")
        for subdir, stats in growth.items():
            print(f"{subdir:15}: {stats['growth_rate_mb_per_hour']:6.2f}MB/h, {stats['recent_files']:3d}ê°œ íŒŒì¼")
    elif args.action == 'maintenance':
        report = manager.generate_maintenance_report()
        print("\n=== ìœ ì§€ë³´ìˆ˜ ë¦¬í¬íŠ¸ ===")
        print(f"ì´ ì‚¬ìš©ëŸ‰: {report['disk_usage']['total']['size_gb']:.2f}GB")
        print("\nê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(report['maintenance_recommendations'], 1):
            print(f"  {i}. {rec}")

def show_stats(args):
    """ë¡œê·¸ í†µê³„ í‘œì‹œ"""
    log_dir = Path('logs')
    
    print("\n=== ë¡œê·¸ ë””ë ‰í† ë¦¬ í†µê³„ ===")
    
    for subdir in ['app', 'error', 'system', 'audit', 'performance', 'reports', 'archive']:
        subdir_path = log_dir / subdir
        if subdir_path.exists():
            files = list(subdir_path.rglob('*'))
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            
            print(f"{subdir:12}: {len(files):3d}ê°œ íŒŒì¼, {total_size/1024/1024:8.1f}MB")
        else:
            print(f"{subdir:12}: ë””ë ‰í† ë¦¬ ì—†ìŒ")

def tail_logs(args):
    """ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    log_file = Path('logs') / args.type / f"{args.file}.log"
    
    if not log_file.exists():
        print(f"ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {log_file}")
        return
    
    print(f"ğŸ” {log_file} ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ (Ctrl+Cë¡œ ì¢…ë£Œ)")
    
    try:
        # tail -f ëª…ë ¹ì–´ ì‹¤í–‰
        process = subprocess.Popen(['tail', '-f', str(log_file)], 
                                 stdout=subprocess.PIPE, 
                                 stderr=subprocess.PIPE,
                                 universal_newlines=True)
        
        for line in process.stdout:
            print(line.rstrip())
            
    except KeyboardInterrupt:
        print("\nëª¨ë‹ˆí„°ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        process.terminate()
    except Exception as e:
        print(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")

def main():
    parser = argparse.ArgumentParser(description='Movie MLOps ë¡œê·¸ ê´€ë¦¬ ë„êµ¬')
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # ë¡œê·¸ ë¶„ì„
    analyze_parser = subparsers.add_parser('analyze', help='ë¡œê·¸ ë¶„ì„')
    analyze_parser.add_argument('type', choices=['errors', 'performance', 'health', 'api', 'daily'],
                               help='ë¶„ì„ ìœ í˜•')
    analyze_parser.add_argument('--hours', type=int, default=24,
                               help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ê°’: 24)')
    analyze_parser.add_argument('--output', help='ê²°ê³¼ ì €ì¥ íŒŒì¼')
    analyze_parser.set_defaults(func=analyze_logs)
    
    # ë¡œê·¸ ê´€ë¦¬
    manage_parser = subparsers.add_parser('manage', help='ë¡œê·¸ ê´€ë¦¬')
    manage_parser.add_argument('action', choices=['clean', 'compress', 'archive', 'usage', 'growth', 'maintenance'],
                              help='ê´€ë¦¬ ì‘ì—…')
    manage_parser.add_argument('--days', type=int, default=30,
                              help='ì‘ì—… ëŒ€ìƒ ì¼ìˆ˜ (ê¸°ë³¸ê°’: 30)')
    manage_parser.set_defaults(func=manage_logs)
    
    # í†µê³„ í‘œì‹œ
    stats_parser = subparsers.add_parser('stats', help='ë¡œê·¸ í†µê³„')
    stats_parser.set_defaults(func=show_stats)
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
    tail_parser = subparsers.add_parser('tail', help='ë¡œê·¸ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§')
    tail_parser.add_argument('type', choices=['app', 'error', 'system', 'performance'],
                            help='ë¡œê·¸ ìœ í˜•')
    tail_parser.add_argument('file', help='ë¡œê·¸ íŒŒì¼ëª… (í™•ì¥ì ì œì™¸)')
    tail_parser.set_defaults(func=tail_logs)
    
    # ì¸ìˆ˜ íŒŒì‹± ë° ì‹¤í–‰
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    try:
        args.func(args)
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
