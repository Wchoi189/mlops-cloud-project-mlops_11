#!/usr/bin/env python3
"""
ë¡œê¹… ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ëª¨ë“  ë¡œê¹… ì»´í¬ë„ŒíŠ¸ì˜ ê¸°ëŠ¥ì„ ê²€ì¦í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •
"""

import sys
import time
import random
import json
from pathlib import Path
from datetime import datetime
import traceback

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root / 'src'))

from logging_system.log_manager import get_logger, log_performance
from logging_system.decorators import log_execution, LogContext, log_api_call
from logging_system.analyzers.log_analyzer import LogAnalyzer

class LoggingSystemTester:
    """ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.test_results = {
            'start_time': datetime.now().isoformat(),
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'test_details': []
        }
        
        # í…ŒìŠ¤íŠ¸ìš© ë¡œê±°ë“¤
        self.api_logger = get_logger('test_api', 'test_api.log')
        self.data_logger = get_logger('test_data', 'test_data.log')
        self.system_logger = get_logger('test_system', 'test_system.log')
    
    def run_test(self, test_name, test_func):
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        self.test_results['tests_run'] += 1
        start_time = time.time()
        
        try:
            print(f"ğŸ§ª {test_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
            test_func()
            
            duration = time.time() - start_time
            self.test_results['tests_passed'] += 1
            self.test_results['test_details'].append({
                'name': test_name,
                'status': 'PASSED',
                'duration': duration,
                'error': None
            })
            print(f"âœ… {test_name} í…ŒìŠ¤íŠ¸ ì„±ê³µ ({duration:.3f}s)")
            
        except Exception as e:
            duration = time.time() - start_time
            self.test_results['tests_failed'] += 1
            self.test_results['test_details'].append({
                'name': test_name,
                'status': 'FAILED',
                'duration': duration,
                'error': str(e)
            })
            print(f"âŒ {test_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print(f"   ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
    
    @log_execution('test', log_args=True, log_result=True)
    def test_function_logging(self, param1, param2=None):
        """í•¨ìˆ˜ ë¡œê¹… ë°ì½”ë ˆì´í„° í…ŒìŠ¤íŠ¸"""
        time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
        result = f"Result: {param1} + {param2}"
        return result
    
    def test_context_logging(self):
        """ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        with LogContext('test', 'context_operation') as ctx:
            ctx.add_metadata('test_data', 'sample')
            ctx.log_info("Context operation started")
            
            time.sleep(0.2)
            
            ctx.add_metadata('processed_items', 100)
            ctx.log_info("Processing completed")
            
            # ì˜ë„ì ìœ¼ë¡œ ì„±ê³µì ì¸ ì™„ë£Œ
            return "Context test completed"
    
    def test_error_logging(self):
        """ì—ëŸ¬ ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        try:
            # ì˜ë„ì  ì—ëŸ¬ ë°œìƒ
            result = 1 / 0
        except ZeroDivisionError as e:
            self.api_logger.error(f"Test error occurred: {e}")
            self.api_logger.critical("This is a critical test error for demonstration")
        
        # ë‹¤ì–‘í•œ ì—ëŸ¬ ìœ í˜• ì‹œë®¬ë ˆì´ì…˜
        self.api_logger.error("ConnectionError: Failed to connect to TMDB API")
        self.api_logger.error("TimeoutError: Request timed out after 30 seconds")
        self.api_logger.error("ValidationError: Invalid movie data format")
    
    def test_performance_logging(self):
        """ì„±ëŠ¥ ë¡œê¹… í…ŒìŠ¤íŠ¸"""
        operations = [
            ('data_processing', 0.5, 2.0),
            ('api_call', 0.1, 1.0),
            ('database_query', 0.2, 0.8),
            ('file_operation', 0.05, 0.3),
            ('validation', 0.3, 1.5)
        ]
        
        for operation, min_time, max_time in operations:
            # ëœë¤ ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
            duration = random.uniform(min_time, max_time)
            time.sleep(duration)
            
            metadata = {
                'records_processed': random.randint(100, 1000),
                'memory_used_mb': random.randint(50, 500),
                'cpu_usage_percent': random.randint(10, 80)
            }
            
            log_performance('test', operation, duration, metadata)
    
    @log_api_call('test_endpoint', log_request=True, log_response=True)
    def simulate_api_call(self, endpoint, params=None, timeout=30):
        """API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜"""
        # ëœë¤ ì‘ë‹µ ì‹œê°„
        response_time = random.uniform(0.1, 2.0)
        time.sleep(response_time)
        
        # ëœë¤ ì„±ê³µ/ì‹¤íŒ¨
        if random.random() < 0.8:  # 80% ì„±ê³µë¥ 
            return {
                'status': 'success',
                'data': {'movies': [{'id': i} for i in range(20)]},
                'response_time': response_time
            }
        else:
            raise Exception("API call failed: Rate limit exceeded")
    
    def test_api_simulation(self):
        """API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
        scenarios = [
            ('get_popular_movies', {'page': 1}),
            ('get_movie_details', {'movie_id': 123}),
            ('search_movies', {'query': 'action'}),
            ('get_trending', {'time_window': 'day'}),
            ('get_genres', {})
        ]
        
        for endpoint, params in scenarios:
            try:
                result = self.simulate_api_call(endpoint, params)
                self.api_logger.info(f"API call to {endpoint} succeeded")
            except Exception as e:
                self.api_logger.error(f"API call to {endpoint} failed: {e}")
    
    def test_data_quality_simulation(self):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜"""
        with LogContext('data_quality', 'batch_validation') as ctx:
            total_records = 1000
            valid_records = random.randint(800, 950)
            
            ctx.add_metadata('total_records', total_records)
            ctx.add_metadata('valid_records', valid_records)
            ctx.add_metadata('validation_rate', valid_records / total_records * 100)
            
            ctx.log_info(f"Validating {total_records} records")
            
            # ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰
            for i in range(0, total_records, 100):
                time.sleep(0.02)
                progress = min(i + 100, total_records)
                ctx.log_info(f"Processed {progress}/{total_records} records")
            
            validation_rate = valid_records / total_records * 100
            if validation_rate < 85:
                self.data_logger.warning(f"Low validation rate: {validation_rate:.1f}%")
            
            ctx.log_info(f"Validation completed: {validation_rate:.1f}% valid")
    
    def test_bulk_logging(self):
        """ëŒ€ëŸ‰ ë¡œê¹… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        # 1000ê°œì˜ ë¡œê·¸ ë©”ì‹œì§€ ìƒì„±
        for i in range(1000):
            if i % 100 == 0:
                self.system_logger.info(f"Bulk logging progress: {i}/1000")
            
            # ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ë¡œê·¸
            level = random.choice(['debug', 'info', 'warning', 'error'])
            message = f"Bulk log message {i}: {random.choice(['processing', 'validating', 'saving', 'loading'])} data"
            
            if level == 'debug':
                self.system_logger.debug(message)
            elif level == 'info':
                self.system_logger.info(message)
            elif level == 'warning':
                self.system_logger.warning(message)
            elif level == 'error':
                self.system_logger.error(message)
        
        duration = time.time() - start_time
        throughput = 1000 / duration
        
        self.system_logger.info(f"Bulk logging completed: {throughput:.1f} messages/second")
        print(f"ğŸ“Š ëŒ€ëŸ‰ ë¡œê¹… ì„±ëŠ¥: {throughput:.1f} ë©”ì‹œì§€/ì´ˆ")
    
    def test_log_analyzer(self):
        """ë¡œê·¸ ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
        analyzer = LogAnalyzer()
        
        # ì—ëŸ¬ ë¶„ì„ í…ŒìŠ¤íŠ¸
        error_analysis = analyzer.analyze_error_logs(1)  # ìµœê·¼ 1ì‹œê°„
        assert 'total_errors' in error_analysis
        
        # ì„±ëŠ¥ ë¶„ì„ í…ŒìŠ¤íŠ¸
        perf_analysis = analyzer.analyze_performance_logs(1)
        assert 'total_operations' in perf_analysis
        
        # ê±´ê°•ë„ ë¦¬í¬íŠ¸ í…ŒìŠ¤íŠ¸
        health_report = analyzer.generate_health_report()
        assert 'health_score' in health_report
        assert 0 <= health_report['health_score'] <= 100
        
        print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼: ì—ëŸ¬ {error_analysis['total_errors']}ê°œ, "
              f"ì‘ì—… {perf_analysis['total_operations']}ê°œ, "
              f"ê±´ê°•ë„ {health_report['health_score']}/100")
    
    def test_log_file_creation(self):
        """ë¡œê·¸ íŒŒì¼ ìƒì„± í™•ì¸"""
        log_files = [
            'logs/app/test_api.log',
            'logs/app/test_data.log',
            'logs/app/test_system.log',
            'logs/performance/test_performance.log'
        ]
        
        for log_file in log_files:
            file_path = Path(log_file)
            if file_path.exists():
                size = file_path.stat().st_size
                print(f"ğŸ“ {log_file}: {size} bytes")
            else:
                print(f"âš ï¸ {log_file}: íŒŒì¼ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    def run_comprehensive_test(self):
        """ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸ§ª ë¡œê¹… ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*60)
        
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_cases = [
            ("í•¨ìˆ˜ ë¡œê¹… ë°ì½”ë ˆì´í„°", lambda: self.test_function_logging("test_param1", param2="test_param2")),
            ("ì»¨í…ìŠ¤íŠ¸ ë¡œê¹…", self.test_context_logging),
            ("ì—ëŸ¬ ë¡œê¹…", self.test_error_logging),
            ("ì„±ëŠ¥ ë¡œê¹…", self.test_performance_logging),
            ("API ì‹œë®¬ë ˆì´ì…˜", self.test_api_simulation),
            ("ë°ì´í„° í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜", self.test_data_quality_simulation),
            ("ëŒ€ëŸ‰ ë¡œê¹… ì„±ëŠ¥", self.test_bulk_logging),
            ("ë¡œê·¸ ë¶„ì„ê¸°", self.test_log_analyzer),
            ("ë¡œê·¸ íŒŒì¼ ìƒì„±", self.test_log_file_creation)
        ]
        
        for test_name, test_func in test_cases:
            self.run_test(test_name, test_func)
            time.sleep(0.1)  # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²©
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        self.test_results['end_time'] = datetime.now().isoformat()
        duration = time.time() - time.mktime(
            datetime.fromisoformat(self.test_results['start_time']).timetuple()
        )
        self.test_results['total_duration'] = duration
        
        print("\n" + "="*60)
        print("ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        print(f"ì´ í…ŒìŠ¤íŠ¸: {self.test_results['tests_run']}")
        print(f"ì„±ê³µ: {self.test_results['tests_passed']}")
        print(f"ì‹¤íŒ¨: {self.test_results['tests_failed']}")
        print(f"ì„±ê³µë¥ : {(self.test_results['tests_passed'] / self.test_results['tests_run'] * 100):.1f}%")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {duration:.2f}ì´ˆ")
        
        if self.test_results['tests_failed'] > 0:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for test in self.test_results['test_details']:
                if test['status'] == 'FAILED':
                    print(f"  - {test['name']}: {test['error']}")
        
        print(f"\nğŸ“ ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜:")
        print(f"  - ì• í”Œë¦¬ì¼€ì´ì…˜: logs/app/")
        print(f"  - ì—ëŸ¬: logs/error/")
        print(f"  - ì„±ëŠ¥: logs/performance/")
        print(f"  - ë¶„ì„ ê²°ê³¼: logs/reports/")
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        result_file = Path('logs/reports/logging_test_results.json')
        result_file.parent.mkdir(exist_ok=True, parents=True)
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"ğŸ“‹ ìƒì„¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {result_file}")
        print("="*60)
        
        return self.test_results

def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("âš¡ ë¹ ë¥¸ ë¡œê¹… í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    # ê¸°ë³¸ ë¡œê±° í…ŒìŠ¤íŠ¸
    logger = get_logger('quick_test', 'quick_test.log')
    logger.info("Quick test started")
    logger.warning("This is a test warning")
    logger.error("This is a test error")
    
    # ì„±ëŠ¥ ë¡œê¹… í…ŒìŠ¤íŠ¸
    start_time = time.time()
    time.sleep(0.1)
    log_performance('quick_test', 'test_operation', time.time() - start_time)
    
    print("âœ… ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--quick', action='store_true', help='ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰')
    parser.add_argument('--comprehensive', action='store_true', help='ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.quick:
        run_quick_test()
    elif args.comprehensive or len(sys.argv) == 1:
        tester = LoggingSystemTester()
        tester.run_comprehensive_test()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
