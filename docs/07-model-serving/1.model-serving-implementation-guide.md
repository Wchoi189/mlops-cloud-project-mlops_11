# 7ë‹¨ê³„: ML í”„ë ˆì„ì›Œí¬ì™€ ëª¨ë¸ ì„œë¹™ í†µí•© - ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ë‹¤ì–‘í•œ ML í”„ë ˆì„ì›Œí¬ì™€ ëª¨ë¸ ì„œë¹™ í”Œë«í¼ì„ í†µí•©í•˜ì—¬ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ì œê³µ

**í•µì‹¬ ê°€ì¹˜**: ê³ ì„±ëŠ¥ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤ì‹œê°„ ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•

---

## ğŸ¯ 7.1 FastAPI ì„œë²„ ê°œë°œ (ì¶”ë¡  API)

### ëª©í‘œ
ê³ ì„±ëŠ¥ REST APIë¥¼ í†µí•œ ëª¨ë¸ ì¶”ë¡  ì„œë¹„ìŠ¤ êµ¬ì¶•

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **7.1.1 FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¡°**
- **ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •**
  ```python
  # src/api/main.py
  from fastapi import FastAPI, HTTPException, Depends
  from fastapi.middleware.cors import CORSMiddleware
  from fastapi.middleware.gzip import GZipMiddleware
  from contextlib import asynccontextmanager
  import logging
  
  from .routers import predictions, health, models
  from .core.config import settings
  from .core.model_loader import ModelLoader
  from .middleware.logging import LoggingMiddleware
  from .middleware.monitoring import MonitoringMiddleware
  
  # ì „ì—­ ëª¨ë¸ ë¡œë”
  model_loader = None
  
  @asynccontextmanager
  async def lifespan(app: FastAPI):
      # ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë”©
      global model_loader
      model_loader = ModelLoader()
      await model_loader.load_models()
      
      yield
      
      # ì¢…ë£Œ ì‹œ ì •ë¦¬
      if model_loader:
          await model_loader.cleanup()
  
  app = FastAPI(
      title="Movie Recommendation API",
      description="ML-powered movie recommendation service",
      version="1.0.0",
      lifespan=lifespan,
      docs_url="/docs" if settings.ENV != "production" else None,
      redoc_url="/redoc" if settings.ENV != "production" else None
  )
  
  # ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
  app.add_middleware(
      CORSMiddleware,
      allow_origins=settings.ALLOWED_ORIGINS,
      allow_credentials=True,
      allow_methods=["*"],
      allow_headers=["*"],
  )
  app.add_middleware(GZipMiddleware, minimum_size=1000)
  app.add_middleware(LoggingMiddleware)
  app.add_middleware(MonitoringMiddleware)
  
  # ë¼ìš°í„° ë“±ë¡
  app.include_router(health.router, prefix="/health", tags=["health"])
  app.include_router(predictions.router, prefix="/api/v1", tags=["predictions"])
  app.include_router(models.router, prefix="/api/v1/models", tags=["models"])
  
  @app.get("/")
  async def root():
      return {
          "message": "Movie Recommendation API",
          "version": "1.0.0",
          "status": "running"
      }
  ```

#### **7.1.2 ì˜ˆì¸¡ API ì—”ë“œí¬ì¸íŠ¸**
- **ì¶”ë¡  ë¼ìš°í„° êµ¬í˜„**
  ```python
  # src/api/routers/predictions.py
  from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
  from typing import List, Optional
  import time
  import asyncio
  from uuid import uuid4
  
  from ..schemas.prediction import (
      PredictionRequest, 
      PredictionResponse, 
      BatchPredictionRequest,
      BatchPredictionResponse
  )
  from ..core.dependencies import get_model_loader, get_cache
  from ..core.model_loader import ModelLoader
  from ..core.cache import CacheManager
  from ..core.monitoring import track_prediction
  
  router = APIRouter()
  
  @router.post("/predict", response_model=PredictionResponse)
  @track_prediction
  async def predict_single(
      request: PredictionRequest,
      model_loader: ModelLoader = Depends(get_model_loader),
      cache: CacheManager = Depends(get_cache)
  ):
      \"\"\"ë‹¨ì¼ ì‚¬ìš©ì ì˜í™” ì¶”ì²œ\"\"\"
      
      # ìºì‹œ í™•ì¸
      cache_key = f"predict:{request.user_id}:{hash(str(request.preferences))}"
      cached_result = await cache.get(cache_key)
      if cached_result:
          return PredictionResponse.parse_raw(cached_result)
      
      try:
          # ëª¨ë¸ ì˜ˆì¸¡ ì‹¤í–‰
          start_time = time.time()
          
          predictions = await model_loader.predict(
              user_id=request.user_id,
              preferences=request.preferences,
              top_k=request.top_k or 10
          )
          
          inference_time = time.time() - start_time
          
          # ì‘ë‹µ ìƒì„±
          response = PredictionResponse(
              user_id=request.user_id,
              recommendations=predictions,
              inference_time_ms=int(inference_time * 1000),
              model_version=model_loader.current_model_version,
              request_id=str(uuid4())
          )
          
          # ìºì‹œ ì €ì¥ (5ë¶„ TTL)
          await cache.set(cache_key, response.json(), ttl=300)
          
          return response
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Prediction failed: {str(e)}"
          )
  
  @router.post("/batch-predict", response_model=BatchPredictionResponse)
  async def predict_batch(
      request: BatchPredictionRequest,
      background_tasks: BackgroundTasks,
      model_loader: ModelLoader = Depends(get_model_loader)
  ):
      \"\"\"ë°°ì¹˜ ì‚¬ìš©ì ì˜í™” ì¶”ì²œ\"\"\"
      
      if len(request.user_ids) > 1000:
          raise HTTPException(
              status_code=400,
              detail="Batch size cannot exceed 1000 users"
          )
      
      try:
          # ë°°ì¹˜ ì˜ˆì¸¡ ì‹¤í–‰
          start_time = time.time()
          
          batch_predictions = await model_loader.predict_batch(
              user_ids=request.user_ids,
              top_k=request.top_k or 10
          )
          
          total_time = time.time() - start_time
          
          response = BatchPredictionResponse(
              predictions=batch_predictions,
              total_users=len(request.user_ids),
              total_time_ms=int(total_time * 1000),
              average_time_per_user_ms=int((total_time / len(request.user_ids)) * 1000),
              model_version=model_loader.current_model_version,
              batch_id=str(uuid4())
          )
          
          # ë¹„ë™ê¸°ë¡œ ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸
          background_tasks.add_task(
              update_usage_statistics,
              batch_size=len(request.user_ids),
              inference_time=total_time
          )
          
          return response
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Batch prediction failed: {str(e)}"
          )
  
  @router.get("/recommendations/{user_id}")
  async def get_user_recommendations(
      user_id: int,
      top_k: Optional[int] = 10,
      model_loader: ModelLoader = Depends(get_model_loader)
  ):
      \"\"\"íŠ¹ì • ì‚¬ìš©ì ì¶”ì²œ ì¡°íšŒ\"\"\"
      
      try:
          recommendations = await model_loader.get_recommendations(
              user_id=user_id,
              top_k=top_k
          )
          
          return {
              "user_id": user_id,
              "recommendations": recommendations,
              "count": len(recommendations)
          }
          
      except Exception as e:
          raise HTTPException(
              status_code=500,
              detail=f"Failed to get recommendations: {str(e)}"
          )
  
  async def update_usage_statistics(batch_size: int, inference_time: float):
      \"\"\"ì‚¬ìš© í†µê³„ ì—…ë°ì´íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)\"\"\"
      # í†µê³„ ì—…ë°ì´íŠ¸ ë¡œì§
      pass
  ```

#### **7.1.3 API ìŠ¤í‚¤ë§ˆ ì •ì˜**
- **Pydantic ëª¨ë¸ ìŠ¤í‚¤ë§ˆ**
  ```python
  # src/api/schemas/prediction.py
  from pydantic import BaseModel, Field, validator
  from typing import List, Optional, Dict, Any
  from datetime import datetime
  
  class UserPreferences(BaseModel):
      \"\"\"ì‚¬ìš©ì ì„ í˜¸ë„\"\"\"
      favorite_genres: Optional[List[str]] = None
      min_rating: Optional[float] = Field(None, ge=0, le=10)
      preferred_languages: Optional[List[str]] = None
      exclude_adult: Optional[bool] = True
      preferred_decade: Optional[str] = None
  
  class PredictionRequest(BaseModel):
      \"\"\"ë‹¨ì¼ ì˜ˆì¸¡ ìš”ì²­\"\"\"
      user_id: int = Field(..., description="ì‚¬ìš©ì ID")
      preferences: Optional[UserPreferences] = None
      top_k: Optional[int] = Field(10, ge=1, le=50, description="ì¶”ì²œ ì˜í™” ìˆ˜")
      include_reasons: Optional[bool] = Field(False, description="ì¶”ì²œ ì´ìœ  í¬í•¨ ì—¬ë¶€")
      
      @validator('user_id')
      def validate_user_id(cls, v):
          if v <= 0:
              raise ValueError('User ID must be positive')
          return v
  
  class MovieRecommendation(BaseModel):
      \"\"\"ì˜í™” ì¶”ì²œ ê²°ê³¼\"\"\"
      movie_id: int
      title: str
      score: float = Field(..., ge=0, le=1)
      genres: List[str]
      release_year: Optional[int] = None
      rating: Optional[float] = None
      poster_url: Optional[str] = None
      reason: Optional[str] = None
  
  class PredictionResponse(BaseModel):
      \"\"\"ë‹¨ì¼ ì˜ˆì¸¡ ì‘ë‹µ\"\"\"
      user_id: int
      recommendations: List[MovieRecommendation]
      inference_time_ms: int
      model_version: str
      request_id: str
      timestamp: datetime = Field(default_factory=datetime.now)
  
  class BatchPredictionRequest(BaseModel):
      \"\"\"ë°°ì¹˜ ì˜ˆì¸¡ ìš”ì²­\"\"\"
      user_ids: List[int] = Field(..., min_items=1, max_items=1000)
      top_k: Optional[int] = Field(10, ge=1, le=50)
      
      @validator('user_ids')
      def validate_user_ids(cls, v):
          if len(set(v)) != len(v):
              raise ValueError('Duplicate user IDs not allowed')
          if any(uid <= 0 for uid in v):
              raise ValueError('All user IDs must be positive')
          return v
  
  class UserPredictions(BaseModel):
      \"\"\"ì‚¬ìš©ìë³„ ì˜ˆì¸¡ ê²°ê³¼\"\"\"
      user_id: int
      recommendations: List[MovieRecommendation]
      success: bool = True
      error_message: Optional[str] = None
  
  class BatchPredictionResponse(BaseModel):
      \"\"\"ë°°ì¹˜ ì˜ˆì¸¡ ì‘ë‹µ\"\"\"
      predictions: List[UserPredictions]
      total_users: int
      successful_predictions: int = 0
      failed_predictions: int = 0
      total_time_ms: int
      average_time_per_user_ms: int
      model_version: str
      batch_id: str
      timestamp: datetime = Field(default_factory=datetime.now)
      
      @validator('successful_predictions', always=True)
      def calculate_successful(cls, v, values):
          if 'predictions' in values:
              return sum(1 for p in values['predictions'] if p.success)
          return v
      
      @validator('failed_predictions', always=True)
      def calculate_failed(cls, v, values):
          if 'predictions' in values:
              return sum(1 for p in values['predictions'] if not p.success)
          return v
  ```

---

## ğŸ¯ 7.2 ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ë¡œì§ êµ¬í˜„

### ëª©í‘œ
íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ëª¨ë¸ ë¡œë”© ë° ì¶”ë¡  ì‹œìŠ¤í…œ êµ¬ì¶•

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **7.2.1 ëª¨ë¸ ë¡œë” í´ë˜ìŠ¤**
- **ë¹„ë™ê¸° ëª¨ë¸ ê´€ë¦¬**
  ```python
  # src/api/core/model_loader.py
  import asyncio
  import pickle
  import numpy as np
  from typing import Dict, List, Optional, Any
  import mlflow.pyfunc
  from mlflow.tracking import MlflowClient
  import logging
  
  from .config import settings
  from ..schemas.prediction import MovieRecommendation, UserPreferences
  
  class ModelLoader:
      def __init__(self):
          self.client = MlflowClient(settings.MLFLOW_TRACKING_URI)
          self.current_model = None
          self.current_model_version = None
          self.model_metadata = {}
          self.movie_catalog = {}
          self.user_embeddings = {}
          self._lock = asyncio.Lock()
          self.logger = logging.getLogger(__name__)
      
      async def load_models(self):
          \"\"\"ìµœì‹  í”„ë¡œë•ì…˜ ëª¨ë¸ ë¡œë“œ\"\"\"
          async with self._lock:
              try:
                  # í”„ë¡œë•ì…˜ ëª¨ë¸ ì¡°íšŒ
                  latest_version = self.client.get_latest_versions(
                      name=settings.MODEL_NAME,
                      stages=["Production"]
                  )
                  
                  if not latest_version:
                      raise Exception("No production model found")
                  
                  model_version = latest_version[0]
                  model_uri = f"models:/{settings.MODEL_NAME}/{model_version.version}"
                  
                  # ëª¨ë¸ ë¡œë“œ
                  self.current_model = mlflow.pyfunc.load_model(model_uri)
                  self.current_model_version = model_version.version
                  
                  # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                  await self._load_model_metadata(model_version)
                  
                  # ì˜í™” ì¹´íƒˆë¡œê·¸ ë¡œë“œ
                  await self._load_movie_catalog()
                  
                  self.logger.info(f"Successfully loaded model version {self.current_model_version}")
                  
              except Exception as e:
                  self.logger.error(f"Failed to load model: {e}")
                  raise
      
      async def _load_model_metadata(self, model_version):
          \"\"\"ëª¨ë¸ ë©”íƒ€ë°ì´í„° ë¡œë“œ\"\"\"
          run_id = model_version.run_id
          run = self.client.get_run(run_id)
          
          self.model_metadata = {
              'accuracy': run.data.metrics.get('accuracy', 0),
              'precision': run.data.metrics.get('precision', 0),
              'recall': run.data.metrics.get('recall', 0),
              'training_date': run.info.start_time,
              'hyperparameters': run.data.params
          }
      
      async def _load_movie_catalog(self):
          \"\"\"ì˜í™” ì¹´íƒˆë¡œê·¸ ë°ì´í„° ë¡œë“œ\"\"\"
          # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ìºì‹œì—ì„œ ë¡œë“œ
          self.movie_catalog = {
              # movie_id: {title, genres, rating, poster_url, ...}
          }
      
      async def predict(
          self, 
          user_id: int, 
          preferences: Optional[UserPreferences] = None,
          top_k: int = 10
      ) -> List[MovieRecommendation]:
          \"\"\"ë‹¨ì¼ ì‚¬ìš©ì ì˜ˆì¸¡\"\"\"
          
          if not self.current_model:
              raise Exception("Model not loaded")
          
          try:
              # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
              input_data = await self._prepare_input_data(user_id, preferences)
              
              # ëª¨ë¸ ì˜ˆì¸¡
              predictions = self.current_model.predict(input_data)
              
              # ê²°ê³¼ í›„ì²˜ë¦¬
              recommendations = await self._postprocess_predictions(
                  predictions, user_id, top_k, preferences
              )
              
              return recommendations
              
          except Exception as e:
              self.logger.error(f"Prediction failed for user {user_id}: {e}")
              raise
      
      async def predict_batch(
          self, 
          user_ids: List[int], 
          top_k: int = 10
      ) -> List[Dict[str, Any]]:
          \"\"\"ë°°ì¹˜ ì‚¬ìš©ì ì˜ˆì¸¡\"\"\"
          
          if not self.current_model:
              raise Exception("Model not loaded")
          
          batch_results = []
          
          # ë°°ì¹˜ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
          chunk_size = 100
          for i in range(0, len(user_ids), chunk_size):
              chunk = user_ids[i:i + chunk_size]
              
              try:
                  # ì²­í¬ë³„ ì˜ˆì¸¡
                  chunk_results = await self._predict_chunk(chunk, top_k)
                  batch_results.extend(chunk_results)
                  
              except Exception as e:
                  self.logger.error(f"Batch prediction failed for chunk {i//chunk_size}: {e}")
                  # ì‹¤íŒ¨í•œ ì‚¬ìš©ìë“¤ì„ ì˜¤ë¥˜ë¡œ í‘œì‹œ
                  for user_id in chunk:
                      batch_results.append({
                          "user_id": user_id,
                          "recommendations": [],
                          "success": False,
                          "error_message": str(e)
                      })
          
          return batch_results
      
      async def _predict_chunk(self, user_ids: List[int], top_k: int):
          \"\"\"ì²­í¬ ë‹¨ìœ„ ì˜ˆì¸¡\"\"\"
          results = []
          
          for user_id in user_ids:
              try:
                  recommendations = await self.predict(user_id, top_k=top_k)
                  results.append({
                      "user_id": user_id,
                      "recommendations": recommendations,
                      "success": True
                  })
              except Exception as e:
                  results.append({
                      "user_id": user_id,
                      "recommendations": [],
                      "success": False,
                      "error_message": str(e)
                  })
          
          return results
      
      async def _prepare_input_data(self, user_id: int, preferences: Optional[UserPreferences]):
          \"\"\"ëª¨ë¸ ì…ë ¥ ë°ì´í„° ì¤€ë¹„\"\"\"
          # ì‚¬ìš©ì í”¼ì²˜ ìƒì„±
          user_features = await self._get_user_features(user_id)
          
          # ì„ í˜¸ë„ í”¼ì²˜ ì¶”ê°€
          if preferences:
              preference_features = await self._encode_preferences(preferences)
              user_features.update(preference_features)
          
          # ëª¨ë¸ ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
          input_array = np.array([list(user_features.values())])
          
          return input_array
      
      async def _get_user_features(self, user_id: int) -> Dict[str, float]:
          \"\"\"ì‚¬ìš©ì í”¼ì²˜ ì¡°íšŒ\"\"\"
          # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í”¼ì²˜ ìŠ¤í† ì–´ì—ì„œ ì¡°íšŒ
          return {
              'user_id': float(user_id),
              'avg_rating': 7.5,
              'total_ratings': 50,
              'preferred_genre_action': 0.3,
              'preferred_genre_drama': 0.7
          }
      
      async def _encode_preferences(self, preferences: UserPreferences) -> Dict[str, float]:
          \"\"\"ì‚¬ìš©ì ì„ í˜¸ë„ ì¸ì½”ë”©\"\"\"
          encoded = {}
          
          if preferences.favorite_genres:
              for genre in preferences.favorite_genres:
                  encoded[f'pref_genre_{genre.lower()}'] = 1.0
          
          if preferences.min_rating:
              encoded['min_rating'] = preferences.min_rating
          
          return encoded
      
      async def _postprocess_predictions(
          self, 
          predictions: np.ndarray, 
          user_id: int, 
          top_k: int,
          preferences: Optional[UserPreferences]
      ) -> List[MovieRecommendation]:
          \"\"\"ì˜ˆì¸¡ ê²°ê³¼ í›„ì²˜ë¦¬\"\"\"
          
          # ìƒìœ„ kê°œ ì˜í™” ì„ íƒ
          top_indices = np.argsort(predictions[0])[-top_k:][::-1]
          top_scores = predictions[0][top_indices]
          
          recommendations = []
          
          for idx, (movie_idx, score) in enumerate(zip(top_indices, top_scores)):
              movie_info = self.movie_catalog.get(movie_idx, {})
              
              recommendation = MovieRecommendation(
                  movie_id=movie_idx,
                  title=movie_info.get('title', f'Movie {movie_idx}'),
                  score=float(score),
                  genres=movie_info.get('genres', []),
                  release_year=movie_info.get('release_year'),
                  rating=movie_info.get('rating'),
                  poster_url=movie_info.get('poster_url'),
                  reason=await self._generate_recommendation_reason(
                      movie_info, preferences, score
                  )
              )
              
              recommendations.append(recommendation)
          
          return recommendations
      
      async def _generate_recommendation_reason(
          self, 
          movie_info: Dict, 
          preferences: Optional[UserPreferences],
          score: float
      ) -> Optional[str]:
          \"\"\"ì¶”ì²œ ì´ìœ  ìƒì„±\"\"\"
          if not preferences:
              return None
          
          reasons = []
          
          if preferences.favorite_genres:
              common_genres = set(movie_info.get('genres', [])) & set(preferences.favorite_genres)
              if common_genres:
                  reasons.append(f"ì„ í˜¸ ì¥ë¥´ ({', '.join(common_genres)}) í¬í•¨")
          
          if preferences.min_rating and movie_info.get('rating', 0) >= preferences.min_rating:
              reasons.append(f"ë†’ì€ í‰ì  ({movie_info['rating']:.1f})")
          
          if score > 0.8:
              reasons.append("ë†’ì€ ì¶”ì²œ ì ìˆ˜")
          
          return "; ".join(reasons) if reasons else None
      
      async def reload_model(self):
          \"\"\"ëª¨ë¸ ì¬ë¡œë“œ (í•« ìŠ¤ì™€í•‘)\"\"\"
          old_version = self.current_model_version
          
          try:
              await self.load_models()
              self.logger.info(f"Model reloaded from version {old_version} to {self.current_model_version}")
              
          except Exception as e:
              self.logger.error(f"Failed to reload model: {e}")
              raise
      
      async def get_model_info(self) -> Dict[str, Any]:
          \"\"\"ëª¨ë¸ ì •ë³´ ì¡°íšŒ\"\"\"
          return {
              'model_version': self.current_model_version,
              'metadata': self.model_metadata,
              'catalog_size': len(self.movie_catalog),
              'status': 'loaded' if self.current_model else 'not_loaded'
          }
      
      async def cleanup(self):
          \"\"\"ë¦¬ì†ŒìŠ¤ ì •ë¦¬\"\"\"
          self.current_model = None
          self.current_model_version = None
          self.model_metadata = {}
          self.movie_catalog = {}
          self.logger.info("Model loader cleaned up")
  ```

#### **7.2.2 ìºì‹± ì‹œìŠ¤í…œ**
- **Redis ê¸°ë°˜ ìºì‹œ ë§¤ë‹ˆì €**
  ```python
  # src/api/core/cache.py
  import redis.asyncio as redis
  import json
  import pickle
  from typing import Any, Optional
  import logging
  
  from .config import settings
  
  class CacheManager:
      def __init__(self):
          self.redis_client = None
          self.logger = logging.getLogger(__name__)
      
      async def connect(self):
          \"\"\"Redis ì—°ê²°\"\"\"
          try:
              self.redis_client = redis.Redis(
                  host=settings.REDIS_HOST,
                  port=settings.REDIS_PORT,
                  password=settings.REDIS_PASSWORD,
                  decode_responses=True,
                  max_connections=20
              )
              
              # ì—°ê²° í…ŒìŠ¤íŠ¸
              await self.redis_client.ping()
              self.logger.info("Connected to Redis cache")
              
          except Exception as e:
              self.logger.error(f"Failed to connect to Redis: {e}")
              raise
      
      async def get(self, key: str) -> Optional[str]:
          \"\"\"ìºì‹œì—ì„œ ê°’ ì¡°íšŒ\"\"\"
          if not self.redis_client:
              return None
          
          try:
              value = await self.redis_client.get(key)
              if value:
                  self.logger.debug(f"Cache hit for key: {key}")
              return value
              
          except Exception as e:
              self.logger.error(f"Cache get error for key {key}: {e}")
              return None
      
      async def set(self, key: str, value: str, ttl: int = 3600):
          \"\"\"ìºì‹œì— ê°’ ì €ì¥\"\"\"
          if not self.redis_client:
              return False
          
          try:
              await self.redis_client.setex(key, ttl, value)
              self.logger.debug(f"Cache set for key: {key}, TTL: {ttl}")
              return True
              
          except Exception as e:
              self.logger.error(f"Cache set error for key {key}: {e}")
              return False
      
      async def delete(self, key: str):
          \"\"\"ìºì‹œì—ì„œ í‚¤ ì‚­ì œ\"\"\"
          if not self.redis_client:
              return False
          
          try:
              result = await self.redis_client.delete(key)
              return result > 0
              
          except Exception as e:
              self.logger.error(f"Cache delete error for key {key}: {e}")
              return False
      
      async def get_json(self, key: str) -> Optional[dict]:
          \"\"\"JSON í˜•íƒœë¡œ ìºì‹œ ì¡°íšŒ\"\"\"
          value = await self.get(key)
          if value:
              try:
                  return json.loads(value)
              except json.JSONDecodeError:
                  self.logger.error(f"Failed to decode JSON for key: {key}")
          return None
      
      async def set_json(self, key: str, value: dict, ttl: int = 3600):
          \"\"\"JSON í˜•íƒœë¡œ ìºì‹œ ì €ì¥\"\"\"
          try:
              json_value = json.dumps(value)
              return await self.set(key, json_value, ttl)
          except Exception as e:
              self.logger.error(f"Failed to encode JSON for key {key}: {e}")
              return False
      
      async def close(self):
          \"\"\"Redis ì—°ê²° ì¢…ë£Œ\"\"\"
          if self.redis_client:
              await self.redis_client.close()
              self.logger.info("Redis connection closed")
  ```

ì´ 7ë‹¨ê³„ë¥¼ ì™„ë£Œí•˜ë©´ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ í†µí•´ ê³ ì„±ëŠ¥ ëª¨ë¸ ì˜ˆì¸¡ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ê³ , ë‹¤ìˆ˜ì˜ ë™ì‹œ ìš”ì²­ì„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ê²¬ê³ í•œ ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œì´ êµ¬ì¶•ë©ë‹ˆë‹¤.
