# 6ë‹¨ê³„: ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì¤‘ì‹¬ì˜ ëª¨ë¸ ê´€ë¦¬ ì²´ê³„ - ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ì²´ê³„ì ì¸ ëª¨ë¸ ìƒëª…ì£¼ê¸° ê´€ë¦¬ë¡œ ëª¨ë¸ í’ˆì§ˆê³¼ ê±°ë²„ë„ŒìŠ¤ í™•ë³´

**í•µì‹¬ ê°€ì¹˜**: ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ê°€ ì›Œí¬í”Œë¡œìš°ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•˜ì—¬ ëª¨ë“  í•™ìŠµëœ ëª¨ë¸ì´ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ê±°ì³ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬ë˜ëŠ” ì‹œìŠ¤í…œ êµ¬ì¶•

---

## ğŸ¯ 6.1 MLflow Tracking Server ì„¤ì¹˜ ë° ì„¤ì •

### ëª©í‘œ
ì‹¤í—˜ ì¶”ì ê³¼ ëª¨ë¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ì¤‘ì•™í™”ëœ MLflow ì„œë²„ êµ¬ì¶•

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **6.1.1 MLflow ì„œë²„ ì„¤ì¹˜**
- **Docker Composeë¥¼ í™œìš©í•œ MLflow í™˜ê²½ êµ¬ì¶•**
  ```yaml
  # docker-compose.mlflow.yml
  version: '3.8'
  
  services:
    postgres:
      image: postgres:15
      container_name: mlflow-postgres
      environment:
        POSTGRES_DB: mlflow
        POSTGRES_USER: mlflow
        POSTGRES_PASSWORD: mlflow_password
      volumes:
        - postgres_data:/var/lib/postgresql/data
      ports:
        - "5432:5432"
      restart: always
  
    minio:
      image: minio/minio:latest
      container_name: mlflow-minio
      environment:
        MINIO_ACCESS_KEY: minio_access_key
        MINIO_SECRET_KEY: minio_secret_key
      volumes:
        - minio_data:/data
      ports:
        - "9000:9000"
        - "9001:9001"
      command: server /data --console-address ":9001"
      restart: always
  
    mlflow:
      image: python:3.11-slim
      container_name: mlflow-server
      depends_on:
        - postgres
        - minio
      environment:
        MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:mlflow_password@postgres:5432/mlflow
        MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://mlflow-artifacts/
        AWS_ACCESS_KEY_ID: minio_access_key
        AWS_SECRET_ACCESS_KEY: minio_secret_key
        MLFLOW_S3_ENDPOINT_URL: http://minio:9000
      ports:
        - "5000:5000"
      volumes:
        - ./mlflow-entrypoint.sh:/entrypoint.sh
      command: /entrypoint.sh
      restart: always
  
  volumes:
    postgres_data:
    minio_data:
  ```

- **MLflow ì„œë²„ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸**
  ```bash
  #!/bin/bash
  # mlflow-entrypoint.sh
  
  # MLflow ì„¤ì¹˜
  pip install mlflow[extras]==2.22.0 psycopg2-binary boto3
  
  # MinIO ë²„í‚· ìƒì„±ì„ ìœ„í•œ ëŒ€ê¸°
  sleep 10
  
  # MLflow ì„œë²„ ì‹œì‘
  mlflow server \
    --backend-store-uri $MLFLOW_BACKEND_STORE_URI \
    --default-artifact-root $MLFLOW_DEFAULT_ARTIFACT_ROOT \
    --host 0.0.0.0 \
    --port 5000
  ```

#### **6.1.2 MLflow í´ë¼ì´ì–¸íŠ¸ ì„¤ì •**
- **Python í´ë¼ì´ì–¸íŠ¸ ì„¤ì •**
  ```python
  # src/mlflow_config.py
  import mlflow
  import os
  from mlflow.tracking import MlflowClient
  
  class MLflowConfig:
      \"\"\"MLflow ì„¤ì • ê´€ë¦¬\"\"\"
      
      def __init__(self):
          # MLflow ì„œë²„ URI ì„¤ì •
          self.tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
          self.experiment_name = os.getenv("MLFLOW_EXPERIMENT_NAME", "movie_recommendation")
          
          # MLflow ì„¤ì •
          mlflow.set_tracking_uri(self.tracking_uri)
          
          # ì‹¤í—˜ ì„¤ì •
          self._setup_experiment()
          
          # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
          self.client = MlflowClient(tracking_uri=self.tracking_uri)
      
      def _setup_experiment(self):
          \"\"\"ì‹¤í—˜ ì„¤ì •\"\"\"
          try:
              # ì‹¤í—˜ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
              experiment = mlflow.get_experiment_by_name(self.experiment_name)
              if experiment is None:
                  # ì‹¤í—˜ ìƒì„±
                  experiment_id = mlflow.create_experiment(
                      name=self.experiment_name,
                      tags={
                          "project": "mlops-movie-recommendation",
                          "version": "1.0",
                          "description": "Movie recommendation system experiments"
                      }
                  )
                  print(f"ì‹¤í—˜ ìƒì„±ë¨: {self.experiment_name} (ID: {experiment_id})")
              else:
                  experiment_id = experiment.experiment_id
                  print(f"ê¸°ì¡´ ì‹¤í—˜ ì‚¬ìš©: {self.experiment_name} (ID: {experiment_id})")
              
              # í™œì„± ì‹¤í—˜ ì„¤ì •
              mlflow.set_experiment(self.experiment_name)
              
          except Exception as e:
              print(f"ì‹¤í—˜ ì„¤ì • ì‹¤íŒ¨: {e}")
              raise
      
      def get_client(self):
          \"\"\"MLflow í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜\"\"\"
          return self.client
  ```

---

## ğŸ¯ 6.2 ì‹¤í—˜ ì¶”ì  ì‹œìŠ¤í…œ êµ¬ì¶•

### ëª©í‘œ
ëª¨ë“  ML ì‹¤í—˜ì„ ì²´ê³„ì ìœ¼ë¡œ ì¶”ì í•˜ê³  ë¹„êµí•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œ êµ¬í˜„

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **6.2.1 ì‹¤í—˜ ì¶”ì  ë˜í¼ í´ë˜ìŠ¤**
- **MLflow ì‹¤í—˜ ì¶”ì  í†µí•©**
  ```python
  # src/tracking/experiment_tracker.py
  import mlflow
  import mlflow.sklearn
  import mlflow.pytorch
  import numpy as np
  import pickle
  import json
  from datetime import datetime
  from typing import Dict, Any, Optional, Union
  import logging
  
  class ExperimentTracker:
      \"\"\"MLflow ì‹¤í—˜ ì¶”ì ê¸°\"\"\"
      
      def __init__(self, experiment_name: str = "movie_recommendation"):
          self.experiment_name = experiment_name
          self.logger = logging.getLogger(__name__)
          
          # MLflow ì„¤ì •
          mlflow.set_experiment(experiment_name)
          
      def start_run(self, run_name: Optional[str] = None, 
                   tags: Optional[Dict[str, str]] = None):
          \"\"\"ì‹¤í—˜ ì‹¤í–‰ ì‹œì‘\"\"\"
          
          # ê¸°ë³¸ íƒœê·¸ ì„¤ì •
          default_tags = {
              "timestamp": datetime.now().isoformat(),
              "project": "mlops-movie-recommendation",
              "stage": "development"
          }
          
          if tags:
              default_tags.update(tags)
              
          # MLflow ì‹¤í–‰ ì‹œì‘
          mlflow.start_run(run_name=run_name, tags=default_tags)
          
          run_id = mlflow.active_run().info.run_id
          self.logger.info(f"ì‹¤í—˜ ì‹¤í–‰ ì‹œì‘: {run_id}")
          
          return run_id
          
      def log_parameters(self, params: Dict[str, Any]):
          \"\"\"í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…\"\"\"
          try:
              # ë‹¨ìˆœ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
              clean_params = {}
              for key, value in params.items():
                  if isinstance(value, (int, float, str, bool)):
                      clean_params[key] = value
                  else:
                      clean_params[key] = str(value)
              
              mlflow.log_params(clean_params)
              self.logger.info(f"íŒŒë¼ë¯¸í„° ë¡œê¹… ì™„ë£Œ: {len(clean_params)}ê°œ")
              
          except Exception as e:
              self.logger.error(f"íŒŒë¼ë¯¸í„° ë¡œê¹… ì‹¤íŒ¨: {e}")
              
      def log_metrics(self, metrics: Dict[str, Union[int, float]], 
                     step: Optional[int] = None):
          \"\"\"ë©”íŠ¸ë¦­ ë¡œê¹…\"\"\"
          try:
              for metric_name, metric_value in metrics.items():
                  if isinstance(metric_value, (int, float, np.integer, np.floating)):
                      mlflow.log_metric(metric_name, float(metric_value), step=step)
                  
              self.logger.info(f"ë©”íŠ¸ë¦­ ë¡œê¹… ì™„ë£Œ: {len(metrics)}ê°œ")
              
          except Exception as e:
              self.logger.error(f"ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")
              
      def log_model(self, model, model_name: str, 
                   model_type: str = "sklearn",
                   signature: Optional[Any] = None,
                   input_example: Optional[Any] = None):
          \"\"\"ëª¨ë¸ ë¡œê¹…\"\"\"
          try:
              if model_type == "sklearn":
                  mlflow.sklearn.log_model(
                      sk_model=model,
                      artifact_path=model_name,
                      signature=signature,
                      input_example=input_example
                  )
              elif model_type == "pytorch":
                  mlflow.pytorch.log_model(
                      pytorch_model=model,
                      artifact_path=model_name,
                      signature=signature,
                      input_example=input_example
                  )
              elif model_type == "custom":
                  # ì»¤ìŠ¤í…€ ëª¨ë¸ (numpy ê¸°ë°˜ ë“±)
                  self._log_custom_model(model, model_name)
              
              self.logger.info(f"ëª¨ë¸ ë¡œê¹… ì™„ë£Œ: {model_name}")
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
              
      def _log_custom_model(self, model, model_name: str):
          \"\"\"ì»¤ìŠ¤í…€ ëª¨ë¸ ë¡œê¹…\"\"\"
          # ëª¨ë¸ì„ pickleë¡œ ì§ë ¬í™”
          model_bytes = pickle.dumps(model)
          
          # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ í›„ ì•„í‹°íŒ©íŠ¸ë¡œ ë¡œê¹…
          import tempfile
          with tempfile.NamedTemporaryFile(suffix='.pkl', delete=False) as f:
              f.write(model_bytes)
              f.flush()
              mlflow.log_artifact(f.name, artifact_path=f"{model_name}/model.pkl")
              
      def log_artifacts(self, artifacts: Dict[str, Any]):
          \"\"\"ì•„í‹°íŒ©íŠ¸ ë¡œê¹…\"\"\"
          try:
              import tempfile
              import os
              
              for artifact_name, artifact_data in artifacts.items():
                  if isinstance(artifact_data, dict):
                      # JSONìœ¼ë¡œ ì €ì¥
                      with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                          json.dump(artifact_data, f, indent=2, default=str)
                          f.flush()
                          mlflow.log_artifact(f.name, artifact_path=f"artifacts/{artifact_name}.json")
                          os.unlink(f.name)
                          
                  elif isinstance(artifact_data, str):
                      # í…ìŠ¤íŠ¸ë¡œ ì €ì¥
                      with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
                          f.write(artifact_data)
                          f.flush()
                          mlflow.log_artifact(f.name, artifact_path=f"artifacts/{artifact_name}.txt")
                          os.unlink(f.name)
                          
              self.logger.info(f"ì•„í‹°íŒ©íŠ¸ ë¡œê¹… ì™„ë£Œ: {len(artifacts)}ê°œ")
              
          except Exception as e:
              self.logger.error(f"ì•„í‹°íŒ©íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
              
      def log_dataset_info(self, dataset_info: Dict[str, Any]):
          \"\"\"ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹…\"\"\"
          try:
              # ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„°ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ë¡œê¹…
              dataset_params = {}
              for key, value in dataset_info.items():
                  if key.startswith('dataset_'):
                      dataset_params[key] = str(value)
                  else:
                      dataset_params[f'dataset_{key}'] = str(value)
              
              mlflow.log_params(dataset_params)
              
              # ë°ì´í„°ì…‹ í†µê³„ë¥¼ ë©”íŠ¸ë¦­ìœ¼ë¡œ ë¡œê¹…
              if 'train_size' in dataset_info:
                  mlflow.log_metric('dataset_train_size', dataset_info['train_size'])
              if 'test_size' in dataset_info:
                  mlflow.log_metric('dataset_test_size', dataset_info['test_size'])
              if 'feature_count' in dataset_info:
                  mlflow.log_metric('dataset_feature_count', dataset_info['feature_count'])
                  
              self.logger.info("ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹… ì™„ë£Œ")
              
          except Exception as e:
              self.logger.error(f"ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹… ì‹¤íŒ¨: {e}")
              
      def end_run(self, status: str = "FINISHED"):
          \"\"\"ì‹¤í—˜ ì‹¤í–‰ ì¢…ë£Œ\"\"\"
          try:
              if mlflow.active_run():
                  run_id = mlflow.active_run().info.run_id
                  mlflow.end_run(status=status)
                  self.logger.info(f"ì‹¤í—˜ ì‹¤í–‰ ì¢…ë£Œ: {run_id}")
              
          except Exception as e:
              self.logger.error(f"ì‹¤í—˜ ì¢…ë£Œ ì‹¤íŒ¨: {e}")
  ```

#### **6.2.2 ëª¨ë¸ í›ˆë ¨ê³¼ ì¶”ì  í†µí•©**
- **í›ˆë ¨ ê³¼ì • ì¶”ì **
  ```python
  # src/training/tracked_trainer.py
  import mlflow
  from typing import Dict, Any, Optional
  import numpy as np
  from sklearn.metrics import accuracy_score, precision_recall_fscore_support
  
  from ..tracking.experiment_tracker import ExperimentTracker
  
  class TrackedModelTrainer:
      \"\"\"MLflow ì¶”ì ì´ í†µí•©ëœ ëª¨ë¸ í›ˆë ¨ê¸°\"\"\"
      
      def __init__(self, experiment_name: str = "movie_recommendation"):
          self.tracker = ExperimentTracker(experiment_name)
          
      def train_with_tracking(self, model_class, train_data, val_data, 
                            hyperparameters: Dict[str, Any],
                            run_name: Optional[str] = None):
          \"\"\"ì¶”ì ê³¼ í•¨ê»˜ ëª¨ë¸ í›ˆë ¨\"\"\"
          
          # ì‹¤í—˜ ì‹¤í–‰ ì‹œì‘
          run_id = self.tracker.start_run(
              run_name=run_name,
              tags={
                  "model_type": model_class.__name__,
                  "training_stage": "supervised_learning"
              }
          )
          
          try:
              # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
              self.tracker.log_parameters(hyperparameters)
              
              # ë°ì´í„°ì…‹ ì •ë³´ ë¡œê¹…
              dataset_info = {
                  "train_size": len(train_data),
                  "val_size": len(val_data),
                  "feature_count": train_data.shape[1] if hasattr(train_data, 'shape') else 'unknown',
                  "training_date": mlflow.utils.time.get_current_time_millis()
              }
              self.tracker.log_dataset_info(dataset_info)
              
              # ëª¨ë¸ ì´ˆê¸°í™”
              model = model_class(**hyperparameters)
              
              # í›ˆë ¨ ì‹¤í–‰
              training_history = self._train_model(model, train_data, val_data)
              
              # í›ˆë ¨ íˆìŠ¤í† ë¦¬ ë¡œê¹…
              for epoch, metrics in enumerate(training_history):
                  self.tracker.log_metrics(metrics, step=epoch)
              
              # ìµœì¢… ëª¨ë¸ í‰ê°€
              final_metrics = self._evaluate_model(model, val_data)
              self.tracker.log_metrics(final_metrics)
              
              # ëª¨ë¸ ì €ì¥
              self.tracker.log_model(
                  model=model,
                  model_name="movie_recommender",
                  model_type="custom"
              )
              
              # ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ
              self.tracker.end_run(status="FINISHED")
              
              return {
                  "run_id": run_id,
                  "model": model,
                  "metrics": final_metrics,
                  "training_history": training_history
              }
              
          except Exception as e:
              # ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì—…ë°ì´íŠ¸
              self.tracker.end_run(status="FAILED")
              raise
              
      def _train_model(self, model, train_data, val_data):
          \"\"\"ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ ë¡œì§\"\"\"
          training_history = []
          
          # ì—í¬í¬ë³„ í›ˆë ¨ (ì˜ˆì‹œ)
          for epoch in range(10):
              # í›ˆë ¨ ìŠ¤í…
              train_loss = model.train_step(train_data)
              
              # ê²€ì¦ ìŠ¤í…
              val_loss, val_accuracy = model.validate(val_data)
              
              # ë©”íŠ¸ë¦­ ê¸°ë¡
              epoch_metrics = {
                  "train_loss": float(train_loss),
                  "val_loss": float(val_loss),
                  "val_accuracy": float(val_accuracy)
              }
              
              training_history.append(epoch_metrics)
              
          return training_history
          
      def _evaluate_model(self, model, test_data):
          \"\"\"ëª¨ë¸ í‰ê°€\"\"\"
          # ì˜ˆì¸¡ ìˆ˜í–‰
          predictions = model.predict(test_data)
          
          # ì‹¤ì œ ë¼ë²¨ (ì˜ˆì‹œ)
          y_true = test_data['labels']  # ì‹¤ì œ êµ¬í˜„ì— ë§ê²Œ ìˆ˜ì •
          
          # ë©”íŠ¸ë¦­ ê³„ì‚°
          accuracy = accuracy_score(y_true, predictions)
          precision, recall, f1, _ = precision_recall_fscore_support(
              y_true, predictions, average='weighted'
          )
          
          return {
              "accuracy": float(accuracy),
              "precision": float(precision),
              "recall": float(recall),
              "f1_score": float(f1)
          }
  ```

---

## ğŸ¯ 6.3 ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì„¤ì •

### ëª©í‘œ
ì¤‘ì•™í™”ëœ ëª¨ë¸ ì €ì¥ì†Œë¡œ ëª¨ë¸ ë²„ì „ ê´€ë¦¬ ë° ë°°í¬ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **6.3.1 ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬ì**
- **ëª¨ë¸ ë“±ë¡ ë° ê´€ë¦¬**
  ```python
  # src/registry/model_registry_manager.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from mlflow.entities.model_registry import RegisteredModel, ModelVersion
  from typing import Dict, Any, List, Optional
  import logging
  from datetime import datetime
  
  class ModelRegistryManager:
      \"\"\"MLflow ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬ì\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.logger = logging.getLogger(__name__)
          
      def register_model(self, model_name: str, run_id: str, 
                        description: Optional[str] = None,
                        tags: Optional[Dict[str, str]] = None) -> str:
          \"\"\"ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì— ëª¨ë¸ ë“±ë¡\"\"\"
          try:
              # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ URI ìƒì„±
              model_uri = f"runs:/{run_id}/movie_recommender"
              
              # ëª¨ë¸ ë“±ë¡
              model_version = mlflow.register_model(
                  model_uri=model_uri,
                  name=model_name
              )
              
              # ì„¤ëª… ì¶”ê°€
              if description:
                  self.client.update_model_version(
                      name=model_name,
                      version=model_version.version,
                      description=description
                  )
              
              # íƒœê·¸ ì¶”ê°€
              if tags:
                  for key, value in tags.items():
                      self.client.set_model_version_tag(
                          name=model_name,
                          version=model_version.version,
                          key=key,
                          value=value
                      )
              
              self.logger.info(
                  f"ëª¨ë¸ ë“±ë¡ ì™„ë£Œ: {model_name} v{model_version.version}"
              )
              
              return model_version.version
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
              raise
              
      def get_model_versions(self, model_name: str) -> List[ModelVersion]:
          \"\"\"ëª¨ë¸ì˜ ëª¨ë“  ë²„ì „ ì¡°íšŒ\"\"\"
          try:
              model_versions = self.client.search_model_versions(
                  filter_string=f"name='{model_name}'"
              )
              
              # ë²„ì „ ë²ˆí˜¸ë¡œ ì •ë ¬ (ìµœì‹  ìˆœ)
              model_versions.sort(
                  key=lambda x: int(x.version), 
                  reverse=True
              )
              
              return model_versions
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ë²„ì „ ì¡°íšŒ ì‹¤íŒ¨: {e}")
              return []
              
      def get_latest_model_version(self, model_name: str, 
                                 stage: str = "Production") -> Optional[ModelVersion]:
          \"\"\"íŠ¹ì • ìŠ¤í…Œì´ì§€ì˜ ìµœì‹  ëª¨ë¸ ë²„ì „ ì¡°íšŒ\"\"\"
          try:
              latest_versions = self.client.get_latest_versions(
                  name=model_name,
                  stages=[stage]
              )
              
              if latest_versions:
                  return latest_versions[0]
              else:
                  self.logger.warning(f"ìŠ¤í…Œì´ì§€ '{stage}'ì— ëª¨ë¸ì´ ì—†ìŒ: {model_name}")
                  return None
                  
          except Exception as e:
              self.logger.error(f"ìµœì‹  ëª¨ë¸ ë²„ì „ ì¡°íšŒ ì‹¤íŒ¨: {e}")
              return None
              
      def transition_model_stage(self, model_name: str, version: str, 
                               stage: str, archive_existing: bool = True) -> bool:
          \"\"\"ëª¨ë¸ ìŠ¤í…Œì´ì§€ ì „í™˜\"\"\"
          try:
              # ê¸°ì¡´ í”„ë¡œë•ì…˜ ëª¨ë¸ì„ ì•„ì¹´ì´ë¸Œë¡œ ì´ë™ (ì„ íƒì‚¬í•­)
              if stage == "Production" and archive_existing:
                  current_prod = self.get_latest_model_version(model_name, "Production")
                  if current_prod:
                      self.client.transition_model_version_stage(
                          name=model_name,
                          version=current_prod.version,
                          stage="Archived"
                      )
                      self.logger.info(
                          f"ê¸°ì¡´ í”„ë¡œë•ì…˜ ëª¨ë¸ ì•„ì¹´ì´ë¸Œ: v{current_prod.version}"
                      )
              
              # ìƒˆ ëª¨ë¸ì„ í•´ë‹¹ ìŠ¤í…Œì´ì§€ë¡œ ì „í™˜
              self.client.transition_model_version_stage(
                  name=model_name,
                  version=version,
                  stage=stage
              )
              
              self.logger.info(
                  f"ëª¨ë¸ ìŠ¤í…Œì´ì§€ ì „í™˜ ì™„ë£Œ: {model_name} v{version} -> {stage}"
              )
              
              return True
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ìŠ¤í…Œì´ì§€ ì „í™˜ ì‹¤íŒ¨: {e}")
              return False
              
      def compare_model_versions(self, model_name: str, 
                               version1: str, version2: str) -> Dict[str, Any]:
          \"\"\"ë‘ ëª¨ë¸ ë²„ì „ ë¹„êµ\"\"\"
          try:
              # ëª¨ë¸ ë²„ì „ ì •ë³´ ì¡°íšŒ
              mv1 = self.client.get_model_version(model_name, version1)
              mv2 = self.client.get_model_version(model_name, version2)
              
              # ì‹¤í–‰ ì •ë³´ ì¡°íšŒ
              run1 = self.client.get_run(mv1.run_id)
              run2 = self.client.get_run(mv2.run_id)
              
              comparison = {
                  "model_name": model_name,
                  "version1": {
                      "version": version1,
                      "stage": mv1.current_stage,
                      "metrics": run1.data.metrics,
                      "params": run1.data.params,
                      "creation_time": mv1.creation_timestamp
                  },
                  "version2": {
                      "version": version2,
                      "stage": mv2.current_stage,
                      "metrics": run2.data.metrics,
                      "params": run2.data.params,
                      "creation_time": mv2.creation_timestamp
                  }
              }
              
              # ì„±ëŠ¥ ì°¨ì´ ê³„ì‚°
              if 'accuracy' in run1.data.metrics and 'accuracy' in run2.data.metrics:
                  acc_diff = run2.data.metrics['accuracy'] - run1.data.metrics['accuracy']
                  comparison['performance_diff'] = {
                      'accuracy_improvement': acc_diff,
                      'better_model': version2 if acc_diff > 0 else version1
                  }
              
              return comparison
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ë²„ì „ ë¹„êµ ì‹¤íŒ¨: {e}")
              return {}
              
      def delete_model_version(self, model_name: str, version: str) -> bool:
          \"\"\"ëª¨ë¸ ë²„ì „ ì‚­ì œ\"\"\"
          try:
              self.client.delete_model_version(
                  name=model_name,
                  version=version
              )
              
              self.logger.info(f"ëª¨ë¸ ë²„ì „ ì‚­ì œ ì™„ë£Œ: {model_name} v{version}")
              return True
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ë²„ì „ ì‚­ì œ ì‹¤íŒ¨: {e}")
              return False
              
      def get_model_lineage(self, model_name: str, version: str) -> Dict[str, Any]:
          \"\"\"ëª¨ë¸ ê³„ë³´ ì •ë³´ ì¡°íšŒ\"\"\"
          try:
              model_version = self.client.get_model_version(model_name, version)
              run = self.client.get_run(model_version.run_id)
              
              lineage = {
                  "model_name": model_name,
                  "version": version,
                  "run_id": model_version.run_id,
                  "experiment_id": run.info.experiment_id,
                  "creation_time": model_version.creation_timestamp,
                  "source": model_version.source,
                  "current_stage": model_version.current_stage,
                  "tags": model_version.tags,
                  "metrics": run.data.metrics,
                  "parameters": run.data.params
              }
              
              # ë°ì´í„°ì…‹ ì •ë³´ ì¶”ì¶œ
              dataset_params = {
                  k: v for k, v in run.data.params.items() 
                  if k.startswith('dataset_')
              }
              if dataset_params:
                  lineage['dataset_info'] = dataset_params
              
              return lineage
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ê³„ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
              return {}
  ```

---

## ğŸ¯ 6.4 ëª¨ë¸ ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶•

### ëª©í‘œ
í”„ë¡œë•ì…˜ ë°°í¬ ì „ ëª¨ë¸ í’ˆì§ˆ ê²€ì¦ ë° ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **6.4.1 ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ**
- **ìë™ ëª¨ë¸ ê²€ì¦**
  ```python
  # src/validation/model_validator.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from typing import Dict, Any, List, Tuple, Optional
  import numpy as np
  import logging
  from datetime import datetime, timedelta
  
  class ModelValidator:
      \"\"\"ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œ\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.logger = logging.getLogger(__name__)
          
          # ê²€ì¦ ê¸°ì¤€ ì„¤ì •
          self.validation_criteria = {
              "min_accuracy": 0.75,
              "min_precision": 0.70,
              "min_recall": 0.65,
              "max_inference_time_ms": 100,
              "min_data_quality_score": 0.8
          }
          
      def validate_model_version(self, model_name: str, version: str) -> Dict[str, Any]:
          \"\"\"ëª¨ë¸ ë²„ì „ ê²€ì¦\"\"\"
          try:
              # ëª¨ë¸ ë²„ì „ ì •ë³´ ì¡°íšŒ
              model_version = self.client.get_model_version(model_name, version)
              run = self.client.get_run(model_version.run_id)
              
              validation_results = {
                  "model_name": model_name,
                  "version": version,
                  "validation_timestamp": datetime.now().isoformat(),
                  "passed": True,
                  "validation_details": {},
                  "recommendations": []
              }
              
              # 1. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€ì¦
              performance_check = self._validate_performance_metrics(run.data.metrics)
              validation_results["validation_details"]["performance"] = performance_check
              
              if not performance_check["passed"]:
                  validation_results["passed"] = False
              
              # 2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
              data_quality_check = self._validate_data_quality(run.data.params)
              validation_results["validation_details"]["data_quality"] = data_quality_check
              
              if not data_quality_check["passed"]:
                  validation_results["passed"] = False
              
              # 3. ëª¨ë¸ í¬ê¸° ë° ë³µì¡ë„ ê²€ì¦
              complexity_check = self._validate_model_complexity(run.data.params)
              validation_results["validation_details"]["complexity"] = complexity_check
              
              # 4. ì´ì „ ëª¨ë¸ê³¼ì˜ ë¹„êµ
              comparison_check = self._compare_with_baseline(model_name, run.data.metrics)
              validation_results["validation_details"]["baseline_comparison"] = comparison_check
              
              if not comparison_check["passed"]:
                  validation_results["passed"] = False
              
              # ê²€ì¦ ê²°ê³¼ë¥¼ ëª¨ë¸ ë²„ì „ì— íƒœê·¸ë¡œ ì¶”ê°€
              self._add_validation_tags(model_name, version, validation_results)
              
              return validation_results
              
          except Exception as e:
              self.logger.error(f"ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
              return {
                  "passed": False,
                  "error": str(e),
                  "validation_timestamp": datetime.now().isoformat()
              }
              
      def _validate_performance_metrics(self, metrics: Dict[str, float]) -> Dict[str, Any]:
          \"\"\"ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê²€ì¦\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "failed_criteria": []
          }
          
          # ì •í™•ë„ ê²€ì¦
          if "accuracy" in metrics:
              accuracy = metrics["accuracy"]
              check_results["details"]["accuracy"] = {
                  "value": accuracy,
                  "threshold": self.validation_criteria["min_accuracy"],
                  "passed": accuracy >= self.validation_criteria["min_accuracy"]
              }
              
              if accuracy < self.validation_criteria["min_accuracy"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("accuracy")
          
          # ì •ë°€ë„ ê²€ì¦
          if "precision" in metrics:
              precision = metrics["precision"]
              check_results["details"]["precision"] = {
                  "value": precision,
                  "threshold": self.validation_criteria["min_precision"],
                  "passed": precision >= self.validation_criteria["min_precision"]
              }
              
              if precision < self.validation_criteria["min_precision"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("precision")
          
          # ì¬í˜„ìœ¨ ê²€ì¦
          if "recall" in metrics:
              recall = metrics["recall"]
              check_results["details"]["recall"] = {
                  "value": recall,
                  "threshold": self.validation_criteria["min_recall"],
                  "passed": recall >= self.validation_criteria["min_recall"]
              }
              
              if recall < self.validation_criteria["min_recall"]:
                  check_results["passed"] = False
                  check_results["failed_criteria"].append("recall")
          
          return check_results
          
      def _validate_data_quality(self, params: Dict[str, str]) -> Dict[str, Any]:
          \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "warnings": []
          }
          
          # ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸
          if "dataset_train_size" in params:
              train_size = int(params["dataset_train_size"])
              check_results["details"]["train_size"] = train_size
              
              if train_size < 1000:
                  check_results["warnings"].append("Training dataset is small (<1000 samples)")
          
          # í”¼ì²˜ ìˆ˜ í™•ì¸
          if "dataset_feature_count" in params:
              feature_count = int(params["dataset_feature_count"])
              check_results["details"]["feature_count"] = feature_count
              
              if feature_count < 3:
                  check_results["warnings"].append("Low number of features")
          
          return check_results
          
      def _validate_model_complexity(self, params: Dict[str, str]) -> Dict[str, Any]:
          \"\"\"ëª¨ë¸ ë³µì¡ë„ ê²€ì¦\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "warnings": []
          }
          
          # í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²€ì¦
          if "hidden_size" in params:
              hidden_size = int(params["hidden_size"])
              check_results["details"]["hidden_size"] = hidden_size
              
              if hidden_size > 1000:
                  check_results["warnings"].append("Large hidden layer size may cause overfitting")
          
          if "learning_rate" in params:
              lr = float(params["learning_rate"])
              check_results["details"]["learning_rate"] = lr
              
              if lr > 0.1:
                  check_results["warnings"].append("High learning rate may cause instability")
              elif lr < 0.0001:
                  check_results["warnings"].append("Very low learning rate may slow convergence")
          
          return check_results
          
      def _compare_with_baseline(self, model_name: str, 
                                current_metrics: Dict[str, float]) -> Dict[str, Any]:
          \"\"\"ê¸°ì¤€ ëª¨ë¸ê³¼ ë¹„êµ\"\"\"
          check_results = {
              "passed": True,
              "details": {},
              "improvement": {}
          }
          
          try:
              # í˜„ì¬ í”„ë¡œë•ì…˜ ëª¨ë¸ ì¡°íšŒ
              prod_versions = self.client.get_latest_versions(
                  name=model_name,
                  stages=["Production"]
              )
              
              if not prod_versions:
                  check_results["details"]["baseline"] = "No production model to compare"
                  return check_results
              
              # ê¸°ì¤€ ëª¨ë¸ ë©”íŠ¸ë¦­ ì¡°íšŒ
              baseline_run = self.client.get_run(prod_versions[0].run_id)
              baseline_metrics = baseline_run.data.metrics
              
              # ì„±ëŠ¥ ë¹„êµ
              if "accuracy" in current_metrics and "accuracy" in baseline_metrics:
                  current_acc = current_metrics["accuracy"]
                  baseline_acc = baseline_metrics["accuracy"]
                  improvement = current_acc - baseline_acc
                  
                  check_results["improvement"]["accuracy"] = {
                      "current": current_acc,
                      "baseline": baseline_acc,
                      "improvement": improvement,
                      "improvement_percent": (improvement / baseline_acc) * 100
                  }
                  
                  # ì„±ëŠ¥ì´ í˜„ì €íˆ ë–¨ì–´ì§€ëŠ” ê²½ìš° ì‹¤íŒ¨
                  if improvement < -0.05:  # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜
                      check_results["passed"] = False
                      check_results["details"]["failure_reason"] = "Significant performance degradation"
              
              return check_results
              
          except Exception as e:
              check_results["details"]["comparison_error"] = str(e)
              return check_results
              
      def _add_validation_tags(self, model_name: str, version: str, 
                             validation_results: Dict[str, Any]):
          \"\"\"ê²€ì¦ ê²°ê³¼ë¥¼ ëª¨ë¸ íƒœê·¸ì— ì¶”ê°€\"\"\"
          try:
              # ê²€ì¦ ìƒíƒœ íƒœê·¸
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="validation_status",
                  value="passed" if validation_results["passed"] else "failed"
              )
              
              # ê²€ì¦ ì‹œê°„ íƒœê·¸
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="validation_timestamp",
                  value=validation_results["validation_timestamp"]
              )
              
              # ì„±ëŠ¥ ë©”íŠ¸ë¦­ íƒœê·¸
              if "performance" in validation_results["validation_details"]:
                  perf = validation_results["validation_details"]["performance"]
                  if "details" in perf and "accuracy" in perf["details"]:
                      acc_info = perf["details"]["accuracy"]
                      self.client.set_model_version_tag(
                          name=model_name,
                          version=version,
                          key="validation_accuracy",
                          value=str(acc_info["value"])
                      )
              
          except Exception as e:
              self.logger.error(f"ê²€ì¦ íƒœê·¸ ì¶”ê°€ ì‹¤íŒ¨: {e}")
  ```

#### **6.4.2 ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°**
- **ìë™ ìŠ¹ì¸ ì‹œìŠ¤í…œ**
  ```python
  # src/approval/approval_workflow.py
  import mlflow
  from mlflow.tracking import MlflowClient
  from typing import Dict, Any, List, Optional
  import logging
  from datetime import datetime
  from enum import Enum
  
  from ..validation.model_validator import ModelValidator
  from ..registry.model_registry_manager import ModelRegistryManager
  
  class ApprovalStatus(Enum):
      PENDING = "pending"
      APPROVED = "approved"
      REJECTED = "rejected"
      NEEDS_REVIEW = "needs_review"
  
  class ModelApprovalWorkflow:
      \"\"\"ëª¨ë¸ ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš°\"\"\"
      
      def __init__(self, tracking_uri: str = "http://localhost:5000"):
          self.client = MlflowClient(tracking_uri=tracking_uri)
          self.validator = ModelValidator(tracking_uri)
          self.registry_manager = ModelRegistryManager(tracking_uri)
          self.logger = logging.getLogger(__name__)
          
          # ìë™ ìŠ¹ì¸ ê¸°ì¤€
          self.auto_approval_criteria = {
              "min_accuracy_improvement": 0.02,  # 2% ì´ìƒ ê°œì„ 
              "max_performance_degradation": 0.01,  # 1% ì´í•˜ ì„±ëŠ¥ ì €í•˜
              "required_validations": ["performance", "data_quality"]
          }
          
      def submit_for_approval(self, model_name: str, version: str,
                            submitted_by: str = "system",
                            notes: Optional[str] = None) -> Dict[str, Any]:
          \"\"\"ìŠ¹ì¸ ìš”ì²­ ì œì¶œ\"\"\"
          try:
              # 1. ëª¨ë¸ ê²€ì¦ ì‹¤í–‰
              validation_results = self.validator.validate_model_version(model_name, version)
              
              # 2. ìŠ¹ì¸ ìƒíƒœ ê²°ì •
              approval_decision = self._determine_approval_status(validation_results)
              
              # 3. ìŠ¹ì¸ ë©”íƒ€ë°ì´í„° ìƒì„±
              approval_metadata = {
                  "model_name": model_name,
                  "version": version,
                  "submitted_by": submitted_by,
                  "submission_timestamp": datetime.now().isoformat(),
                  "status": approval_decision["status"].value,
                  "validation_results": validation_results,
                  "approval_reason": approval_decision["reason"],
                  "notes": notes
              }
              
              # 4. ìŠ¹ì¸ ì •ë³´ë¥¼ ëª¨ë¸ íƒœê·¸ì— ì €ì¥
              self._save_approval_metadata(model_name, version, approval_metadata)
              
              # 5. ìë™ ìŠ¹ì¸ëœ ê²½ìš° ìŠ¤í…Œì´ì§€ ì „í™˜
              if approval_decision["status"] == ApprovalStatus.APPROVED:
                  self._auto_promote_model(model_name, version)
              
              self.logger.info(
                  f"ìŠ¹ì¸ ìš”ì²­ ì²˜ë¦¬ ì™„ë£Œ: {model_name} v{version} - {approval_decision['status'].value}"
              )
              
              return approval_metadata
              
          except Exception as e:
              self.logger.error(f"ìŠ¹ì¸ ìš”ì²­ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
              raise
              
      def _determine_approval_status(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
          \"\"\"ìŠ¹ì¸ ìƒíƒœ ê²°ì •\"\"\"
          
          # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ê±°ë¶€
          if not validation_results.get("passed", False):
              return {
                  "status": ApprovalStatus.REJECTED,
                  "reason": "Model failed validation checks"
              }
          
          # ì„±ëŠ¥ ê°œì„  í™•ì¸
          baseline_comparison = validation_results.get("validation_details", {}).get("baseline_comparison", {})
          
          if "improvement" in baseline_comparison and "accuracy" in baseline_comparison["improvement"]:
              accuracy_improvement = baseline_comparison["improvement"]["accuracy"]["improvement"]
              
              # ìƒë‹¹í•œ ì„±ëŠ¥ ê°œì„  ì‹œ ìë™ ìŠ¹ì¸
              if accuracy_improvement >= self.auto_approval_criteria["min_accuracy_improvement"]:
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": f"Significant accuracy improvement: {accuracy_improvement:.3f}"
                  }
              
              # ì„±ëŠ¥ ì €í•˜ê°€ ì„ê³„ê°’ ë‚´ì¸ ê²½ìš° ìŠ¹ì¸
              elif accuracy_improvement >= -self.auto_approval_criteria["max_performance_degradation"]:
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": "Performance maintained within acceptable range"
                  }
              
              # ì„±ëŠ¥ ì €í•˜ê°€ ì‹¬í•œ ê²½ìš° ê²€í†  í•„ìš”
              else:
                  return {
                      "status": ApprovalStatus.NEEDS_REVIEW,
                      "reason": f"Performance degradation requires review: {accuracy_improvement:.3f}"
                  }
          
          # ê¸°ì¤€ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° (ì²« ë²ˆì§¸ ëª¨ë¸)
          else:
              performance = validation_results.get("validation_details", {}).get("performance", {})
              if performance.get("passed", False):
                  return {
                      "status": ApprovalStatus.APPROVED,
                      "reason": "First model meets validation criteria"
                  }
              else:
                  return {
                      "status": ApprovalStatus.NEEDS_REVIEW,
                      "reason": "First model requires manual review"
                  }
              
      def _save_approval_metadata(self, model_name: str, version: str, 
                                 approval_metadata: Dict[str, Any]):
          \"\"\"ìŠ¹ì¸ ë©”íƒ€ë°ì´í„° ì €ì¥\"\"\"
          try:
              # ì£¼ìš” ìŠ¹ì¸ ì •ë³´ë¥¼ íƒœê·¸ë¡œ ì €ì¥
              tags_to_set = {
                  "approval_status": approval_metadata["status"],
                  "approval_timestamp": approval_metadata["submission_timestamp"],
                  "approval_reason": approval_metadata["approval_reason"],
                  "submitted_by": approval_metadata["submitted_by"]
              }
              
              for key, value in tags_to_set.items():
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key=key,
                      value=str(value)
                  )
              
              # ìƒì„¸ ìŠ¹ì¸ ì •ë³´ë¥¼ ì•„í‹°íŒ©íŠ¸ë¡œ ì €ì¥
              import json
              import tempfile
              
              with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                  json.dump(approval_metadata, f, indent=2, default=str)
                  f.flush()
                  
                  # ëª¨ë¸ ë²„ì „ì˜ run_id ì¡°íšŒ
                  model_version = self.client.get_model_version(model_name, version)
                  
                  # ì•„í‹°íŒ©íŠ¸ ë¡œê¹… (ìƒˆë¡œìš´ runì—ì„œ)
                  with mlflow.start_run(run_id=model_version.run_id):
                      mlflow.log_artifact(f.name, artifact_path="approval/approval_metadata.json")
              
          except Exception as e:
              self.logger.error(f"ìŠ¹ì¸ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
              
      def _auto_promote_model(self, model_name: str, version: str):
          \"\"\"ìë™ ëª¨ë¸ ìŠ¹ê¸‰\"\"\"
          try:
              # ëª¨ë¸ì„ Stagingìœ¼ë¡œ ì „í™˜
              success = self.registry_manager.transition_model_stage(
                  model_name=model_name,
                  version=version,
                  stage="Staging",
                  archive_existing=True
              )
              
              if success:
                  # ìŠ¹ê¸‰ íƒœê·¸ ì¶”ê°€
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="auto_promoted",
                      value="true"
                  )
                  
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="promotion_timestamp",
                      value=datetime.now().isoformat()
                  )
                  
                  self.logger.info(f"ìë™ ìŠ¹ê¸‰ ì™„ë£Œ: {model_name} v{version} -> Staging")
              
          except Exception as e:
              self.logger.error(f"ìë™ ìŠ¹ê¸‰ ì‹¤íŒ¨: {e}")
              
      def get_pending_approvals(self) -> List[Dict[str, Any]]:
          \"\"\"ìŠ¹ì¸ ëŒ€ê¸° ì¤‘ì¸ ëª¨ë¸ ì¡°íšŒ\"\"\"
          try:
              # ëª¨ë“  ë“±ë¡ëœ ëª¨ë¸ ì¡°íšŒ
              registered_models = self.client.search_registered_models()
              
              pending_approvals = []
              
              for model in registered_models:
                  # ê° ëª¨ë¸ì˜ ë²„ì „ë“¤ ì¡°íšŒ
                  versions = self.client.search_model_versions(
                      filter_string=f"name='{model.name}'"
                  )
                  
                  for version in versions:
                      # ìŠ¹ì¸ ìƒíƒœ í™•ì¸
                      approval_status = version.tags.get("approval_status", "unknown")
                      
                      if approval_status in ["pending", "needs_review"]:
                          pending_approvals.append({
                              "model_name": model.name,
                              "version": version.version,
                              "status": approval_status,
                              "submission_time": version.tags.get("approval_timestamp", "unknown"),
                              "current_stage": version.current_stage
                          })
              
              return pending_approvals
              
          except Exception as e:
              self.logger.error(f"ìŠ¹ì¸ ëŒ€ê¸° ëª¨ë¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
              return []
              
      def manual_approve(self, model_name: str, version: str, 
                        approved_by: str, notes: Optional[str] = None) -> bool:
          \"\"\"ìˆ˜ë™ ìŠ¹ì¸\"\"\"
          try:
              # ìŠ¹ì¸ íƒœê·¸ ì—…ë°ì´íŠ¸
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="approval_status",
                  value="approved"
              )
              
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="approved_by",
                  value=approved_by
              )
              
              self.client.set_model_version_tag(
                  name=model_name,
                  version=version,
                  key="manual_approval_timestamp",
                  value=datetime.now().isoformat()
              )
              
              if notes:
                  self.client.set_model_version_tag(
                      name=model_name,
                      version=version,
                      key="approval_notes",
                      value=notes
                  )
              
              # Stagingìœ¼ë¡œ ìŠ¹ê¸‰
              self._auto_promote_model(model_name, version)
              
              self.logger.info(f"ìˆ˜ë™ ìŠ¹ì¸ ì™„ë£Œ: {model_name} v{version} by {approved_by}")
              return True
              
          except Exception as e:
              self.logger.error(f"ìˆ˜ë™ ìŠ¹ì¸ ì‹¤íŒ¨: {e}")
              return False
  ```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### 6.4.1 ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ëª¨ë“  ì‹¤í—˜ê³¼ ëª¨ë¸ì´ MLflowì—ì„œ ì¤‘ì•™ ê´€ë¦¬ë¨
- [ ] ëª¨ë¸ ë²„ì „ë³„ë¡œ ì„±ëŠ¥ê³¼ ë©”íƒ€ë°ì´í„°ê°€ ìë™ ì¶”ì ë¨
- [ ] ëª¨ë¸ ê²€ì¦ ì‹œìŠ¤í…œì´ í’ˆì§ˆ ê¸°ì¤€ì— ë”°ë¼ ìë™ ê²€ì¦ ìˆ˜í–‰
- [ ] ìŠ¹ì¸ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ê²€ì¦ëœ ëª¨ë¸ë§Œ í”„ë¡œë•ì…˜ ë°°í¬
- [ ] ëª¨ë¸ ë¹„êµ ë° ê³„ë³´ ì¶”ì ì´ ê°€ëŠ¥í•¨

### 6.4.2 ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] MLflow Tracking Server ì•ˆì • ìš´ì˜
- [ ] PostgreSQL ë°±ì—”ë“œ ìŠ¤í† ì–´ êµ¬ì„± ì™„ë£Œ
- [ ] MinIO ì•„í‹°íŒ©íŠ¸ ìŠ¤í† ì–´ ì—°ë™ ì™„ë£Œ
- [ ] ìë™ ëª¨ë¸ ê²€ì¦ ë° ìŠ¹ì¸ ì›Œí¬í”Œë¡œìš° êµ¬í˜„
- [ ] ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ API í†µí•© ì™„ë£Œ

### 6.4.3 ìš´ì˜ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ëª¨ë¸ ë“±ë¡ë¶€í„° ë°°í¬ê¹Œì§€ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ìë™í™”
- [ ] ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ì‹œ ìë™ ê°ì§€ ë° ëŒ€ì‘
- [ ] ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ ì •ì±… ì¤€ìˆ˜ í™•ì¸
- [ ] ëª¨ë¸ ê°ì‚¬ ì¶”ì (audit trail) ì™„ì „ êµ¬í˜„
- [ ] íŒ€ì› ëŒ€ìƒ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ì‚¬ìš© êµìœ¡ ì™„ë£Œ

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

### 7ë‹¨ê³„ ì—°ê³„ ì‘ì—…
- ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ì—ì„œ ìŠ¹ì¸ëœ ëª¨ë¸ì„ 7ë‹¨ê³„ ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œìœ¼ë¡œ ìë™ ë°°í¬
- ì„œë¹™ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¡œ í”¼ë“œë°±
- A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ëª¨ë¸ ë²„ì „ ë¹„êµì— í™œìš©

### ê³ ë„í™” ê³„íš
- MLflow Projectsë¥¼ í™œìš©í•œ ì¬í˜„ ê°€ëŠ¥í•œ ì‹¤í—˜ í™˜ê²½
- ëª¨ë¸ ì„±ëŠ¥ ìë™ ëª¨ë‹ˆí„°ë§ ë° ì¬í›ˆë ¨ íŠ¸ë¦¬ê±°
- ë‹¤ì¤‘ í™˜ê²½(dev/staging/prod) ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ êµ¬ì„±
- ëª¨ë¸ ë°°í¬ ìë™í™” ë° ì¹´ë‚˜ë¦¬ ë°°í¬ ì§€ì›

ì´ 6ë‹¨ê³„ë¥¼ ì™„ë£Œí•˜ë©´ ëª¨ë“  ëª¨ë¸ì´ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬ë˜ê³  ê²€ì¦ëœ ëª¨ë¸ë§Œ í”„ë¡œë•ì…˜ì— ë°°í¬ë˜ëŠ” ê²¬ê³ í•œ ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤ ì²´ê³„ê°€ êµ¬ì¶•ë©ë‹ˆë‹¤.
