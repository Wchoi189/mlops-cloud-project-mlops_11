---
title: "1.7 ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶• - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ"
description: "ì²´ê³„ì ì¸ ë¡œê·¸ ê´€ë¦¬ë¡œ ì‹œìŠ¤í…œ ìš´ì˜ ê°€ì‹œì„± í™•ë³´ ë° ë¬¸ì œ ì¶”ì  ìë™í™”"
stage: "01-data-processing"
phase: "implementation"
step: "1.7"
category: "logging-system"
difficulty: "intermediate"
estimated_time: "8-12 hours"
tags:
  - logging
  - monitoring
  - system-observability
  - centralized-logging
  - log-analysis
  - performance-monitoring
authors:
  - mlops-team
last_updated: "2025-06-06"
version: "1.0"
status: "active"
prerequisites:
  - "1.1-1.5 ë‹¨ê³„ êµ¬í˜„ ì™„ë£Œ"
  - "ê¸°ë³¸ ë¡œê¹… ê°œë… ì´í•´"
outcomes:
  - "ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•"
  - "ì»´í¬ë„ŒíŠ¸ë³„ ë¡œê±° ìƒì„± ë° ê´€ë¦¬"
  - "ë¡œê¹… ë°ì½”ë ˆì´í„° ì‹œìŠ¤í…œ êµ¬í˜„"
  - "ë¡œê·¸ ë¶„ì„ ë° íŒ¨í„´ íƒì§€ ë„êµ¬"
  - "ì¼ì¼ ë¡œê·¸ ë¦¬í¬íŠ¸ ìë™ ìƒì„±"
related_docs:
  - "1.5-data-quality-validation-guide.md"
  - "1.8-apache-airflow-basic-setup-guide.md"
  - "../testing/2.comprehensive-testing-guide.md"
---

# 1.7 ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶• - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ì²´ê³„ì ì¸ ë¡œê·¸ ê´€ë¦¬ë¡œ ì‹œìŠ¤í…œ ìš´ì˜ ê°€ì‹œì„± í™•ë³´ ë° ë¬¸ì œ ì¶”ì  ìë™í™”

**í™˜ê²½**: WSL Ubuntu 24.04 + Docker + Python 3.11

**í•µì‹¬ ê°€ì¹˜**: í¬ê´„ì ì¸ ë¡œê¹…ì„ í†µí•œ ì‹œìŠ¤í…œ ì•ˆì •ì„± ë³´ì¥ ë° ìš´ì˜ íš¨ìœ¨ì„± í–¥ìƒ

---

## ğŸ¯ 1.6.1 ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ì•„í‚¤í…ì²˜

### ëª©í‘œ
ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ë¡œê·¸ë¥¼ ì¤‘ì•™ì—ì„œ ìˆ˜ì§‘, ì €ì¥, ë¶„ì„í•  ìˆ˜ ìˆëŠ” í†µí•© ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

### ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¡° ì„¤ê³„

```bash
# ë¡œê¹… ì‹œìŠ¤í…œ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
docker exec mlops-dev bash -c "
mkdir -p src/logging_system/{config,handlers,formatters,filters,analyzers}
mkdir -p logs/{app,error,system,audit,performance,reports,alerts}
mkdir -p logs/archive/{2024,2025}
"
```

**ì¤‘ì•™ ë¡œê¹… ê´€ë¦¬ì êµ¬í˜„**:
```bash
docker exec mlops-dev bash -c "
cat > src/logging_system/log_manager.py << 'EOF'
\"\"\"
ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ê´€ë¦¬ ì‹œìŠ¤í…œ
ëª¨ë“  ì»´í¬ë„ŒíŠ¸ì˜ ë¡œê·¸ë¥¼ í†µí•©í•˜ì—¬ ê´€ë¦¬
\"\"\"

import logging
import logging.handlers
import logging.config
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional

class MovieMLOpsLogger:
    \"\"\"Movie MLOps í”„ë¡œì íŠ¸ ì „ìš© ë¡œê±°\"\"\"
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MovieMLOpsLogger, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if hasattr(self, '_initialized'):
            return
        
        self._initialized = True
        self.log_dir = Path('/app/logs')
        self.config_dir = Path('/app/src/logging_system/config')
        self.loggers = {}
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        self._create_log_directories()
        
        # ë¡œê¹… ì„¤ì • ì´ˆê¸°í™”
        self._setup_logging_config()
        
        # ì‹œìŠ¤í…œ ë¡œê±° ìƒì„±
        self.system_logger = self.get_logger('system', 'system.log')
        self.system_logger.info(\"MovieMLOps ë¡œê¹… ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ\")
    
    def _create_log_directories(self):
        \"\"\"ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±\"\"\"
        log_dirs = [
            'app', 'error', 'system', 'audit', 'performance',
            'reports', 'alerts', 'archive/2024', 'archive/2025'
        ]
        
        for dir_name in log_dirs:
            (self.log_dir / dir_name).mkdir(parents=True, exist_ok=True)
    
    def _setup_logging_config(self):
        \"\"\"ë¡œê¹… ì„¤ì • êµ¬ì„±\"\"\"
        config = {
            'version': 1,
            'disable_existing_loggers': False,
            'formatters': {
                'detailed': {
                    'format': '[{asctime}] {levelname:8} {name:15} {funcName:20} L{lineno:3d} | {message}',
                    'style': '{',
                    'datefmt': '%Y-%m-%d %H:%M:%S'
                },
                'simple': {
                    'format': '[{asctime}] {levelname:8} | {message}',
                    'style': '{',
                    'datefmt': '%H:%M:%S'
                },
                'performance': {
                    'format': '[{asctime}] PERF {name:15} | {message}',
                    'style': '{',
                    'datefmt': '%Y-%m-%d %H:%M:%S'
                }
            },
            'handlers': {
                'console': {
                    'class': 'logging.StreamHandler',
                    'level': 'INFO',
                    'formatter': 'simple',
                    'stream': 'ext://sys.stdout'
                },
                'file': {
                    'class': 'logging.handlers.RotatingFileHandler',
                    'level': 'DEBUG',
                    'formatter': 'detailed',
                    'filename': str(self.log_dir / 'app' / 'application.log'),
                    'maxBytes': 10485760,  # 10MB
                    'backupCount': 5
                },
                'error_file': {
                    'class': 'logging.handlers.RotatingFileHandler',
                    'level': 'ERROR',
                    'formatter': 'detailed',
                    'filename': str(self.log_dir / 'error' / 'error.log'),
                    'maxBytes': 10485760,
                    'backupCount': 10
                }
            },
            'loggers': {
                'root': {
                    'level': 'DEBUG',
                    'handlers': ['console', 'file', 'error_file']
                }
            }
        }
        
        # ì„¤ì • ì €ì¥
        config_file = self.config_dir / 'logging_config.json'
        config_file.parent.mkdir(parents=True, exist_ok=True)
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        
        # ë¡œê¹… ì„¤ì • ì ìš©
        logging.config.dictConfig(config)
    
    def get_logger(self, name: str, log_file: str = None, level: str = 'INFO') -> logging.Logger:
        \"\"\"íŠ¹ì • ì»´í¬ë„ŒíŠ¸ìš© ë¡œê±° ìƒì„±\"\"\"
        logger_key = f\"{name}_{log_file or 'default'}\"
        
        if logger_key in self.loggers:
            return self.loggers[logger_key]
        
        logger = logging.getLogger(name)
        logger.setLevel(getattr(logging, level.upper()))
        
        # íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ (ì§€ì •ëœ ê²½ìš°)
        if log_file:
            file_handler = logging.handlers.RotatingFileHandler(
                str(self.log_dir / 'app' / log_file),
                maxBytes=10485760,
                backupCount=5
            )
            file_handler.setFormatter(
                logging.Formatter(
                    '[{asctime}] {levelname:8} {name:15} | {message}',
                    style='{',
                    datefmt='%Y-%m-%d %H:%M:%S'
                )
            )
            logger.addHandler(file_handler)
        
        self.loggers[logger_key] = logger
        return logger
    
    def get_performance_logger(self, component: str) -> logging.Logger:
        \"\"\"ì„±ëŠ¥ ì¸¡ì • ì „ìš© ë¡œê±°\"\"\"
        perf_logger = logging.getLogger(f'performance.{component}')
        
        if not perf_logger.handlers:
            handler = logging.handlers.RotatingFileHandler(
                str(self.log_dir / 'performance' / f'{component}_performance.log'),
                maxBytes=10485760,
                backupCount=3
            )
            handler.setFormatter(
                logging.Formatter(
                    '[{asctime}] PERF {name:15} | {message}',
                    style='{',
                    datefmt='%Y-%m-%d %H:%M:%S'
                )
            )
            perf_logger.addHandler(handler)
            perf_logger.setLevel(logging.INFO)
        
        return perf_logger
    
    def log_performance(self, component: str, operation: str, duration: float, 
                       metadata: Dict[str, Any] = None):
        \"\"\"ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡\"\"\"
        perf_logger = self.get_performance_logger(component)
        metadata = metadata or {}
        
        message = f\"{operation} completed in {duration:.3f}s\"
        if metadata:
            message += f\" | {json.dumps(metadata, default=str)}\"
        
        perf_logger.info(message)
    
    def shutdown(self):
        \"\"\"ë¡œê¹… ì‹œìŠ¤í…œ ì¢…ë£Œ\"\"\"
        # ëª¨ë“  í•¸ë“¤ëŸ¬ ë‹«ê¸°
        for logger in self.loggers.values():
            for handler in logger.handlers:
                handler.close()
        
        self.system_logger.info(\"MovieMLOps ë¡œê¹… ì‹œìŠ¤í…œ ì¢…ë£Œ\")

# ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
_log_manager = None

def get_logger(name: str, log_file: str = None, level: str = 'INFO') -> logging.Logger:
    \"\"\"ì „ì—­ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜\"\"\"
    global _log_manager
    if _log_manager is None:
        _log_manager = MovieMLOpsLogger()
    return _log_manager.get_logger(name, log_file, level)

def log_performance(component: str, operation: str, duration: float, metadata: Dict[str, Any] = None):
    \"\"\"ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡\"\"\"
    global _log_manager
    if _log_manager is None:
        _log_manager = MovieMLOpsLogger()
    _log_manager.log_performance(component, operation, duration, metadata)
EOF
"
```

---

## ğŸ¯ 1.6.2 ë¡œê¹… ë°ì½”ë ˆì´í„° ë° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬

### ì‹¤í–‰ ì¶”ì  ë°ì½”ë ˆì´í„°

```bash
docker exec mlops-dev bash -c "
cat > src/logging_system/decorators.py << 'EOF'
\"\"\"
ë¡œê¹… ë°ì½”ë ˆì´í„° ëª¨ìŒ
í•¨ìˆ˜ ì‹¤í–‰ ì¶”ì , ì„±ëŠ¥ ì¸¡ì •, ì—ëŸ¬ ë¡œê¹… ìë™í™”
\"\"\"

import functools
import time
import traceback
import json
from typing import Any, Callable, Dict, List
from .log_manager import get_logger, log_performance

def log_execution(component: str = None, log_file: str = None, 
                 level: str = 'INFO', log_args: bool = False, 
                 log_result: bool = False):
    \"\"\"í•¨ìˆ˜ ì‹¤í–‰ ë¡œê¹… ë°ì½”ë ˆì´í„°\"\"\"
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            comp_name = component or func.__module__.split('.')[-1]
            logger = get_logger(comp_name, log_file, level)
            
            # í•¨ìˆ˜ ì‹œì‘ ë¡œê·¸
            start_time = time.time()
            func_name = func.__name__
            
            log_msg = f\"Starting {func_name}\"
            if log_args and (args or kwargs):
                args_str = \", \".join([str(arg)[:100] for arg in args])
                kwargs_str = \", \".join([f\"{k}={str(v)[:100]}\" for k, v in kwargs.items()])
                log_msg += f\" with args: ({args_str}) kwargs: {{{kwargs_str}}}\"
            
            logger.info(log_msg)
            
            try:
                # í•¨ìˆ˜ ì‹¤í–‰
                result = func(*args, **kwargs)
                
                # ì™„ë£Œ ë¡œê·¸
                duration = time.time() - start_time
                log_msg = f\"Completed {func_name} in {duration:.3f}s\"
                
                if log_result and result is not None:
                    result_str = str(result)[:200]
                    log_msg += f\" with result: {result_str}\"
                
                logger.info(log_msg)
                
                # ì„±ëŠ¥ ë¡œê·¸ ê¸°ë¡
                log_performance(comp_name, func_name, duration, {
                    'args_count': len(args),
                    'kwargs_count': len(kwargs)
                })
                
                return result
                
            except Exception as e:
                # ì—ëŸ¬ ë¡œê·¸
                duration = time.time() - start_time
                logger.error(f\"Failed {func_name} after {duration:.3f}s: {str(e)}\")
                logger.debug(f\"Full traceback for {func_name}: {traceback.format_exc()}\")
                raise
        
        return wrapper
    return decorator

def log_api_call(api_name: str = None, log_request: bool = True, 
                log_response: bool = False, sensitive_fields: List[str] = None):
    \"\"\"API í˜¸ì¶œ ë¡œê¹… ë°ì½”ë ˆì´í„°\"\"\"
    sensitive_fields = sensitive_fields or ['api_key', 'password', 'token']
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            api = api_name or func.__name__
            logger = get_logger('api_calls', 'api_calls.log')
            
            start_time = time.time()
            
            # ìš”ì²­ ë¡œê·¸ (ë¯¼ê°í•œ ì •ë³´ ì œì™¸)
            if log_request:
                clean_kwargs = {}
                for k, v in kwargs.items():
                    if k.lower() in sensitive_fields:
                        clean_kwargs[k] = '***HIDDEN***'
                    else:
                        clean_kwargs[k] = v
                
                logger.info(f\"API Call {api} started with params: {json.dumps(clean_kwargs, default=str)}\")
            
            try:
                result = func(*args, **kwargs)
                duration = time.time() - start_time
                
                # ì„±ê³µ ë¡œê·¸
                logger.info(f\"API Call {api} succeeded in {duration:.3f}s\")
                
                # ì„±ëŠ¥ ë¡œê·¸
                log_performance('api', api, duration, {
                    'status': 'success',
                    'response_size': len(str(result)) if result else 0
                })
                
                return result
                
            except Exception as e:
                duration = time.time() - start_time
                logger.error(f\"API Call {api} failed after {duration:.3f}s: {str(e)}\")
                
                # ì„±ëŠ¥ ë¡œê·¸ (ì‹¤íŒ¨)
                log_performance('api', api, duration, {
                    'status': 'failed',
                    'error': str(e)
                })
                
                raise
        
        return wrapper
    return decorator

class LogContext:
    \"\"\"ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬\"\"\"
    
    def __init__(self, component: str, operation: str, log_file: str = None):
        self.component = component
        self.operation = operation
        self.logger = get_logger(component, log_file)
        self.start_time = None
        self.metadata = {}
    
    def __enter__(self):
        self.start_time = time.time()
        self.logger.info(f\"{self.operation} started\")
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        duration = time.time() - self.start_time if self.start_time else 0
        
        if exc_type is None:
            self.logger.info(f\"{self.operation} completed successfully in {duration:.3f}s\")
            log_performance(self.component, self.operation, duration, self.metadata)
        else:
            self.logger.error(f\"{self.operation} failed after {duration:.3f}s: {str(exc_val)}\")
            self.logger.debug(f\"Error traceback: {traceback.format_exc()}\")
    
    def add_metadata(self, key: str, value: Any):
        \"\"\"ë©”íƒ€ë°ì´í„° ì¶”ê°€\"\"\"
        self.metadata[key] = value
    
    def log_info(self, message: str):
        \"\"\"ì»¨í…ìŠ¤íŠ¸ ë‚´ ì •ë³´ ë¡œê·¸\"\"\"
        self.logger.info(f\"{self.operation} | {message}\")
EOF
"
```

---

## ğŸ¯ 1.6.3 ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ

### ë¡œê·¸ ë¶„ì„ê¸°

```bash
docker exec mlops-dev bash -c "
cat > src/logging_system/analyzers/log_analyzer.py << 'EOF'
\"\"\"
ë¡œê·¸ ë¶„ì„ ë„êµ¬
ë¡œê·¸ íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ íŒ¨í„´, ì˜¤ë¥˜, ì„±ëŠ¥ ì´ìŠˆë¥¼ ìë™ìœ¼ë¡œ íƒì§€
\"\"\"

import re
import json
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from collections import defaultdict, Counter

class LogAnalyzer:
    \"\"\"ë¡œê·¸ íŒŒì¼ ë¶„ì„ê¸°\"\"\"
    
    def __init__(self, log_dir: str = '/app/logs'):
        self.log_dir = Path(log_dir)
        self.error_patterns = [
            (r'ERROR.*?ConnectionError', 'connection_error'),
            (r'ERROR.*?TimeoutError', 'timeout_error'),
            (r'ERROR.*?HTTP 404', 'not_found_error'),
            (r'ERROR.*?HTTP 500', 'server_error'),
            (r'ERROR.*?ValidationError', 'validation_error'),
            (r'CRITICAL', 'critical_error')
        ]
        
        self.performance_thresholds = {
            'api_call': 5.0,  # 5ì´ˆ
            'data_processing': 30.0,  # 30ì´ˆ
            'database_operation': 2.0  # 2ì´ˆ
        }
    
    def analyze_error_logs(self, hours: int = 24) -> Dict[str, Any]:
        \"\"\"ì—ëŸ¬ ë¡œê·¸ ë¶„ì„\"\"\"
        since = datetime.now() - timedelta(hours=hours)
        error_files = list(self.log_dir.glob('error/*.log'))
        
        error_analysis = {
            'analysis_period': f'Last {hours} hours',
            'total_errors': 0,
            'error_types': {},
            'error_timeline': [],
            'frequent_errors': [],
            'critical_errors': []
        }
        
        all_errors = []
        
        for log_file in error_files:
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line_num, line in enumerate(f, 1):
                        # ì‹œê°„ íŒŒì‹±
                        timestamp_match = re.search(r'\\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})', line)
                        if timestamp_match:
                            log_time = datetime.strptime(timestamp_match.group(1), '%Y-%m-%d %H:%M:%S')
                            if log_time < since:
                                continue
                        
                        # ì—ëŸ¬ íŒ¨í„´ ë§¤ì¹­
                        for pattern, error_type in self.error_patterns:
                            if re.search(pattern, line, re.IGNORECASE):
                                error_info = {
                                    'file': str(log_file),
                                    'line_num': line_num,
                                    'timestamp': log_time.isoformat() if timestamp_match else None,
                                    'type': error_type,
                                    'message': line.strip(),
                                    'severity': 'CRITICAL' if 'CRITICAL' in line else 'ERROR'
                                }
                                all_errors.append(error_info)
                                break
            
            except Exception as e:
                print(f\"Error reading {log_file}: {e}\")
                continue
        
        # ë¶„ì„ ê²°ê³¼ ì§‘ê³„
        error_analysis['total_errors'] = len(all_errors)
        
        # ì—ëŸ¬ ìœ í˜•ë³„ ì§‘ê³„
        error_type_counts = Counter(error['type'] for error in all_errors)
        error_analysis['error_types'] = dict(error_type_counts.most_common())
        
        # ì‹œê°„ëŒ€ë³„ ì—ëŸ¬ ë¶„í¬
        hourly_errors = defaultdict(int)
        for error in all_errors:
            if error['timestamp']:
                hour = datetime.fromisoformat(error['timestamp']).strftime('%Y-%m-%d %H:00')
                hourly_errors[hour] += 1
        
        error_analysis['error_timeline'] = [
            {'hour': hour, 'count': count} 
            for hour, count in sorted(hourly_errors.items())
        ]
        
        # ë¹ˆë²ˆí•œ ì—ëŸ¬ ë©”ì‹œì§€
        message_counts = Counter(error['message'][:100] for error in all_errors)
        error_analysis['frequent_errors'] = [
            {'message': msg, 'count': count}
            for msg, count in message_counts.most_common(10)
        ]
        
        # ì‹¬ê°í•œ ì—ëŸ¬
        critical_errors = [error for error in all_errors if error['severity'] == 'CRITICAL']
        error_analysis['critical_errors'] = critical_errors[-10:]  # ìµœê·¼ 10ê°œ
        
        return error_analysis
    
    def analyze_performance_logs(self, hours: int = 24) -> Dict[str, Any]:
        \"\"\"ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„\"\"\"
        since = datetime.now() - timedelta(hours=hours)
        perf_files = list(self.log_dir.glob('performance/*.log'))
        
        performance_analysis = {
            'analysis_period': f'Last {hours} hours',
            'total_operations': 0,
            'slow_operations': [],
            'component_stats': {}
        }
        
        all_operations = []
        
        for log_file in perf_files:
            try:
                with open(log_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        # ì„±ëŠ¥ ë¡œê·¸ íŒŒì‹±
                        perf_match = re.search(
                            r'\\[(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2})\\] PERF ([\\w\\.]+)\\s+\\| (\\w+) completed in ([\\d\\.]+)s',
                            line
                        )
                        
                        if perf_match:
                            timestamp_str, component, operation, duration_str = perf_match.groups()
                            log_time = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S')
                            
                            if log_time < since:
                                continue
                            
                            duration = float(duration_str)
                            
                            op_info = {
                                'timestamp': log_time.isoformat(),
                                'component': component,
                                'operation': operation,
                                'duration': duration
                            }
                            all_operations.append(op_info)
            
            except Exception as e:
                print(f\"Error reading {log_file}: {e}\")
                continue
        
        performance_analysis['total_operations'] = len(all_operations)
        
        # ëŠë¦° ì‘ì—… íƒì§€
        slow_ops = []
        for op in all_operations:
            operation_type = self._categorize_operation(op['operation'])
            threshold = self.performance_thresholds.get(operation_type, 10.0)
            
            if op['duration'] > threshold:
                slow_ops.append({
                    **op,
                    'threshold': threshold,
                    'slowness_factor': op['duration'] / threshold
                })
        
        performance_analysis['slow_operations'] = sorted(
            slow_ops, key=lambda x: x['slowness_factor'], reverse=True
        )[:20]
        
        # ì»´í¬ë„ŒíŠ¸ë³„ í†µê³„
        component_stats = defaultdict(lambda: {'count': 0, 'total_time': 0, 'avg_time': 0, 'max_time': 0})
        
        for op in all_operations:
            comp_stats = component_stats[op['component']]
            comp_stats['count'] += 1
            comp_stats['total_time'] += op['duration']
            comp_stats['max_time'] = max(comp_stats['max_time'], op['duration'])
        
        for comp, stats in component_stats.items():
            stats['avg_time'] = stats['total_time'] / stats['count'] if stats['count'] > 0 else 0
        
        performance_analysis['component_stats'] = dict(component_stats)
        
        return performance_analysis
    
    def _categorize_operation(self, operation: str) -> str:
        \"\"\"ì‘ì—… ìœ í˜• ë¶„ë¥˜\"\"\"
        operation_lower = operation.lower()
        
        if any(term in operation_lower for term in ['api', 'request', 'call']):
            return 'api_call'
        elif any(term in operation_lower for term in ['process', 'transform', 'clean']):
            return 'data_processing'
        elif any(term in operation_lower for term in ['select', 'insert', 'update', 'delete', 'query']):
            return 'database_operation'
        else:
            return 'other'
    
    def generate_health_report(self) -> Dict[str, Any]:
        \"\"\"ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"
        error_analysis = self.analyze_error_logs(24)
        perf_analysis = self.analyze_performance_logs(24)
        
        # ê±´ê°• ì ìˆ˜ ê³„ì‚° (100ì  ë§Œì )
        health_score = 100
        issues = []
        
        # ì—ëŸ¬ ì ìˆ˜ (ìµœëŒ€ 40ì  ê°ì )
        error_rate = error_analysis['total_errors']
        if error_rate > 100:
            health_score -= 40
            issues.append('high_error_rate')
        elif error_rate > 50:
            health_score -= 25
            issues.append('moderate_error_rate')
        elif error_rate > 20:
            health_score -= 10
            issues.append('some_errors')
        
        # ì‹¬ê°í•œ ì—ëŸ¬ (ìµœëŒ€ 30ì  ê°ì )
        critical_count = len(error_analysis['critical_errors'])
        if critical_count > 5:
            health_score -= 30
            issues.append('critical_errors')
        elif critical_count > 0:
            health_score -= 15
            issues.append('some_critical_errors')
        
        # ì„±ëŠ¥ ì ìˆ˜ (ìµœëŒ€ 30ì  ê°ì )
        slow_ops = len(perf_analysis['slow_operations'])
        if slow_ops > 20:
            health_score -= 30
            issues.append('performance_issues')
        elif slow_ops > 10:
            health_score -= 15
            issues.append('some_slow_operations')
        
        # ê±´ê°• ë“±ê¸‰
        if health_score >= 90:
            grade = \"ğŸŸ¢ Excellent\"
        elif health_score >= 80:
            grade = \"ğŸŸ¡ Good\"
        elif health_score >= 70:
            grade = \"ğŸŸ  Fair\"
        else:
            grade = \"ğŸ”´ Poor\"
        
        return {
            'timestamp': datetime.now().isoformat(),
            'health_score': max(0, health_score),
            'grade': grade,
            'issues': issues,
            'error_summary': {
                'total_errors': error_analysis['total_errors'],
                'critical_errors': critical_count,
                'top_error_types': list(error_analysis['error_types'].keys())[:3]
            },
            'performance_summary': {
                'total_operations': perf_analysis['total_operations'],
                'slow_operations': slow_ops,
                'slowest_component': max(
                    perf_analysis['component_stats'].items(),
                    key=lambda x: x[1]['avg_time'],
                    default=('none', {})
                )[0]
            },
            'recommendations': self._generate_recommendations(issues, error_analysis, perf_analysis)
        }
    
    def _generate_recommendations(self, issues: List[str], error_analysis: Dict, perf_analysis: Dict) -> List[str]:
        \"\"\"ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±\"\"\"
        recommendations = []
        
        if 'high_error_rate' in issues:
            recommendations.append(\"ì—ëŸ¬ ë°œìƒë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ì ê²€í•˜ê³  ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”.\")
        
        if 'critical_errors' in issues:
            recommendations.append(\"ì‹¬ê°í•œ ì—ëŸ¬ê°€ ë‹¤ìˆ˜ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ ì¡°ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")
        
        if 'performance_issues' in issues:
            recommendations.append(\"ì„±ëŠ¥ ì €í•˜ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ëŠë¦° ì‘ì—…ë“¤ì„ ìµœì í™”í•˜ì„¸ìš”.\")
        
        # ê°€ì¥ ë¹ˆë²ˆí•œ ì—ëŸ¬ ìœ í˜• ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if error_analysis['error_types']:
            top_error = list(error_analysis['error_types'].keys())[0]
            if top_error == 'connection_error':
                recommendations.append(\"ì—°ê²° ì˜¤ë¥˜ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœì™€ ì™¸ë¶€ ì„œë¹„ìŠ¤ë¥¼ ì ê²€í•˜ì„¸ìš”.\")
            elif top_error == 'timeout_error':
                recommendations.append(\"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ê°€ ë§ìŠµë‹ˆë‹¤. ìš”ì²­ ì‹œê°„ ì œí•œì„ ì¡°ì •í•˜ê±°ë‚˜ ì„±ëŠ¥ì„ ê°œì„ í•˜ì„¸ìš”.\")
        
        if not recommendations:
            recommendations.append(\"ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ìš´ì˜ë˜ê³  ìˆìŠµë‹ˆë‹¤.\")
        
        return recommendations

def generate_daily_log_report():
    \"\"\"ì¼ì¼ ë¡œê·¸ ë¦¬í¬íŠ¸ ìƒì„±\"\"\"
    analyzer = LogAnalyzer()
    
    # ê±´ê°• ìƒíƒœ ë¦¬í¬íŠ¸ ìƒì„±
    health_report = analyzer.generate_health_report()
    
    # ìƒì„¸ ë¶„ì„
    error_analysis = analyzer.analyze_error_logs(24)
    perf_analysis = analyzer.analyze_performance_logs(24)
    
    # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    report_dir = Path('/app/logs/reports')
    report_dir.mkdir(exist_ok=True)
    
    date_str = datetime.now().strftime('%Y%m%d')
    report_file = report_dir / f'daily_log_report_{date_str}.json'
    
    full_report = {
        'report_date': date_str,
        'health_report': health_report,
        'error_analysis': error_analysis,
        'performance_analysis': perf_analysis
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(full_report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f\"\\n=== ì¼ì¼ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸ ({date_str}) ===\")
    print(f\"ì‹œìŠ¤í…œ ê±´ê°•ë„: {health_report['grade']} ({health_report['health_score']}/100)\")
    print(f\"ì´ ì—ëŸ¬ ìˆ˜: {error_analysis['total_errors']}\")
    print(f\"ì´ ì‘ì—… ìˆ˜: {perf_analysis['total_operations']}\")
    print(f\"ëŠë¦° ì‘ì—… ìˆ˜: {len(perf_analysis['slow_operations'])}\")
    print(f\"\\nê¶Œì¥ì‚¬í•­:\")
    for i, rec in enumerate(health_report['recommendations'], 1):
        print(f\"  {i}. {rec}\")
    print(f\"\\nğŸ“ ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}\")
    
    return full_report
EOF
"
```

---

## ğŸ¯ 1.6.4 ë¡œê·¸ ê´€ë¦¬ ë° í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### ì¢…í•© ë¡œê·¸ ê´€ë¦¬ ë„êµ¬

```bash
docker exec mlops-dev bash -c "
cat > scripts/log_management.py << 'EOF'
#!/usr/bin/env python3
\"\"\"
ì¢…í•© ë¡œê·¸ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ë¡œê·¸ ë¶„ì„, ì •ë¦¬, ëª¨ë‹ˆí„°ë§ì„ í†µí•© ê´€ë¦¬
\"\"\"

import sys
import argparse
from pathlib import Path
from datetime import datetime
import json

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append('/app/src')

from logging_system.analyzers.log_analyzer import LogAnalyzer, generate_daily_log_report

def analyze_logs(args):
    \"\"\"ë¡œê·¸ ë¶„ì„ ì‹¤í–‰\"\"\"
    analyzer = LogAnalyzer()
    
    if args.type == 'errors':
        result = analyzer.analyze_error_logs(args.hours)
        print(f\"\\n=== ì—ëŸ¬ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ {args.hours}ì‹œê°„) ===\")
        print(f\"ì´ ì—ëŸ¬ ìˆ˜: {result['total_errors']}\")
        print(f\"ì—ëŸ¬ ìœ í˜•: {result['error_types']}\")
        
    elif args.type == 'performance':
        result = analyzer.analyze_performance_logs(args.hours)
        print(f\"\\n=== ì„±ëŠ¥ ë¡œê·¸ ë¶„ì„ (ìµœê·¼ {args.hours}ì‹œê°„) ===\")
        print(f\"ì´ ì‘ì—… ìˆ˜: {result['total_operations']}\")
        print(f\"ëŠë¦° ì‘ì—… ìˆ˜: {len(result['slow_operations'])}\")
        
    elif args.type == 'health':
        result = analyzer.generate_health_report()
        print(f\"\\n=== ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ===\")
        print(f\"ê±´ê°•ë„: {result['grade']} ({result['health_score']}/100)\")
        print(f\"ì´ìŠˆ: {result['issues']}\")
        print(\"\\nê¶Œì¥ì‚¬í•­:\")
        for i, rec in enumerate(result['recommendations'], 1):
            print(f\"  {i}. {rec}\")
    
    elif args.type == 'daily':
        result = generate_daily_log_report()
        print(\"ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\")
    
    if args.output:
        with open(args.output, 'w') as f:
            json.dump(result, f, indent=2, default=str)
        print(f\"\\nê²°ê³¼ê°€ {args.output}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")

def show_stats(args):
    \"\"\"ë¡œê·¸ í†µê³„ í‘œì‹œ\"\"\"
    log_dir = Path('/app/logs')
    
    print(\"\\n=== ë¡œê·¸ ë””ë ‰í† ë¦¬ í†µê³„ ===\")
    
    for subdir in ['app', 'error', 'system', 'audit', 'performance']:
        subdir_path = log_dir / subdir
        if subdir_path.exists():
            files = list(subdir_path.glob('*'))
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            
            print(f\"{subdir:12}: {len(files):3d}ê°œ íŒŒì¼, {total_size/1024/1024:8.1f}MB\")

def main():
    parser = argparse.ArgumentParser(description='Movie MLOps ë¡œê·¸ ê´€ë¦¬ ë„êµ¬')
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # ë¡œê·¸ ë¶„ì„
    analyze_parser = subparsers.add_parser('analyze', help='ë¡œê·¸ ë¶„ì„')
    analyze_parser.add_argument('type', choices=['errors', 'performance', 'health', 'daily'],
                               help='ë¶„ì„ ìœ í˜•')
    analyze_parser.add_argument('--hours', type=int, default=24,
                               help='ë¶„ì„ ê¸°ê°„ (ì‹œê°„, ê¸°ë³¸ê°’: 24)')
    analyze_parser.add_argument('--output', help='ê²°ê³¼ ì €ì¥ íŒŒì¼')
    analyze_parser.set_defaults(func=analyze_logs)
    
    # í†µê³„ í‘œì‹œ
    stats_parser = subparsers.add_parser('stats', help='ë¡œê·¸ í†µê³„')
    stats_parser.set_defaults(func=show_stats)
    
    # ì¸ìˆ˜ íŒŒì‹± ë° ì‹¤í–‰
    args = parser.parse_args()
    
    if args.command is None:
        parser.print_help()
        return
    
    try:
        args.func(args)
    except Exception as e:
        print(f\"ì˜¤ë¥˜ ë°œìƒ: {e}\")
        sys.exit(1)

if __name__ == \"__main__\":
    main()
EOF

chmod +x scripts/log_management.py
"
```

### ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

```bash
docker exec mlops-dev bash -c "
cat > scripts/test_logging_system.py << 'EOF'
#!/usr/bin/env python3
\"\"\"
ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
\"\"\"

import sys
import time
import random
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append('/app/src')

from logging_system.log_manager import get_logger, log_performance
from logging_system.decorators import log_execution, LogContext

# í…ŒìŠ¤íŠ¸ ë¡œê±°ë“¤
api_logger = get_logger('test_api', 'test_api.log')
data_logger = get_logger('test_data', 'test_data.log')

@log_execution('test', log_args=True, log_result=True)
def test_function_logging(param1, param2=None):
    \"\"\"í•¨ìˆ˜ ë¡œê¹… í…ŒìŠ¤íŠ¸\"\"\"
    time.sleep(0.1)  # ì‹œë®¬ë ˆì´ì…˜
    return f\"Result: {param1} + {param2}\"

def test_context_logging():
    \"\"\"ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸\"\"\"
    with LogContext('test', 'context_operation') as ctx:
        ctx.add_metadata('test_data', 'sample')
        ctx.log_info(\"Context operation started\")
        
        time.sleep(0.2)
        
        ctx.add_metadata('processed_items', 100)
        ctx.log_info(\"Processing completed\")

def test_error_logging():
    \"\"\"ì—ëŸ¬ ë¡œê¹… í…ŒìŠ¤íŠ¸\"\"\"
    try:
        # ì˜ë„ì  ì—ëŸ¬ ë°œìƒ
        result = 1 / 0
    except Exception as e:
        api_logger.error(f\"Test error occurred: {e}\")
        api_logger.critical(\"This is a critical test error\")

def test_performance_logging():
    \"\"\"ì„±ëŠ¥ ë¡œê¹… í…ŒìŠ¤íŠ¸\"\"\"
    operations = ['data_processing', 'api_call', 'database_query']
    
    for operation in operations:
        # ëœë¤ ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
        duration = random.uniform(0.1, 3.0)
        time.sleep(duration)
        
        metadata = {
            'records_processed': random.randint(100, 1000),
            'memory_used_mb': random.randint(50, 500)
        }
        
        log_performance('test', operation, duration, metadata)

def test_api_simulation():
    \"\"\"API í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸\"\"\"
    scenarios = [
        ('successful_call', False),
        ('timeout_call', False),
        ('failed_call', True),
        ('rate_limited', False)
    ]
    
    for scenario, should_fail in scenarios:
        start_time = time.time()
        
        try:
            if should_fail:
                api_logger.error(f\"API Call {scenario} failed: Connection timeout\")
                raise Exception(f\"Simulated {scenario} failure\")
            else:
                duration = random.uniform(0.5, 2.0)
                time.sleep(duration)
                api_logger.info(f\"API Call {scenario} succeeded in {duration:.3f}s\")
                
        except Exception as e:
            duration = time.time() - start_time
            api_logger.error(f\"API Call {scenario} failed after {duration:.3f}s: {str(e)}\")

def test_data_quality_simulation():
    \"\"\"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹œë®¬ë ˆì´ì…˜\"\"\"
    with LogContext('data_quality', 'batch_validation') as ctx:
        total_records = 1000
        valid_records = random.randint(800, 950)
        
        ctx.add_metadata('total_records', total_records)
        ctx.add_metadata('valid_records', valid_records)
        ctx.add_metadata('validation_rate', valid_records / total_records * 100)
        
        ctx.log_info(f\"Validating {total_records} records\")
        
        # ì‹œë®¬ë ˆì´ì…˜ ì§„í–‰
        for i in range(0, total_records, 100):
            time.sleep(0.05)
            ctx.log_info(f\"Processed {min(i + 100, total_records)}/{total_records} records\")
        
        validation_rate = valid_records / total_records * 100
        if validation_rate < 85:
            data_logger.warning(f\"Low validation rate: {validation_rate:.1f}%\")
        
        ctx.log_info(f\"Validation completed: {validation_rate:.1f}% valid\")

def run_comprehensive_test():
    \"\"\"ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰\"\"\"
    print(\"\\n=== ë¡œê¹… ì‹œìŠ¤í…œ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===\")
    
    print(\"1. í•¨ìˆ˜ ë¡œê¹… í…ŒìŠ¤íŠ¸...\")
    test_function_logging(\"test_param1\", param2=\"test_param2\")
    
    print(\"2. ì»¨í…ìŠ¤íŠ¸ ë¡œê¹… í…ŒìŠ¤íŠ¸...\")
    test_context_logging()
    
    print(\"3. ì—ëŸ¬ ë¡œê¹… í…ŒìŠ¤íŠ¸...\")
    test_error_logging()
    
    print(\"4. ì„±ëŠ¥ ë¡œê¹… í…ŒìŠ¤íŠ¸...\")
    test_performance_logging()
    
    print(\"5. API ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸...\")
    test_api_simulation()
    
    print(\"6. ë°ì´í„° í’ˆì§ˆ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸...\")
    test_data_quality_simulation()
    
    print(\"\\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===\")
    print(\"ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”:\")
    print(\"  - /app/logs/app/test_api.log\")
    print(\"  - /app/logs/app/test_data.log\")
    print(\"  - /app/logs/performance/test_performance.log\")
    print(\"  - /app/logs/error/error.log\")

if __name__ == \"__main__\":
    run_comprehensive_test()
EOF

chmod +x scripts/test_logging_system.py
"
```

---

## ğŸ¯ 1.6.5 ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ ë¡œê¹… í†µí•©

### TMDB API ì»¤ë„¥í„° ë¡œê¹… í†µí•©

```bash
docker exec mlops-dev bash -c "
# ê¸°ì¡´ TMDB API ì»¤ë„¥í„°ì— ë¡œê¹… ì¶”ê°€
if [ -f src/data_processing/tmdb_api_connector.py ]; then
    cp src/data_processing/tmdb_api_connector.py src/data_processing/tmdb_api_connector_backup.py
    
    # ë¡œê¹… import ì¶”ê°€
    sed -i '1i import sys' src/data_processing/tmdb_api_connector.py
    sed -i '2i sys.path.append(\"\/app\/src\")' src/data_processing/tmdb_api_connector.py
    sed -i '3i from logging_system.log_manager import get_logger' src/data_processing/tmdb_api_connector.py
    sed -i '4i from logging_system.decorators import log_api_call, LogContext' src/data_processing/tmdb_api_connector.py
    
    echo \"TMDB API Connectorì— ë¡œê¹…ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.\"
else
    echo \"TMDB API Connector íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"
fi
"
```

### ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸° ë¡œê¹… í†µí•©

```bash
docker exec mlops-dev bash -c "
# ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°ì— ë¡œê¹… ì¶”ê°€
if [ -f src/data_processing/quality_validator.py ]; then
    cp src/data_processing/quality_validator.py src/data_processing/quality_validator_backup.py
    
    # ë¡œê¹… import ì¶”ê°€
    sed -i '1i import sys' src/data_processing/quality_validator.py
    sed -i '2i sys.path.append(\"\/app\/src\")' src/data_processing/quality_validator.py
    sed -i '3i from logging_system.log_manager import get_logger' src/data_processing/quality_validator.py
    sed -i '4i from logging_system.decorators import log_execution, LogContext' src/data_processing/quality_validator.py
    
    echo \"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°ì— ë¡œê¹…ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤.\"
else
    echo \"ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"
fi
"
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### 1.6.1 ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [x] ì¤‘ì•™ ì§‘ì¤‘ì‹ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶• âœ…
- [x] ì»´í¬ë„ŒíŠ¸ë³„ ë¡œê±° ìƒì„± ë° ê´€ë¦¬ âœ…
- [x] ë¡œê·¸ ë ˆë²¨ë³„ ë¶„ë¦¬ ì €ì¥ (DEBUG, INFO, WARNING, ERROR, CRITICAL) âœ…
- [x] ì„±ëŠ¥ ë¡œê·¸ ìë™ ê¸°ë¡ ì‹œìŠ¤í…œ âœ…
- [x] ë¡œê·¸ íšŒì „ ë° ì••ì¶• ê´€ë¦¬ âœ…

### 1.6.2 ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [x] ë¡œê¹… ë°ì½”ë ˆì´í„° ì‹œìŠ¤í…œ êµ¬í˜„ âœ…
- [x] ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ê¸°ë°˜ ë¡œê¹… âœ…
- [x] ë¡œê·¸ ë¶„ì„ ë° íŒ¨í„´ íƒì§€ ë„êµ¬ âœ…
- [x] ìë™ ë¡œê·¸ ì•„ì¹´ì´ë¸Œ ì‹œìŠ¤í…œ âœ…
- [x] ê¸°ì¡´ ì»´í¬ë„ŒíŠ¸ ë¡œê¹… í†µí•© âœ…

### 1.6.3 ìš´ì˜ì  ì™„ë£Œ ê¸°ì¤€
- [x] ì¼ì¼ ë¡œê·¸ ë¦¬í¬íŠ¸ ìë™ ìƒì„± âœ…
- [x] ì‹œìŠ¤í…œ ê±´ê°•ë„ ëª¨ë‹ˆí„°ë§ âœ…
- [x] ë¡œê·¸ ê´€ë¦¬ ëª…ë ¹ì–´ ë„êµ¬ âœ…
- [x] ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ âœ…
- [x] ë¡œê·¸ í†µê³„ ë° ë¶„ì„ ëŒ€ì‹œë³´ë“œ âœ…

---

## ğŸš€ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
docker exec mlops-dev python scripts/test_logging_system.py

# ë¡œê·¸ í†µê³„ í™•ì¸
docker exec mlops-dev python scripts/log_management.py stats

# ì¼ì¼ ë¦¬í¬íŠ¸ ìƒì„±
docker exec mlops-dev python scripts/log_management.py analyze daily

# ì‹œìŠ¤í…œ ê±´ê°•ë„ í™•ì¸
docker exec mlops-dev python scripts/log_management.py analyze health
```

### ë¡œê·¸ íŒŒì¼ í™•ì¸

```bash
# ìƒì„±ëœ ë¡œê·¸ íŒŒì¼ë“¤ í™•ì¸
docker exec mlops-dev bash -c "
echo '=== ë¡œê·¸ ë””ë ‰í† ë¦¬ êµ¬ì¡° ==='
find logs -type f -name '*.log' | head -10

echo -e '\n=== ìµœê·¼ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ==='
tail -5 logs/app/application.log 2>/dev/null || echo 'No application log yet'

echo -e '\n=== ìµœê·¼ ì—ëŸ¬ ë¡œê·¸ ==='
tail -5 logs/error/error.log 2>/dev/null || echo 'No error log yet'

echo -e '\n=== ìµœê·¼ ì„±ëŠ¥ ë¡œê·¸ ==='
tail -5 logs/performance/*_performance.log 2>/dev/null || echo 'No performance log yet'
"
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

### 1.7 Apache Airflowì™€ í†µí•©
- Airflow DAG ë‚´ì—ì„œ ë¡œê¹… ì‹œìŠ¤í…œ í™œìš©
- DAG ì‹¤í–‰ ë¡œê·¸ë¥¼ ì¤‘ì•™ ë¡œê¹… ì‹œìŠ¤í…œìœ¼ë¡œ í†µí•©
- Airflow íƒœìŠ¤í¬ë³„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### 2ë‹¨ê³„ í”¼ì²˜ ìŠ¤í† ì–´ë¡œ ì—°ê³„
- í”¼ì²˜ ìƒì„± ê³¼ì •ì˜ ìƒì„¸ ë¡œê¹…
- í”¼ì²˜ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¡œê¹…
- í”¼ì²˜ ì‚¬ìš© íŒ¨í„´ ì¶”ì  ë° ë¶„ì„

### MLOps ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°€ì‹œì„±
- ëª¨ë“  ë‹¨ê³„ì˜ ë¡œê·¸ í†µí•© ëª¨ë‹ˆí„°ë§
- íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ ë³‘ëª© ì§€ì  ìë™ íƒì§€
- ìš´ì˜ ì¸ì‚¬ì´íŠ¸ ê¸°ë°˜ ìµœì í™” ê¶Œì¥ì‚¬í•­

**ğŸ¯ ëª©í‘œ ë‹¬ì„±**: í¬ê´„ì ì¸ ë¡œê¹… ì‹œìŠ¤í…œì„ í†µí•œ ì™„ì „í•œ ìš´ì˜ ê°€ì‹œì„± í™•ë³´!

**MLOps 1ë‹¨ê³„ ì™„ë£Œ**: ë°ì´í„° ì²˜ë¦¬ë¶€í„° ë¡œê¹…ê¹Œì§€ ì „ì²´ ê¸°ë°˜ ì¸í”„ë¼ êµ¬ì¶• ì™„ë£Œ!

ì´ì œ 1ë‹¨ê³„ì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ ì™„ë¹„ë˜ì–´ ì•ˆì •ì ì´ê³  ê´€ì°° ê°€ëŠ¥í•œ MLOps ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì™„ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
