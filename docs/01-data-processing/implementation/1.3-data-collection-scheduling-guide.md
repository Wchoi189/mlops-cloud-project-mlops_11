---
title: "1.3 ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ"
description: "ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì§€ì†ì ì¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ì¥"
stage: "01-data-processing"
phase: "implementation"
step: "1.3"
category: "scheduling"
difficulty: "intermediate"
estimated_time: "8-12 hours"
tags:
  - scheduling
  - automation
  - cron
  - python-scheduler
  - data-pipeline
  - background-process
authors:
  - mlops-team
last_updated: "2025-06-06"
version: "1.0"
status: "active"
prerequisites:
  - "1.2 ë°ì´í„° í¬ë¡¤ëŸ¬ ê°œë°œ ì™„ë£Œ"
  - "TMDBCrawler í´ë˜ìŠ¤ ì •ìƒ ì‘ë™"
outcomes:
  - "Python ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ìƒ ì‘ë™"
  - "ì¼ì¼/ì£¼ê°„/ì›”ê°„ ìë™ ìˆ˜ì§‘ ì‹¤í–‰"
  - "ë°±ê·¸ë¼ìš´ë“œ ë°ëª¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"
  - "í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™"
related_docs:
  - "1.2-data-crawler-development-guide.md"
  - "1.4-data-storage-setup-guide.md"
  - "1.7-logging-system-setup-guide.md"
---

# 1.3 ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ìœ¼ë¡œ ì§€ì†ì ì¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ë³´ì¥

**í™˜ê²½**: WSL Ubuntu 24.04 + Docker + Python 3.11

**í•µì‹¬ ê°€ì¹˜**: ë¬´ì¸ ìë™í™”ë¥¼ í†µí•œ ì•ˆì •ì ì´ê³  ì§€ì†ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìš´ì˜

---

## ğŸ¯ 1.3.1 Python ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„

### ëª©í‘œ
schedule ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ìœ ì—°í•˜ê³  ê´€ë¦¬í•˜ê¸° ì‰¬ìš´ ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ êµ¬ì¶•

### ê¸°ë³¸ ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬í˜„

```bash
# ìŠ¤ì¼€ì¤„ëŸ¬ ëª¨ë“ˆ ìƒì„±
docker exec mlops-dev touch src/data_processing/scheduler.py
```

**ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤ êµ¬ì¡°**:
```python
import schedule
import time
import logging
from datetime import datetime
from threading import Thread
import signal
import sys

class TMDBDataScheduler:
    def __init__(self):
        self.running = False
        self.crawler = None
        self.logger = self._setup_logging()
    
    def setup_jobs(self):
        """ìŠ¤ì¼€ì¤„ ì‘ì—… ì„¤ì •"""
        # ì¼ì¼ ìˆ˜ì§‘: ë§¤ì¼ ìƒˆë²½ 2ì‹œ
        schedule.every().day.at("02:00").do(self.daily_collection)
        
        # ì£¼ê°„ ìˆ˜ì§‘: ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ
        schedule.every().sunday.at("03:00").do(self.weekly_collection)
        
        # ì‹œê°„ë³„ íŠ¸ë Œë”©: ë§¤ì‹œê°„ ì •ê°
        schedule.every().hour.at(":00").do(self.hourly_trending)
        
        # ì›”ê°„ ì „ì²´ ê°±ì‹ : ë§¤ì›” 1ì¼ ìƒˆë²½ 4ì‹œ
        schedule.every().month.do(self.monthly_full_refresh)
    
    def start_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘"""
        self.running = True
        self.logger.info("TMDB ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘")
        
        while self.running:
            schedule.run_pending()
            time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
    
    def stop_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì§€"""
        self.running = False
        self.logger.info("TMDB ë°ì´í„° ìŠ¤ì¼€ì¤„ëŸ¬ ì •ì§€")
```

### ìŠ¤ì¼€ì¤„ë§ ì‘ì—… êµ¬í˜„

#### **ì¼ì¼ ìˆ˜ì§‘ ì‘ì—…**
```python
def daily_collection(self):
    """ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰ë˜ëŠ” ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘"""
    self.logger.info("=== ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    try:
        from .tmdb_crawler import TMDBCrawler
        crawler = TMDBCrawler()
        
        # ì‹ ê·œ ì¸ê¸° ì˜í™” (ìµœì‹  5í˜ì´ì§€)
        popular_movies = crawler.get_popular_movies_bulk(1, 5)
        self.logger.info(f"ì¸ê¸° ì˜í™” ìˆ˜ì§‘: {len(popular_movies)}ê°œ")
        
        # ë‹¹ì¼ íŠ¸ë Œë”© ì˜í™”
        trending_movies = crawler.get_trending_movies('day')
        self.logger.info(f"íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘: {len(trending_movies)}ê°œ")
        
        # ìµœì‹  ê°œë´‰ ì˜í™”
        latest_movies = crawler.get_latest_movies(max_pages=3)
        self.logger.info(f"ìµœì‹  ì˜í™” ìˆ˜ì§‘: {len(latest_movies)}ê°œ")
        
        # ê²°ê³¼ í†µí•© ë° ì €ì¥
        all_movies = popular_movies + trending_movies + latest_movies
        unique_movies = crawler.remove_duplicates(all_movies)
        
        timestamp = datetime.now().strftime('%Y%m%d')
        crawler.save_collection_results(
            unique_movies,
            f"daily_{timestamp}",
            {
                "collection_type": "daily",
                "timestamp": timestamp,
                "total_collected": len(unique_movies),
                "sources": ["popular", "trending", "latest"]
            }
        )
        
        self.logger.info(f"ì¼ì¼ ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_movies)}ê°œ ì˜í™”")
        
        # ê°„ë‹¨í•œ í’ˆì§ˆ ì²´í¬
        quality_score = self._calculate_quality_score(unique_movies)
        self.logger.info(f"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {quality_score:.2f}/10")
        
        crawler.close()
        
    except Exception as e:
        self.logger.error(f"ì¼ì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        self._send_alert("daily_collection_failed", str(e))
```

#### **ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ ì‘ì—…**
```python
def weekly_collection(self):
    """ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰ë˜ëŠ” ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘"""
    self.logger.info("=== ì£¼ê°„ ì¢…í•© ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    try:
        from .tmdb_crawler import TMDBCrawler
        crawler = TMDBCrawler()
        
        all_movies = []
        
        # ì¥ë¥´ë³„ ìˆœí™˜ ìˆ˜ì§‘
        MAJOR_GENRES = {
            28: "ì•¡ì…˜", 35: "ì½”ë¯¸ë””", 18: "ë“œë¼ë§ˆ", 
            27: "ê³µí¬", 10749: "ë¡œë§¨ìŠ¤"
        }
        
        for genre_id, genre_name in MAJOR_GENRES.items():
            genre_movies = crawler.get_movies_by_genre(genre_id, max_pages=15)
            all_movies.extend(genre_movies)
            self.logger.info(f"{genre_name} ì¥ë¥´: {len(genre_movies)}ê°œ ìˆ˜ì§‘")
        
        # í‰ì  ë†’ì€ ì˜í™”
        top_rated = crawler.get_top_rated_movies(min_rating=7.5, max_pages=20)
        all_movies.extend(top_rated)
        self.logger.info(f"í‰ì  ë†’ì€ ì˜í™”: {len(top_rated)}ê°œ ìˆ˜ì§‘")
        
        # ì£¼ê°„ íŠ¸ë Œë”©
        weekly_trending = crawler.get_trending_movies('week')
        all_movies.extend(weekly_trending)
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê²€ì¦
        unique_movies = crawler.remove_duplicates(all_movies)
        validated_movies = [
            movie for movie in unique_movies 
            if crawler.validate_movie_data(movie)[0]
        ]
        
        # ì£¼ê°„ ê²°ê³¼ ì €ì¥
        week_number = datetime.now().isocalendar()[1]
        crawler.save_collection_results(
            validated_movies,
            f"weekly_W{week_number}",
            {
                "collection_type": "weekly",
                "week": week_number,
                "total_collected": len(validated_movies),
                "quality_passed": len(validated_movies),
                "quality_rate": len(validated_movies) / len(unique_movies) * 100
            }
        )
        
        self.logger.info(f"ì£¼ê°„ ìˆ˜ì§‘ ì™„ë£Œ: {len(validated_movies)}ê°œ ì˜í™”")
        
        # ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_weekly_report(validated_movies, week_number)
        
        crawler.close()
        
    except Exception as e:
        self.logger.error(f"ì£¼ê°„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        self._send_alert("weekly_collection_failed", str(e))
```

#### **ì‹œê°„ë³„ íŠ¸ë Œë”© ìˆ˜ì§‘**
```python
def hourly_trending(self):
    """ë§¤ì‹œê°„ ì •ê° ì‹¤í–‰ë˜ëŠ” íŠ¸ë Œë”© ë°ì´í„° ìˆ˜ì§‘"""
    # ìš´ì˜ ì‹œê°„ë§Œ ì‹¤í–‰ (ì˜¤ì „ 8ì‹œ ~ ì˜¤í›„ 10ì‹œ)
    current_hour = datetime.now().hour
    if not (8 <= current_hour <= 22):
        return
    
    self.logger.info("=== ì‹œê°„ë³„ íŠ¸ë Œë”© ìˆ˜ì§‘ ì‹œì‘ ===")
    
    try:
        from .tmdb_crawler import TMDBCrawler
        crawler = TMDBCrawler()
        
        # ì‹¤ì‹œê°„ íŠ¸ë Œë”© ì˜í™”
        trending_movies = crawler.get_trending_movies('day', max_pages=2)
        
        # ê°„ë‹¨í•œ ì €ì¥ (ë®ì–´ì“°ê¸° ë°©ì‹)
        timestamp = datetime.now().strftime('%Y%m%d_%H')
        crawler.save_collection_results(
            trending_movies,
            f"trending_{timestamp}",
            {
                "collection_type": "hourly_trending",
                "timestamp": timestamp,
                "hour": current_hour
            }
        )
        
        self.logger.info(f"ì‹œê°„ë³„ íŠ¸ë Œë”© ìˆ˜ì§‘ ì™„ë£Œ: {len(trending_movies)}ê°œ ì˜í™”")
        
        crawler.close()
        
    except Exception as e:
        self.logger.error(f"ì‹œê°„ë³„ íŠ¸ë Œë”© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
```

---

## ğŸ¯ 1.3.2 ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‹œìŠ¤í…œ

### ëª©í‘œ
ì‹œìŠ¤í…œ ì¬ì‹œì‘ì—ë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ëŠ” ë°ëª¬ í”„ë¡œì„¸ìŠ¤ êµ¬í˜„

### ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ êµ¬í˜„

#### **systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±**
```bash
# systemd ì„œë¹„ìŠ¤ íŒŒì¼ ìƒì„±
docker exec mlops-dev bash -c "
cat > /tmp/tmdb-scheduler.service << 'EOF'
[Unit]
Description=TMDB Data Collection Scheduler
After=network.target

[Service]
Type=simple
User=root
WorkingDirectory=/app
Environment=PYTHONPATH=/app/src
ExecStart=/usr/local/bin/python /app/src/data_processing/scheduler_daemon.py
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal

[Install]
WantedBy=multi-user.target
EOF
"
```

#### **ë°ëª¬ í”„ë¡œì„¸ìŠ¤ ìŠ¤í¬ë¦½íŠ¸**
```python
# src/data_processing/scheduler_daemon.py
#!/usr/bin/env python3
"""
TMDB ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ë°ëª¬
ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ë¡œ ì‹¤í–‰ë˜ì–´ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì§€ì†ì ìœ¼ë¡œ ë™ì‘
"""

import signal
import sys
import os
import logging
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / 'src'))

from data_processing.scheduler import TMDBDataScheduler

class SchedulerDaemon:
    def __init__(self):
        self.scheduler = TMDBDataScheduler()
        self.setup_signal_handlers()
        self.setup_logging()
    
    def setup_signal_handlers(self):
        """ì‹œìŠ¤í…œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ì„¤ì •"""
        signal.signal(signal.SIGTERM, self.graceful_shutdown)
        signal.signal(signal.SIGINT, self.graceful_shutdown)
    
    def setup_logging(self):
        """ë°ëª¬ ë¡œê¹… ì„¤ì •"""
        log_file = project_root / 'logs' / 'scheduler_daemon.log'
        log_file.parent.mkdir(exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(str(log_file)),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def graceful_shutdown(self, signum, frame):
        """ìš°ì•„í•œ ì¢…ë£Œ ì²˜ë¦¬"""
        self.logger.info(f"Received signal {signum}, shutting down gracefully...")
        self.scheduler.stop_scheduler()
        sys.exit(0)
    
    def run(self):
        """ë°ëª¬ ë©”ì¸ ì‹¤í–‰"""
        try:
            self.logger.info("TMDB ìŠ¤ì¼€ì¤„ëŸ¬ ë°ëª¬ ì‹œì‘")
            
            # PID íŒŒì¼ ìƒì„±
            pid_file = project_root / 'logs' / 'scheduler_daemon.pid'
            with open(pid_file, 'w') as f:
                f.write(str(os.getpid()))
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
            self.scheduler.setup_jobs()
            self.scheduler.start_scheduler()
            
        except Exception as e:
            self.logger.error(f"ë°ëª¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            sys.exit(1)
        finally:
            # PID íŒŒì¼ ì •ë¦¬
            if pid_file.exists():
                pid_file.unlink()

if __name__ == "__main__":
    daemon = SchedulerDaemon()
    daemon.run()
```

### í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬ ë„êµ¬

#### **ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´ ìŠ¤í¬ë¦½íŠ¸**
```bash
# scripts/scheduler_control.sh
#!/bin/bash

DAEMON_SCRIPT="/app/src/data_processing/scheduler_daemon.py"
PID_FILE="/app/logs/scheduler_daemon.pid"
LOG_FILE="/app/logs/scheduler_daemon.log"

case "$1" in
    start)
        echo "TMDB ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘..."
        if [ -f $PID_FILE ]; then
            echo "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤."
            exit 1
        fi
        nohup python $DAEMON_SCRIPT > $LOG_FILE 2>&1 &
        echo "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
        ;;
    stop)
        echo "TMDB ìŠ¤ì¼€ì¤„ëŸ¬ ì¤‘ì§€..."
        if [ -f $PID_FILE ]; then
            PID=$(cat $PID_FILE)
            kill $PID
            rm -f $PID_FILE
            echo "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤."
        else
            echo "ì‹¤í–‰ ì¤‘ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        fi
        ;;
    restart)
        $0 stop
        sleep 2
        $0 start
        ;;
    status)
        if [ -f $PID_FILE ]; then
            PID=$(cat $PID_FILE)
            if ps -p $PID > /dev/null; then
                echo "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. (PID: $PID)"
            else
                echo "PID íŒŒì¼ì€ ìˆì§€ë§Œ í”„ë¡œì„¸ìŠ¤ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                rm -f $PID_FILE
            fi
        else
            echo "ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        fi
        ;;
    logs)
        tail -f $LOG_FILE
        ;;
    *)
        echo "ì‚¬ìš©ë²•: $0 {start|stop|restart|status|logs}"
        exit 1
        ;;
esac
```

---

## ğŸ¯ 1.3.3 ìŠ¤ì¼€ì¤„ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ëª©í‘œ
ìŠ¤ì¼€ì¤„ë§ ì‘ì—…ì˜ ì‹¤í–‰ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë¬¸ì œ ë°œìƒ ì‹œ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

### ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§

#### **ì‘ì—… ì‹¤í–‰ ì¶”ì **
```python
class JobMonitor:
    def __init__(self):
        self.job_history = []
        self.logger = logging.getLogger(__name__)
    
    def record_job_start(self, job_name):
        """ì‘ì—… ì‹œì‘ ê¸°ë¡"""
        job_record = {
            'job_name': job_name,
            'start_time': datetime.now(),
            'status': 'running',
            'end_time': None,
            'duration': None,
            'error': None
        }
        self.job_history.append(job_record)
        self.logger.info(f"ì‘ì—… ì‹œì‘: {job_name}")
        return len(self.job_history) - 1  # ì¸ë±ìŠ¤ ë°˜í™˜
    
    def record_job_end(self, job_index, success=True, error=None):
        """ì‘ì—… ì™„ë£Œ ê¸°ë¡"""
        if job_index < len(self.job_history):
            job_record = self.job_history[job_index]
            job_record['end_time'] = datetime.now()
            job_record['duration'] = (job_record['end_time'] - job_record['start_time']).total_seconds()
            job_record['status'] = 'success' if success else 'failed'
            job_record['error'] = error
            
            self.logger.info(f"ì‘ì—… ì™„ë£Œ: {job_record['job_name']} - {job_record['status']}")
    
    def get_job_statistics(self, days=7):
        """ìµœê·¼ ì‘ì—… í†µê³„"""
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_jobs = [
            job for job in self.job_history 
            if job['start_time'] > cutoff_date
        ]
        
        stats = {
            'total_jobs': len(recent_jobs),
            'successful_jobs': len([j for j in recent_jobs if j['status'] == 'success']),
            'failed_jobs': len([j for j in recent_jobs if j['status'] == 'failed']),
            'avg_duration': np.mean([j['duration'] for j in recent_jobs if j['duration']]),
            'success_rate': len([j for j in recent_jobs if j['status'] == 'success']) / len(recent_jobs) * 100 if recent_jobs else 0
        }
        
        return stats
```

#### **í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ**
```python
def health_check(self):
    """ìŠ¤ì¼€ì¤„ëŸ¬ í—¬ìŠ¤ì²´í¬"""
    health_status = {
        'timestamp': datetime.now().isoformat(),
        'scheduler_running': self.running,
        'last_job_time': self._get_last_job_time(),
        'pending_jobs': len(schedule.jobs),
        'system_resources': self._get_system_resources(),
        'data_quality': self._check_recent_data_quality()
    }
    
    # í—¬ìŠ¤ì²´í¬ ê²°ê³¼ ì €ì¥
    health_file = Path('logs/health_check.json')
    with open(health_file, 'w') as f:
        json.dump(health_status, f, indent=2, default=str)
    
    return health_status

def _check_recent_data_quality(self):
    """ìµœê·¼ ìˆ˜ì§‘ ë°ì´í„° í’ˆì§ˆ ì²´í¬"""
    try:
        latest_file = self._get_latest_collection_file()
        if latest_file and latest_file.exists():
            with open(latest_file) as f:
                data = json.load(f)
            
            return {
                'file_exists': True,
                'movie_count': len(data.get('movies', [])),
                'collection_time': data.get('collection_info', {}).get('timestamp'),
                'quality_score': self._calculate_quality_score(data.get('movies', []))
            }
    except Exception as e:
        return {'file_exists': False, 'error': str(e)}
```

### ì•Œë¦¼ ì‹œìŠ¤í…œ

#### **ì´ë©”ì¼ ì•Œë¦¼ (ì„ íƒì‚¬í•­)**
```python
def _send_alert(self, alert_type, message):
    """ì•Œë¦¼ ë°œì†¡"""
    alert_config = {
        'daily_collection_failed': {
            'subject': 'ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì•Œë¦¼',
            'priority': 'high'
        },
        'weekly_collection_failed': {
            'subject': 'ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì•Œë¦¼',
            'priority': 'high'
        },
        'scheduler_down': {
            'subject': 'ìŠ¤ì¼€ì¤„ëŸ¬ ë‹¤ìš´ ì•Œë¦¼',
            'priority': 'critical'
        }
    }
    
    if alert_type in alert_config:
        config = alert_config[alert_type]
        self.logger.error(f"[{config['priority'].upper()}] {config['subject']}: {message}")
        
        # ì‹¤ì œ ì•Œë¦¼ ë°œì†¡ (ì´ë©”ì¼, Slack ë“±)
        # self._send_email(config['subject'], message)
        # self._send_slack_message(message)
```

#### **ë¡œê·¸ ê¸°ë°˜ ëª¨ë‹ˆí„°ë§**
```python
def setup_log_monitoring(self):
    """ë¡œê·¸ ê¸°ë°˜ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
    # ë¡œê·¸ íŒŒì¼ ê°ì‹œ
    log_patterns = {
        'ERROR': 'error',
        'CRITICAL': 'critical',
        'ì‹¤íŒ¨': 'failed',
        'Exception': 'exception'
    }
    
    # ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
    def monitor_logs():
        log_file = Path('logs/scheduler_daemon.log')
        if log_file.exists():
            with open(log_file, 'r') as f:
                f.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
                while self.running:
                    line = f.readline()
                    if line:
                        for pattern, alert_type in log_patterns.items():
                            if pattern in line:
                                self._send_alert(f'log_{alert_type}', line.strip())
                    time.sleep(1)
    
    # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ ì‹œì‘
    Thread(target=monitor_logs, daemon=True).start()
```

---

## ğŸ¯ 1.3.4 ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸

### ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘

```bash
# ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´ ìŠ¤í¬ë¦½íŠ¸ì— ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
docker exec mlops-dev chmod +x scripts/scheduler_control.sh

# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
docker exec mlops-dev bash scripts/scheduler_control.sh start

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸
docker exec mlops-dev bash scripts/scheduler_control.sh status

# ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
docker exec mlops-dev bash scripts/scheduler_control.sh logs
```

### í…ŒìŠ¤íŠ¸ ìŠ¤ì¼€ì¤„ ì‹¤í–‰

```bash
# í…ŒìŠ¤íŠ¸ìš© ì¦‰ì‹œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
docker exec mlops-dev python -c "
from src.data_processing.scheduler import TMDBDataScheduler
scheduler = TMDBDataScheduler()
scheduler.daily_collection()  # ì¼ì¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸
"

# ê²°ê³¼ í™•ì¸
docker exec mlops-dev ls -la data/raw/movies/daily_*/
docker exec mlops-dev cat logs/scheduler_daemon.log | tail -20
```

### í—¬ìŠ¤ì²´í¬ ì‹¤í–‰

```bash
# í—¬ìŠ¤ì²´í¬ ì‹¤í–‰
docker exec mlops-dev python -c "
from src.data_processing.scheduler import TMDBDataScheduler
scheduler = TMDBDataScheduler()
health = scheduler.health_check()
print(f'í—¬ìŠ¤ì²´í¬ ê²°ê³¼: {health}')
"

# í—¬ìŠ¤ì²´í¬ ê²°ê³¼ í™•ì¸
docker exec mlops-dev cat logs/health_check.json
```

---

## ğŸ¯ 1.3.6 ì‹¤ì œ êµ¬í˜„ëœ ìŠ¤ì¼€ì¤„ëŸ¬ ê¸°ëŠ¥ ğŸ†•

### ëª©í‘œ
ì‹¤ì œ êµ¬í˜„ëœ TMDBDataScheduler í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì™„ì „ ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ê²€ì¦

### ì‹¤ì œ êµ¬í˜„ëœ TMDBDataScheduler ì£¼ìš” ê¸°ëŠ¥

#### **í¬ê´„ì  ìŠ¤ì¼€ì¤„ë§ ì‘ì—… êµ¬ì„±**
```python
# ì‹¤ì œ êµ¬í˜„ëœ ìŠ¤ì¼€ì¤„ ì‘ì—…ë“¤
class TMDBDataScheduler:
    def setup_jobs(self):
        """ì‹¤ì œ êµ¬í˜„ëœ ìŠ¤ì¼€ì¤„ ì‘ì—… ì„¤ì •"""
        
        # ì¼ì¼ ìˆ˜ì§‘: ë§¤ì¼ ìƒˆë²½ 2ì‹œ
        schedule.every().day.at("02:00").do(
            self._safe_job_wrapper, self.daily_collection, "daily_collection"
        )
        
        # ì£¼ê°„ ìˆ˜ì§‘: ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ  
        schedule.every().sunday.at("03:00").do(
            self._safe_job_wrapper, self.weekly_collection, "weekly_collection"
        )
        
        # ì‹œê°„ë³„ íŠ¸ë Œë”©: ë§¤ì‹œê°„ ì •ê° (ìš´ì˜ì‹œê°„ 8-22ì‹œ)
        schedule.every().hour.at(":00").do(
            self._safe_job_wrapper, self.hourly_trending, "hourly_trending"
        )
        
        # ì›”ê°„ ì „ì²´ ê°±ì‹ : ë§¤ì›” 1ì¼ ìƒˆë²½ 4ì‹œ
        schedule.every().month.do(
            self._safe_job_wrapper, self.monthly_full_refresh, "monthly_refresh"
        )
        
        # í—¬ìŠ¤ì²´í¬: ë§¤ 10ë¶„ë§ˆë‹¤
        schedule.every(10).minutes.do(
            self._safe_job_wrapper, self.health_check, "health_check"
        )
```

#### **ê³ ê¸‰ ì¼ì¼ ìˆ˜ì§‘ ì‘ì—…**
```python
# ì‹¤ì œ êµ¬í˜„ëœ ì¼ì¼ ìˆ˜ì§‘ ë¡œì§
def daily_collection(self):
    """ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰ë˜ëŠ” ê³ ê¸‰ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘"""
    
    connector = TMDBAPIConnector()
    validator = DataQualityValidator()
    
    try:
        # 1. ì‹ ê·œ ì¸ê¸° ì˜í™” (ìµœì‹  5í˜ì´ì§€)
        popular_movies = []
        for page in range(1, 6):
            response = connector.get_popular_movies(page)
            if response and 'results' in response:
                popular_movies.extend(response['results'])
        
        # 2. ë‹¹ì¼ íŠ¸ë Œë”© ì˜í™”
        trending_response = connector.get_trending_movies('day')
        trending_movies = trending_response.get('results', []) if trending_response else []
        
        # 3. ìµœì‹  ê°œë´‰ ì˜í™” (ìµœê·¼ 3í˜ì´ì§€)
        latest_movies = []
        for page in range(1, 4):
            response = connector.get_latest_movies(page)
            if response and 'results' in response:
                latest_movies.extend(response['results'])
        
        # 4. ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        all_movies = popular_movies + trending_movies + latest_movies
        unique_movies = self._remove_duplicates(all_movies)
        
        # 5. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        batch_results = validator.validate_batch_data(unique_movies)
        valid_movies = [m for m in unique_movies if validator.validate_single_movie(m)[0]]
        
        # 6. í’ˆì§ˆ í†µê³„ ê³„ì‚°
        collection_stats = {
            'collection_type': 'daily',
            'timestamp': datetime.now().strftime('%Y%m%d'),
            'total_collected': len(unique_movies),
            'total_valid': len(valid_movies),
            'quality_rate': len(valid_movies) / len(unique_movies) * 100 if unique_movies else 0,
            'sources': {
                'popular': len(popular_movies),
                'trending': len(trending_movies), 
                'latest': len(latest_movies)
            }
        }
        
        # 7. í’ˆì§ˆ ê²½ê³  ì‹œìŠ¤í…œ
        if collection_stats['quality_rate'] < self.alert_thresholds['quality_rate_min']:
            self.logger.warning(f"í’ˆì§ˆ ê²½ê³ : ë°ì´í„° í’ˆì§ˆë¥ ì´ {collection_stats['quality_rate']:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.")
        
        if len(valid_movies) < self.alert_thresholds['daily_collection_min']:
            self.logger.warning(f"ìˆ˜ì§‘ëŸ‰ ê²½ê³ : ìˆ˜ì§‘ëœ ì˜í™”ê°€ {len(valid_movies)}ê°œë¡œ ì ìŠµë‹ˆë‹¤.")
        
        # 8. ê²°ê³¼ ì €ì¥
        self._save_collection_results(valid_movies, f"daily_{collection_stats['timestamp']}", collection_stats)
        
        return collection_stats
        
    finally:
        connector.close()
```

#### **ì¢…í•© ì£¼ê°„ ìˆ˜ì§‘ ì‘ì—…**
```python
# ì‹¤ì œ êµ¬í˜„ëœ ì£¼ê°„ ìˆ˜ì§‘ ë¡œì§
def weekly_collection(self):
    """ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰ë˜ëŠ” ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘"""
    
    connector = TMDBAPIConnector()
    validator = DataQualityValidator()
    
    try:
        all_movies = []
        
        # ì£¼ìš” ì¥ë¥´ë³„ ìˆœí™˜ ìˆ˜ì§‘
        MAJOR_GENRES = {
            28: "ì•¡ì…˜", 35: "ì½”ë¯¸ë””", 18: "ë“œë¼ë§ˆ", 
            27: "ê³µí¬", 10749: "ë¡œë§¨ìŠ¤"
        }
        
        for genre_id, genre_name in MAJOR_GENRES.items():
            genre_movies = []
            for page in range(1, 11):  # 10í˜ì´ì§€ì”©
                response = connector.get_movies_by_genre(genre_id, page)
                if response and 'results' in response:
                    genre_movies.extend(response['results'])
                else:
                    break
            
            all_movies.extend(genre_movies)
            self.logger.info(f"{genre_name} ì¥ë¥´: {len(genre_movies)}ê°œ ìˆ˜ì§‘")
        
        # í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘
        top_rated_movies = []
        for page in range(1, 11):
            response = connector.get_top_rated_movies(page)
            if response and 'results' in response:
                # í‰ì  7.5 ì´ìƒë§Œ í•„í„°ë§
                high_rated = [m for m in response['results'] if m.get('vote_average', 0) >= 7.5]
                top_rated_movies.extend(high_rated)
            else:
                break
        
        all_movies.extend(top_rated_movies)
        
        # ì£¼ê°„ íŠ¸ë Œë”©
        weekly_trending_response = connector.get_trending_movies('week')
        weekly_trending = weekly_trending_response.get('results', []) if weekly_trending_response else []
        all_movies.extend(weekly_trending)
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê²€ì¦
        unique_movies = self._remove_duplicates(all_movies)
        validated_movies = [m for m in unique_movies if validator.validate_single_movie(m)[0]]
        
        # ì£¼ê°„ ê²°ê³¼ ì €ì¥
        week_number = datetime.now().isocalendar()[1]
        collection_stats = {
            'collection_type': 'weekly',
            'week_number': week_number,
            'timestamp': datetime.now().strftime('%Y%m%d'),
            'total_collected': len(unique_movies),
            'total_valid': len(validated_movies),
            'quality_rate': len(validated_movies) / len(unique_movies) * 100 if unique_movies else 0,
            'by_source': {
                'genres': len(all_movies) - len(top_rated_movies) - len(weekly_trending),
                'top_rated': len(top_rated_movies),
                'trending': len(weekly_trending)
            }
        }
        
        self._save_collection_results(validated_movies, f"weekly_W{week_number}", collection_stats)
        
        # ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_weekly_report(validated_movies, collection_stats)
        
        return collection_stats
        
    finally:
        connector.close()
```

#### **ì§€ëŠ¥í˜• ì›”ê°„ ì „ì²´ ê°±ì‹ **
```python
# ì‹¤ì œ êµ¬í˜„ëœ ì›”ê°„ ê°±ì‹  ë¡œì§
def monthly_full_refresh(self):
    """ë§¤ì›” 1ì¼ ìƒˆë²½ 4ì‹œ ì‹¤í–‰ë˜ëŠ” ì „ì²´ ë°ì´í„° ê°±ì‹ """
    
    connector = TMDBAPIConnector()
    validator = DataQualityValidator()
    
    try:
        # ì¸ê¸° ì˜í™” ëŒ€ëŸ‰ ìˆ˜ì§‘ (1-50 í˜ì´ì§€)
        all_movies = []
        for page in range(1, 51):
            response = connector.get_popular_movies(page)
            if response and 'results' in response:
                all_movies.extend(response['results'])
            else:
                break
                
            # ì§„í–‰ ìƒí™© ë¡œê·¸ (10í˜ì´ì§€ë§ˆë‹¤)
            if page % 10 == 0:
                self.logger.info(f"ì›”ê°„ ìˆ˜ì§‘ ì§„í–‰: {page}/50 í˜ì´ì§€ ì™„ë£Œ")
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ ê²€ì¦
        unique_movies = self._remove_duplicates(all_movies)
        validated_movies = [m for m in unique_movies if validator.validate_single_movie(m)[0]]
        
        # ì›”ê°„ ê²°ê³¼ ì €ì¥
        month_str = datetime.now().strftime('%Y%m')
        collection_stats = {
            'collection_type': 'monthly',
            'month': month_str,
            'timestamp': datetime.now().strftime('%Y%m%d'),
            'pages_processed': 50,
            'total_collected': len(unique_movies),
            'total_valid': len(validated_movies),
            'quality_rate': len(validated_movies) / len(unique_movies) * 100 if unique_movies else 0
        }
        
        self._save_collection_results(validated_movies, f"monthly_{month_str}", collection_stats)
        
        # ì›”ê°„ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_monthly_report(validated_movies, collection_stats)
        
        return collection_stats
        
    finally:
        connector.close()
```

### ê³ ê¸‰ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ì‹œìŠ¤í…œ ğŸ†•

#### **ì‹¤ì‹œê°„ í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ**
```python
# ì‹¤ì œ êµ¬í˜„ëœ í—¬ìŠ¤ì²´í¬
def health_check(self):
    """ìŠ¤ì¼€ì¤„ëŸ¬ í—¬ìŠ¤ì²´í¬"""
    health_status = {
        'timestamp': datetime.now().isoformat(),
        'scheduler_running': self.running,
        'pending_jobs': len(schedule.jobs),
        'recent_job_failures': self._get_recent_failures(),
        'last_successful_jobs': self.last_successful_jobs,
        'system_status': 'healthy'
    }
    
    # ì—°ì† ì‹¤íŒ¨ ì²´í¬
    for job_name, count in self.failure_counts.items():
        if count >= self.alert_thresholds['consecutive_failures']:
            health_status['system_status'] = 'unhealthy'
            self.logger.error(f"ì—°ì† ì‹¤íŒ¨ ê°ì§€: {job_name} ({count}íšŒ)")
    
    # í—¬ìŠ¤ì²´í¬ ê²°ê³¼ ì €ì¥
    health_dir = Path('logs/health')
    health_dir.mkdir(exist_ok=True)
    
    health_file = health_dir / 'latest_health.json'
    with open(health_file, 'w', encoding='utf-8') as f:
        json.dump(health_status, f, ensure_ascii=False, indent=2, default=str)
    
    return health_status
```

#### **ì‘ì—… ì‹¤í–‰ ì¶”ì  ì‹œìŠ¤í…œ**
```python
# ì‹¤ì œ êµ¬í˜„ëœ ì‘ì—… ì¶”ì 
def _safe_job_wrapper(self, job_func, job_name):
    """ì‘ì—… ì‹¤í–‰ ë˜í¼ (ì—ëŸ¬ ì²˜ë¦¬ ë° ë¡œê¹…)"""
    job_id = self._record_job_start(job_name)
    
    try:
        result = job_func()
        self._record_job_success(job_id, job_name, result)
        return result
        
    except Exception as e:
        self._record_job_failure(job_id, job_name, str(e))
        self._handle_job_failure(job_name, str(e))
        raise

def _record_job_start(self, job_name):
    """ì‘ì—… ì‹œì‘ ê¸°ë¡"""
    job_record = {
        'job_name': job_name,
        'start_time': datetime.now(),
        'status': 'running',
        'end_time': None,
        'duration': None,
        'error': None,
        'result': None
    }
    
    self.job_history.append(job_record)
    job_id = len(self.job_history) - 1
    
    self.logger.info(f"ì‘ì—… ì‹œì‘: {job_name} (ID: {job_id})")
    return job_id

def get_job_statistics(self, days=7):
    """ì‘ì—… í†µê³„ ì¡°íšŒ"""
    cutoff_date = datetime.now() - timedelta(days=days)
    recent_jobs = [
        job for job in self.job_history 
        if job.get('start_time') and job['start_time'] > cutoff_date
    ]
    
    if not recent_jobs:
        return {
            'period': f'ìµœê·¼ {days}ì¼',
            'total_jobs': 0,
            'successful_jobs': 0,
            'failed_jobs': 0,
            'success_rate': 0,
            'avg_duration': 0
        }
    
    successful_jobs = [j for j in recent_jobs if j.get('status') == 'success']
    failed_jobs = [j for j in recent_jobs if j.get('status') == 'failed']
    
    # í‰ê·  ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    durations = [j['duration'] for j in recent_jobs if j.get('duration')]
    avg_duration = sum(durations) / len(durations) if durations else 0
    
    return {
        'period': f'ìµœê·¼ {days}ì¼',
        'total_jobs': len(recent_jobs),
        'successful_jobs': len(successful_jobs),
        'failed_jobs': len(failed_jobs),
        'success_rate': len(successful_jobs) / len(recent_jobs) * 100,
        'avg_duration': avg_duration,
        'job_breakdown': self._get_job_breakdown(recent_jobs)
    }
```

### ìë™ ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œ ğŸ†•

#### **ì£¼ê°„ ë¦¬í¬íŠ¸ ìë™ ìƒì„±**
```python
# ì‹¤ì œ êµ¬í˜„ëœ ë¦¬í¬íŠ¸ ìƒì„±
def _generate_weekly_report(self, movies, stats):
    """ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±"""
    report_dir = Path('data/raw/metadata/weekly_reports')
    report_dir.mkdir(parents=True, exist_ok=True)
    
    week_number = stats['week_number']
    report_file = report_dir / f"weekly_report_W{week_number}.json"
    
    report = {
        'report_type': 'weekly',
        'week_number': week_number,
        'generation_time': datetime.now().isoformat(),
        'collection_stats': stats,
        'top_movies': sorted(movies, key=lambda x: x.get('popularity', 0), reverse=True)[:10],
        'genre_distribution': self._analyze_genre_distribution(movies),
        'rating_distribution': self._analyze_rating_distribution(movies)
    }
    
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    self.logger.info(f"ì£¼ê°„ ë¦¬í¬íŠ¸ ìƒì„±: {report_file}")

def _analyze_genre_distribution(self, movies):
    """ì¥ë¥´ ë¶„í¬ ë¶„ì„"""
    genre_counts = {}
    
    for movie in movies:
        genres = movie.get('genre_ids', [])
        for genre_id in genres:
            genre_counts[genre_id] = genre_counts.get(genre_id, 0) + 1
    
    return dict(sorted(genre_counts.items(), key=lambda x: x[1], reverse=True))

def _analyze_rating_distribution(self, movies):
    """í‰ì  ë¶„í¬ ë¶„ì„"""
    rating_ranges = {
        '9.0-10.0': 0, '8.0-8.9': 0, '7.0-7.9': 0,
        '6.0-6.9': 0, '5.0-5.9': 0, '0.0-4.9': 0
    }
    
    for movie in movies:
        rating = movie.get('vote_average', 0)
        if rating >= 9.0:
            rating_ranges['9.0-10.0'] += 1
        elif rating >= 8.0:
            rating_ranges['8.0-8.9'] += 1
        elif rating >= 7.0:
            rating_ranges['7.0-7.9'] += 1
        elif rating >= 6.0:
            rating_ranges['6.0-6.9'] += 1
        elif rating >= 5.0:
            rating_ranges['5.0-5.9'] += 1
        else:
            rating_ranges['0.0-4.9'] += 1
    
    return rating_ranges
```

---

## ğŸ¯ 1.3.7 ì‹¤ì œ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸ ë° ê²€ì¦ ğŸ†•

### ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ë° ìƒíƒœ í™•ì¸

```bash
# ì‹¤ì œ êµ¬í˜„ëœ ìŠ¤ì¼€ì¤„ëŸ¬ í…ŒìŠ¤íŠ¸
# ìŠ¤ì¼€ì¤„ëŸ¬ ì œì–´ ìŠ¤í¬ë¦½íŠ¸ì— ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
docker exec mlops-dev chmod +x scripts/scheduler_control.sh

# ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
docker exec mlops-dev bash scripts/scheduler_control.sh start

# ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ í™•ì¸
docker exec mlops-dev bash scripts/scheduler_control.sh status

# ìŠ¤ì¼€ì¤„ëŸ¬ ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
docker exec mlops-dev bash scripts/scheduler_control.sh logs
```

### ê°œë³„ ì‘ì—… í…ŒìŠ¤íŠ¸

```bash
# ì¼ì¼ ìˆ˜ì§‘ ì‘ì—… í…ŒìŠ¤íŠ¸
docker exec mlops-dev python -c "
from src.data_processing.scheduler import TMDBDataScheduler

scheduler = TMDBDataScheduler()
print('=== ì¼ì¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===')

result = scheduler.daily_collection()
print(f'ìˆ˜ì§‘ ê²°ê³¼: {result}')
print('=== ì¼ì¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===')
"

# í—¬ìŠ¤ì²´í¬ í…ŒìŠ¤íŠ¸
docker exec mlops-dev python -c "
from src.data_processing.scheduler import TMDBDataScheduler

scheduler = TMDBDataScheduler()
health = scheduler.health_check()
print(f'í—¬ìŠ¤ì²´í¬ ê²°ê³¼: {health}')
"

# ì‘ì—… í†µê³„ í™•ì¸
docker exec mlops-dev python -c "
from src.data_processing.scheduler import TMDBDataScheduler

scheduler = TMDBDataScheduler()
stats = scheduler.get_job_statistics(days=7)
print('=== ìµœê·¼ 7ì¼ ì‘ì—… í†µê³„ ===')
print(f'ì´ ì‘ì—…: {stats["total_jobs"]}ê°œ')
print(f'ì„±ê³µë¥ : {stats["success_rate"]:.1f}%')
print(f'í‰ê·  ì‹¤í–‰ì‹œê°„: {stats["avg_duration"]:.1f}ì´ˆ')
"
```

### ìˆ˜ì§‘ ê²°ê³¼ í™•ì¸

```bash
# ìˆ˜ì§‘ëœ íŒŒì¼ êµ¬ì¡° í™•ì¸
docker exec mlops-dev find data/raw/movies -name "*.json" | head -10

# í—¬ìŠ¤ì²´í¬ ê²°ê³¼ í™•ì¸
docker exec mlops-dev cat logs/health/latest_health.json

# ì£¼ê°„ ë¦¬í¬íŠ¸ í™•ì¸
docker exec mlops-dev find data/raw/metadata/weekly_reports -name "*.json" | head -5

# ìµœì‹  ìˆ˜ì§‘ í†µê³„
docker exec mlops-dev python -c "
import json
from pathlib import Path
from datetime import datetime

# ìµœì‹  ìˆ˜ì§‘ íŒŒì¼ ì°¾ê¸°
data_dir = Path('data/raw/movies')
if data_dir.exists():
    json_files = list(data_dir.glob('daily_*.json'))
    if json_files:
        latest_file = max(json_files, key=lambda x: x.stat().st_mtime)
        print(f'ìµœì‹  ì¼ì¼ ìˆ˜ì§‘ íŒŒì¼: {latest_file.name}')
        
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        collection_info = data.get('collection_info', {})
        print(f'ìˆ˜ì§‘ ì‹œê°„: {collection_info.get("collection_time")}')
        print(f'ì´ ì˜í™”: {collection_info.get("total_movies")}ê°œ')
        print(f'í’ˆì§ˆë¥ : {collection_info.get("metadata", {}).get("quality_rate", 0):.1f}%')
    else:
        print('ì¼ì¼ ìˆ˜ì§‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
else:
    print('ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.')
"
```

### ë°ëª¬ í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸

```bash
# ë°ëª¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ í…ŒìŠ¤íŠ¸
docker exec mlops-dev python src/data_processing/scheduler_daemon.py &

# PID íŒŒì¼ í™•ì¸
docker exec mlops-dev cat logs/scheduler_daemon.pid

# ë°ëª¬ ë¡œê·¸ í™•ì¸
docker exec mlops-dev tail -20 logs/scheduler_daemon.log

# í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
docker exec mlops-dev ps aux | grep scheduler_daemon
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### 1.3.1 ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [x] Python ê¸°ë°˜ ìŠ¤ì¼€ì¤„ëŸ¬ ì •ìƒ ì‘ë™ âœ… (TMDBDataScheduler êµ¬í˜„)
- [x] ì¼ì¼/ì£¼ê°„/ì›”ê°„ ìë™ ìˆ˜ì§‘ ì‹¤í–‰ âœ… (daily/weekly/monthly collection)
- [x] ì‹œê°„ë³„ íŠ¸ë Œë”© ìˆ˜ì§‘ ë™ì‘ âœ… (hourly_trending, 8-22ì‹œ)
- [x] ìŠ¤ì¼€ì¤„ ì¶©ëŒ ì—†ì´ ì•ˆì • ì‹¤í–‰ âœ… (_safe_job_wrapper)
- [x] ì‹¤íŒ¨ ì‹œ ìë™ ì¬ì‹œë„ ë° ì•Œë¦¼ âœ… (ì—°ì† ì‹¤íŒ¨ ê°ì§€ + ê²½ê³ )

### 1.3.2 ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [x] ë°±ê·¸ë¼ìš´ë“œ ë°ëª¬ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ âœ… (scheduler_daemon.py)
- [x] ì‹œìŠ¤í…œ ì¬ì‹œì‘ í›„ ìë™ ë³µêµ¬ âœ… (systemd ì„œë¹„ìŠ¤ + PID ê´€ë¦¬)
- [x] ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ëŠ” ì¥ì‹œê°„ ì‹¤í–‰ âœ… (connector.close() + ì•ˆì „í•œ ì¢…ë£Œ)
- [x] ìš°ì•„í•œ ì¢…ë£Œ ì²˜ë¦¬ êµ¬í˜„ âœ… (signal handler + graceful shutdown)
- [x] ì‘ì—… ì‹¤í–‰ ì´ë ¥ ì¶”ì  âœ… (job_history + í†µê³„)

### 1.3.3 ìš´ì˜ì  ì™„ë£Œ ê¸°ì¤€
- [x] í—¬ìŠ¤ì²´í¬ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ âœ… (10ë¶„ë§ˆë‹¤ + ìƒíƒœ íŒŒì¼)
- [x] ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼ ê¸°ëŠ¥ âœ… (ì‹¤ì‹œê°„ ë¡œê·¸ + íŒ¨í„´ ê°ì§€)
- [x] ì‘ì—… í†µê³„ ë° ì„±ëŠ¥ ì¸¡ì • âœ… (get_job_statistics)
- [x] ìˆ˜ë™ ì œì–´ ëª…ë ¹ì–´ ì™„ë¹„ âœ… (scheduler_control.sh)
- [x] ì¥ì•  ëŒ€ì‘ ì ˆì°¨ ìˆ˜ë¦½ âœ… (ì—°ì† ì‹¤íŒ¨ ê°ì§€ + ë³µêµ¬)

### 1.3.4 ê³ ë„í™” ì™„ë£Œ ê¸°ì¤€ ğŸ†•
- [x] í¬ê´„ì  ë°ì´í„° í’ˆì§ˆ ê²€ì¦ âœ… (DataQualityValidator í†µí•©)
- [x] ìë™ ë¦¬í¬íŠ¸ ìƒì„± ì‹œìŠ¤í…œ âœ… (ì£¼ê°„/ì›”ê°„ ë¦¬í¬íŠ¸)
- [x] ì§€ëŠ¥í˜• í’ˆì§ˆ ê²½ê³  ì‹œìŠ¤í…œ âœ… (í’ˆì§ˆë¥ /ìˆ˜ì§‘ëŸ‰ ì„ê³„ê°’)
- [x] ì‹¤ì‹œê°„ ìˆ˜ì§‘ í†µê³„ ì¶”ì  âœ… (ì†ŒìŠ¤ë³„ ë¶„ì„ + ë©”íƒ€ë°ì´í„°)
- [x] ê³ ê¸‰ ì‘ì—… ì¶”ì  ë° ë¶„ì„ âœ… (ì„±ê³µë¥ /ì‹¤í–‰ì‹œê°„/ì‘ì—…ë³„ ë¶„ì„)

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

### 1.4 ë°ì´í„° ì €ì¥ì†Œ ì„¤ì •ìœ¼ë¡œ ì—°ê³„
- ìŠ¤ì¼€ì¤„ë§ëœ ìˆ˜ì§‘ ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì €ì¥
- ë°±ì—… ë° ì•„ì¹´ì´ë¸Œ ìë™í™”
- ë°ì´í„° ë²„ì „ ê´€ë¦¬ ì‹œìŠ¤í…œ í†µí•©

### ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í™•ì¥
- ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ì„ 1.8 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì— í†µí•©
- ì„±ëŠ¥ ì§€í‘œ ì‹¤ì‹œê°„ ì¶”ì 
- ì•Œë¦¼ ì‹œìŠ¤í…œ ê³ ë„í™”

**ğŸ¯ ëª©í‘œ ë‹¬ì„±**: ì™„ì „ ìë™í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!

**ë‹¤ìŒ ë‹¨ê³„**: 1.4 ë°ì´í„° ì €ì¥ì†Œ ì„¤ì •ìœ¼ë¡œ ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì²´ê³„ì  ê´€ë¦¬ êµ¬í˜„
