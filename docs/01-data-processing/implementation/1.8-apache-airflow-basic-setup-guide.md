---
title: "1.8 Apache Airflow ê¸°ì´ˆ ì„¤ì • - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ"
description: "ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ìœ„í•œ Airflow ê¸°ì´ˆ í™˜ê²½ êµ¬ì¶•"
stage: "01-data-processing"
phase: "implementation"
step: "1.8"
category: "workflow-orchestration"
difficulty: "intermediate"
estimated_time: "8-12 hours"
tags:
  - airflow
  - workflow-orchestration
  - dag
  - automation
  - pipeline-management
  - docker-compose
authors:
  - mlops-team
last_updated: "2025-06-06"
version: "1.0"
status: "active"
prerequisites:
  - "1.1-1.7 ë‹¨ê³„ êµ¬í˜„ ì™„ë£Œ"
  - "Docker Compose ê¸°ë³¸ ì§€ì‹"
  - "Airflow ê°œë… ì´í•´"
outcomes:
  - "Docker ê¸°ë°˜ Airflow ì„¤ì¹˜ ì™„ë£Œ"
  - "ê¸°ë³¸ DAG êµ¬í˜„ ì™„ë£Œ"
  - "TMDB ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš° ì‘ë™"
  - "ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG êµ¬í˜„"
  - "ì›¹ UIë¥¼ í†µí•œ DAG ëª¨ë‹ˆí„°ë§"
related_docs:
  - "1.7-logging-system-setup-guide.md"
  - "../1.data-processing-implementation-guide.md"
  - "../testing/2.comprehensive-testing-guide.md"
---

# 1.8 Apache Airflow ê¸°ì´ˆ ì„¤ì • - WSL Ubuntu 24.04 êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ ìœ„í•œ Airflow ê¸°ì´ˆ í™˜ê²½ êµ¬ì¶•

**í™˜ê²½**: WSL Ubuntu 24.04 + Docker + Python 3.11

**í•µì‹¬ ê°€ì¹˜**: ë³µì¡í•œ ML ì›Œí¬í”Œë¡œìš°ì˜ ìë™í™” ë° ì˜ì¡´ì„± ê´€ë¦¬ë¥¼ í†µí•œ ê²¬ê³ í•œ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

---

## ğŸ¯ 1.7.1 Apache Airflow ì„¤ì¹˜ ë° ì„¤ì •

### ëª©í‘œ
Docker ê¸°ë°˜ Airflow í™˜ê²½ êµ¬ì¶• ë° ê¸°ë³¸ ì„¤ì •

### Docker Compose ê¸°ë°˜ Airflow ì„¤ì¹˜

```bash
# Airflow Docker Compose ì„¤ì • ë””ë ‰í† ë¦¬ ìƒì„±
docker exec mlops-dev bash -c "
mkdir -p airflow/{dags,logs,plugins,config}
cd airflow
"
```

**docker-compose-airflow.yml ìƒì„±**:
```bash
docker exec mlops-dev bash -c "
cat > airflow/docker-compose-airflow.yml << 'EOF'
version: '3.8'

x-airflow-common:
  &airflow-common
  image: apache/airflow:2.7.0-python3.11
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # ë¡œê¹… ì„¤ì •
    AIRFLOW__LOGGING__LOGGING_LEVEL: INFO
    AIRFLOW__LOGGING__FAB_LOGGING_LEVEL: WARN
    AIRFLOW__LOGGING__COLORED_CONSOLE_LOG: 'false'
    AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true'
    # TMDB API ì„¤ì • (í™˜ê²½ë³€ìˆ˜ë¡œ ì „ë‹¬)
    TMDB_API_KEY: \${TMDB_API_KEY}
    TMDB_REGION: \${TMDB_REGION:-KR}
    TMDB_LANGUAGE: \${TMDB_LANGUAGE:-ko-KR}
  volumes:
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins
    - ./config:/opt/airflow/config
    - ../data:/opt/airflow/data
    - ../src:/opt/airflow/src
  user: \"\${AIRFLOW_UID:-50000}:0\"
  depends_on:
    &airflow-common-depends-on
    redis:
      condition: service_healthy
    postgres:
      condition: service_healthy

services:
  postgres:
    image: postgres:13
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: [\"CMD\", \"pg_isready\", \"-U\", \"airflow\"]
      interval: 5s
      retries: 5
    restart: always

  redis:
    image: redis:latest
    expose:
      - 6379
    healthcheck:
      test: [\"CMD\", \"redis-cli\", \"ping\"]
      interval: 5s
      timeout: 30s
      retries: 50
    restart: always

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8080:8080
    healthcheck:
      test: [\"CMD\", \"curl\", \"--fail\", \"http://localhost:8080/health\"]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [\"CMD-SHELL\", 'airflow jobs check --job-type SchedulerJob --hostname \"\$\${HOSTNAME}\"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-worker:
    <<: *airflow-common
    command: celery worker
    healthcheck:
      test:
        - \"CMD-SHELL\"
        - 'celery --app airflow.executors.celery_executor.app inspect ping -d \"celery@\$\${HOSTNAME}\"'
      interval: 10s
      timeout: 10s
      retries: 5
    environment:
      <<: *airflow-common-env
      DUMB_INIT_SETSID: \"0\"
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: [\"CMD-SHELL\", 'airflow jobs check --job-type TriggererJob --hostname \"\$\${HOSTNAME}\"']
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        function ver() {
          printf \"%04d%04d%04d%04d\" \$\${1//./ }
        }
        airflow_version=\$\$(AIRFLOW__LOGGING__LOGGING_LEVEL=INFO && airflow version)
        airflow_version_comparable=\$\$(ver \$\${airflow_version})
        min_airflow_version=2.2.0
        min_airflow_version_comparable=\$\$(ver \$\${min_airflow_version})
        if (( airflow_version_comparable < min_airflow_version_comparable )); then
          echo
          echo -e \"\033[1;31mERROR!!!: Too old Airflow version \$\${airflow_version}!\033[0m\"
          echo \"The minimum Airflow version supported: \$\${min_airflow_version}. Only use this or higher!\"
          echo
          exit 1
        fi
        if [[ -z \"\$\${AIRFLOW_UID}\" ]]; then
          echo
          echo -e \"\033[1;33mWARNING!!!: AIRFLOW_UID not set!\033[0m\"
          echo \"If you are on Linux, you SHOULD follow the instructions below to set \"
          echo \"AIRFLOW_UID environment variable, otherwise files will be owned by root.\"
          echo \"For other operating systems you can get rid of the warning with manually created .env file:\"
          echo \"    See: https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#setting-the-right-airflow-user\"
          echo
        fi
        one_meg=1048576
        mem_available=\$\$((\$\$(getconf _PHYS_PAGES) * \$\$(getconf PAGE_SIZE) / one_meg))
        cpus_available=\$\$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=\$\$(df / | tail -1 | awk '{print \$\$4}')
        warning_resources=\"false\"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e \"\033[1;33mWARNING!!!: Not enough memory available for Docker.\033[0m\"
          echo \"At least 4GB of memory required. You have \$\$((\$\${mem_available} / one_meg))GB available.\"
          echo
          warning_resources=\"true\"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e \"\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\033[0m\"
          echo \"At least 2 CPUs recommended. You have \$\${cpus_available} available.\"
          echo
          warning_resources=\"true\"
        fi
        if (( disk_available < one_meg )); then
          echo
          echo -e \"\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\033[0m\"
          echo \"At least 1 GiB recommended. You have \$\$((\$\${disk_available} / one_meg))GiB available.\"
          echo
          warning_resources=\"true\"
        fi
        if [[ \$\${warning_resources} == \"true\" ]]; then
          echo
          echo -e \"\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\033[0m\"
          echo \"Please follow the instructions to increase amount of resources available:\"
          echo \"   https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html#before-you-begin\"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R \"\$\${AIRFLOW_UID}:0\" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: \${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: \${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: \"0:0\"
    volumes:
      - .:/sources

  airflow-cli:
    <<: *airflow-common
    profiles:
      - debug
    environment:
      <<: *airflow-common-env
      CONNECTION_CHECK_MAX_COUNT: \"0\"
    command:
      - bash
      - -c
      - airflow

volumes:
  postgres-db-volume:
EOF
"
```

### Airflow í™˜ê²½ ì„¤ì •

```bash
# .env íŒŒì¼ì— Airflow ì„¤ì • ì¶”ê°€
docker exec mlops-dev bash -c "
cat >> .env << 'EOF'

# Airflow ì„¤ì •
AIRFLOW_UID=50000
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin123
AIRFLOW__CORE__PARALLELISM=16
AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=16
EOF
"
```

---

## ğŸ¯ 1.7.2 ê¸°ë³¸ DAG êµ¬í˜„

### TMDB ë°ì´í„° ìˆ˜ì§‘ DAG

```bash
# ì²« ë²ˆì§¸ DAG ìƒì„± - ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘
docker exec mlops-dev bash -c "
cat > airflow/dags/tmdb_data_collection_dag.py << 'EOF'
\"\"\"
TMDB ë°ì´í„° ìˆ˜ì§‘ DAG
ë§¤ì¼ ìë™ìœ¼ë¡œ ì˜í™” ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ì›Œí¬í”Œë¡œìš°
\"\"\"

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
import sys
import os

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì •
sys.path.append('/opt/airflow/src')

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}

# DAG ì •ì˜
dag = DAG(
    'tmdb_daily_collection',
    default_args=default_args,
    description='TMDB ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš°',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ
    max_active_runs=1,
    tags=['tmdb', 'data-collection', 'daily'],
)

def collect_popular_movies(**context):
    \"\"\"ì¸ê¸° ì˜í™” ë°ì´í„° ìˆ˜ì§‘\"\"\"
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    # API ì»¤ë„¥í„° ìƒì„±
    connector = TMDBAPIConnector()
    
    try:
        # ì¸ê¸° ì˜í™” ìˆ˜ì§‘ (ìµœì‹  5í˜ì´ì§€)
        all_movies = []
        for page in range(1, 6):
            response = connector.get_popular_movies(page)
            if response and 'results' in response:
                all_movies.extend(response['results'])
        
        # ìˆ˜ì§‘ í†µê³„
        collection_stats = {
            'collection_type': 'daily_popular',
            'collection_date': context['ds'],
            'total_collected': len(all_movies),
            'pages_processed': 5,
            'start_time': context['ts'],
            'dag_run_id': context['dag_run'].run_id
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/daily')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f\"popular_movies_{context['ds_nodash']}.json\"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': all_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f\"âœ… ì¸ê¸° ì˜í™” {len(all_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")
        print(f\"ğŸ“ ì €ì¥ ìœ„ì¹˜: {output_file}\")
        
        # XComì— í†µê³„ ì •ë³´ ì €ì¥ (ë‹¤ìŒ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©)
        return collection_stats
        
    except Exception as e:
        print(f\"âŒ ì¸ê¸° ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")
        raise
    finally:
        connector.close()

def collect_trending_movies(**context):
    \"\"\"íŠ¸ë Œë”© ì˜í™” ë°ì´í„° ìˆ˜ì§‘\"\"\"
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘
        trending_response = connector.get_trending_movies('day')
        trending_movies = trending_response.get('results', []) if trending_response else []
        
        collection_stats = {
            'collection_type': 'daily_trending',
            'collection_date': context['ds'],
            'total_collected': len(trending_movies),
            'time_window': 'day',
            'dag_run_id': context['dag_run'].run_id
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/trending')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f\"trending_movies_{context['ds_nodash']}.json\"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': trending_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f\"âœ… íŠ¸ë Œë”© ì˜í™” {len(trending_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")
        return collection_stats
        
    except Exception as e:
        print(f\"âŒ íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")
        raise
    finally:
        connector.close()

def validate_collected_data(**context):
    \"\"\"ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦\"\"\"
    from data_processing.quality_validator import DataQualityValidator
    import json
    from pathlib import Path
    
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° íŒŒì¼ë“¤ í™•ì¸
    data_files = [
        f\"/opt/airflow/data/raw/movies/daily/popular_movies_{context['ds_nodash']}.json\",
        f\"/opt/airflow/data/raw/movies/trending/trending_movies_{context['ds_nodash']}.json\"
    ]
    
    validator = DataQualityValidator()
    validation_results = {
        'validation_date': context['ds'],
        'files_validated': [],
        'overall_quality_score': 0,
        'total_movies_validated': 0,
        'total_movies_passed': 0
    }
    
    all_movies = []
    
    for file_path in data_files:
        if Path(file_path).exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                movies = data.get('movies', [])
                all_movies.extend(movies)
                
                validation_results['files_validated'].append({
                    'file': file_path,
                    'movie_count': len(movies)
                })
    
    if all_movies:
        # ë°°ì¹˜ ê²€ì¦ ì‹¤í–‰
        batch_results = validator.validate_batch_data(all_movies)
        
        validation_results.update({
            'total_movies_validated': batch_results['total_movies'],
            'total_movies_passed': batch_results['valid_movies'],
            'validation_rate': (batch_results['valid_movies'] / batch_results['total_movies'] * 100) if batch_results['total_movies'] > 0 else 0,
            'quality_distribution': batch_results['quality_distribution'],
            'common_issues': batch_results['common_issues']
        })
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        report_dir = Path('/opt/airflow/data/raw/metadata/quality_reports')
        report_dir.mkdir(parents=True, exist_ok=True)
        
        report_file = report_dir / f\"validation_report_{context['ds_nodash']}.json\"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(validation_results, f, ensure_ascii=False, indent=2, default=str)
        
        print(f\"âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ: {validation_results['validation_rate']:.1f}% í†µê³¼\")
        print(f\"ğŸ“Š ê²€ì¦ ë³´ê³ ì„œ: {report_file}\")
        
        # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼
        if validation_results['validation_rate'] < 80:
            print(f\"âš ï¸ ë°ì´í„° í’ˆì§ˆ ê²½ê³ : ê²€ì¦ í†µê³¼ìœ¨ì´ {validation_results['validation_rate']:.1f}%ë¡œ ë‚®ìŠµë‹ˆë‹¤.\")
        
        return validation_results
    
    else:
        raise ValueError(\"ê²€ì¦í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")

def generate_daily_summary(**context):
    \"\"\"ì¼ì¼ ìˆ˜ì§‘ ìš”ì•½ ìƒì„±\"\"\"
    task_instance = context['task_instance']
    
    # ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ê²°ê³¼ ìˆ˜ì§‘
    popular_stats = task_instance.xcom_pull(task_ids='collect_popular_movies')
    trending_stats = task_instance.xcom_pull(task_ids='collect_trending_movies')
    validation_results = task_instance.xcom_pull(task_ids='validate_collected_data')
    
    # ì¼ì¼ ìš”ì•½ ìƒì„±
    daily_summary = {
        'summary_date': context['ds'],
        'dag_run_id': context['dag_run'].run_id,
        'collection_summary': {
            'popular_movies': popular_stats.get('total_collected', 0) if popular_stats else 0,
            'trending_movies': trending_stats.get('total_collected', 0) if trending_stats else 0,
            'total_collected': 0
        },
        'quality_summary': validation_results if validation_results else {},
        'execution_summary': {
            'start_time': context['dag_run'].start_date.isoformat() if context['dag_run'].start_date else None,
            'end_time': datetime.now().isoformat(),
            'duration_minutes': 0
        }
    }
    
    # ì´ ìˆ˜ì§‘ëŸ‰ ê³„ì‚°
    daily_summary['collection_summary']['total_collected'] = (
        daily_summary['collection_summary']['popular_movies'] +
        daily_summary['collection_summary']['trending_movies']
    )
    
    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    if context['dag_run'].start_date:
        duration = datetime.now() - context['dag_run'].start_date
        daily_summary['execution_summary']['duration_minutes'] = duration.total_seconds() / 60
    
    # ìš”ì•½ ì €ì¥
    summary_dir = Path('/opt/airflow/data/raw/metadata/daily_summaries')
    summary_dir.mkdir(parents=True, exist_ok=True)
    
    summary_file = summary_dir / f\"daily_summary_{context['ds_nodash']}.json\"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(daily_summary, f, ensure_ascii=False, indent=2, default=str)
    
    # ì½˜ì†” ì¶œë ¥
    print(\"\\n\" + \"=\"*50)
    print(\"ğŸ“ˆ TMDB ì¼ì¼ ìˆ˜ì§‘ ìš”ì•½\")
    print(\"=\"*50)
    print(f\"ğŸ“… ìˆ˜ì§‘ ë‚ ì§œ: {context['ds']}\")
    print(f\"ğŸ¬ ì¸ê¸° ì˜í™”: {daily_summary['collection_summary']['popular_movies']}ê°œ\")
    print(f\"ğŸ”¥ íŠ¸ë Œë”© ì˜í™”: {daily_summary['collection_summary']['trending_movies']}ê°œ\")
    print(f\"ğŸ“Š ì´ ìˆ˜ì§‘ëŸ‰: {daily_summary['collection_summary']['total_collected']}ê°œ\")
    
    if validation_results:
        print(f\"âœ… í’ˆì§ˆ ê²€ì¦: {validation_results.get('validation_rate', 0):.1f}% í†µê³¼\")
    
    print(f\"â±ï¸ ì‹¤í–‰ ì‹œê°„: {daily_summary['execution_summary']['duration_minutes']:.1f}ë¶„\")
    print(f\"ğŸ“ ìš”ì•½ ë³´ê³ ì„œ: {summary_file}\")
    print(\"=\"*50)
    
    return daily_summary

# íƒœìŠ¤í¬ ì •ì˜
collect_popular_task = PythonOperator(
    task_id='collect_popular_movies',
    python_callable=collect_popular_movies,
    dag=dag,
    doc_md=\"\"\"
    ## ì¸ê¸° ì˜í™” ìˆ˜ì§‘
    
    TMDB APIì—ì„œ ì¸ê¸° ì˜í™” ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ìµœì‹  5í˜ì´ì§€ (ì•½ 100ê°œ ì˜í™”)
    - JSON í˜•íƒœë¡œ ì €ì¥
    - ìˆ˜ì§‘ ë©”íƒ€ë°ì´í„° í¬í•¨
    \"\"\"
)

collect_trending_task = PythonOperator(
    task_id='collect_trending_movies',
    python_callable=collect_trending_movies,
    dag=dag,
    doc_md=\"\"\"
    ## íŠ¸ë Œë”© ì˜í™” ìˆ˜ì§‘
    
    ë‹¹ì¼ íŠ¸ë Œë”© ì˜í™” ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - ì¼ê°„ íŠ¸ë Œë”© ì˜í™”
    - ì‹¤ì‹œê°„ ì¸ê¸° ë°˜ì˜
    \"\"\"
)

validate_data_task = PythonOperator(
    task_id='validate_collected_data',
    python_callable=validate_collected_data,
    dag=dag,
    doc_md=\"\"\"
    ## ë°ì´í„° í’ˆì§ˆ ê²€ì¦
    
    ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ í’ˆì§ˆì„ ê²€ì¦í•©ë‹ˆë‹¤.
    - í•„ìˆ˜ í•„ë“œ í™•ì¸
    - ë°ì´í„° íƒ€ì… ê²€ì¦
    - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì ìš©
    - í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
    \"\"\"
)

generate_summary_task = PythonOperator(
    task_id='generate_daily_summary',
    python_callable=generate_daily_summary,
    dag=dag,
    doc_md=\"\"\"
    ## ì¼ì¼ ìš”ì•½ ìƒì„±
    
    í•˜ë£¨ ì „ì²´ ìˆ˜ì§‘ ì‘ì—…ì˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ìˆ˜ì§‘ í†µê³„
    - í’ˆì§ˆ ì§€í‘œ
    - ì‹¤í–‰ ì‹œê°„
    - ì¢…í•© ë³´ê³ ì„œ
    \"\"\"
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
[collect_popular_task, collect_trending_task] >> validate_data_task >> generate_summary_task
EOF
"
```

### ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG

```bash
# ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG ìƒì„±
docker exec mlops-dev bash -c "
cat > airflow/dags/tmdb_weekly_collection_dag.py << 'EOF'
\"\"\"
TMDB ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG
ì£¼ê°„ ë‹¨ìœ„ë¡œ ì¥ë¥´ë³„, í‰ì ë³„ ì˜í™” ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ìˆ˜ì§‘
\"\"\"

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.dates import days_ago
import sys

sys.path.append('/opt/airflow/src')

default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
    'catchup': False
}

dag = DAG(
    'tmdb_weekly_comprehensive',
    default_args=default_args,
    description='TMDB ì£¼ê°„ ì¢…í•© ë°ì´í„° ìˆ˜ì§‘',
    schedule_interval='0 3 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 3ì‹œ
    max_active_runs=1,
    tags=['tmdb', 'data-collection', 'weekly', 'comprehensive'],
)

# ì£¼ìš” ì¥ë¥´ ì •ì˜
MAJOR_GENRES = {
    28: \"ì•¡ì…˜\",
    35: \"ì½”ë¯¸ë””\", 
    18: \"ë“œë¼ë§ˆ\",
    27: \"ê³µí¬\",
    10749: \"ë¡œë§¨ìŠ¤\",
    878: \"SF\",
    53: \"ìŠ¤ë¦´ëŸ¬\",
    16: \"ì• ë‹ˆë©”ì´ì…˜\"
}

def collect_genre_movies(genre_id, genre_name, **context):
    \"\"\"ì¥ë¥´ë³„ ì˜í™” ìˆ˜ì§‘\"\"\"
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # ì¥ë¥´ë³„ ì˜í™” ìˆ˜ì§‘ (15í˜ì´ì§€)
        all_movies = []
        for page in range(1, 16):
            response = connector.get_movies_by_genre(genre_id, page)
            if response and 'results' in response:
                all_movies.extend(response['results'])
            else:
                break
        
        # ì¤‘ë³µ ì œê±°
        unique_movies = []
        seen_ids = set()
        for movie in all_movies:
            if movie.get('id') not in seen_ids:
                unique_movies.append(movie)
                seen_ids.add(movie.get('id'))
        
        collection_stats = {
            'collection_type': 'weekly_genre',
            'genre_id': genre_id,
            'genre_name': genre_name,
            'collection_date': context['ds'],
            'total_collected': len(unique_movies),
            'pages_processed': 15,
            'week_number': datetime.strptime(context['ds'], '%Y-%m-%d').isocalendar()[1]
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/genre')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f\"{genre_name.lower()}_{context['ds_nodash']}.json\"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': unique_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f\"âœ… {genre_name} ì¥ë¥´ ì˜í™” {len(unique_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")
        return collection_stats
        
    except Exception as e:
        print(f\"âŒ {genre_name} ì¥ë¥´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")
        raise
    finally:
        connector.close()

def collect_top_rated_movies(**context):
    \"\"\"í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘\"\"\"
    from data_processing.tmdb_api_connector import TMDBAPIConnector
    import json
    from pathlib import Path
    
    connector = TMDBAPIConnector()
    
    try:
        # í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘
        all_movies = []
        for page in range(1, 21):  # 20í˜ì´ì§€
            response = connector.get_top_rated_movies(page)
            if response and 'results' in response:
                # í‰ì  7.5 ì´ìƒë§Œ í•„í„°ë§
                high_rated = [m for m in response['results'] if m.get('vote_average', 0) >= 7.5]
                all_movies.extend(high_rated)
            else:
                break
        
        collection_stats = {
            'collection_type': 'weekly_top_rated',
            'collection_date': context['ds'],
            'total_collected': len(all_movies),
            'min_rating': 7.5,
            'pages_processed': 20
        }
        
        # ë°ì´í„° ì €ì¥
        data_dir = Path('/opt/airflow/data/raw/movies/weekly')
        data_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = data_dir / f\"top_rated_{context['ds_nodash']}.json\"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'movies': all_movies,
                'collection_info': collection_stats
            }, f, ensure_ascii=False, indent=2, default=str)
        
        print(f\"âœ… í‰ì  ë†’ì€ ì˜í™” {len(all_movies)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\")
        return collection_stats
        
    except Exception as e:
        print(f\"âŒ í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘ ì‹¤íŒ¨: {e}\")
        raise
    finally:
        connector.close()

def consolidate_weekly_data(**context):
    \"\"\"ì£¼ê°„ ìˆ˜ì§‘ ë°ì´í„° í†µí•©\"\"\"
    import json
    from pathlib import Path
    
    # ëª¨ë“  ì£¼ê°„ ìˆ˜ì§‘ íŒŒì¼ í†µí•©
    data_sources = [
        ('/opt/airflow/data/raw/movies/genre', 'ì¥ë¥´ë³„'),
        ('/opt/airflow/data/raw/movies/weekly', 'í‰ì ë³„')
    ]
    
    all_movies = []
    collection_summary = {
        'consolidation_date': context['ds'],
        'week_number': datetime.strptime(context['ds'], '%Y-%m-%d').isocalendar()[1],
        'sources_processed': [],
        'total_unique_movies': 0,
        'by_category': {}
    }
    
    seen_ids = set()
    
    for data_dir, category in data_sources:
        data_path = Path(data_dir)
        if data_path.exists():
            category_count = 0
            for file_path in data_path.glob(f\"*{context['ds_nodash']}.json\"):
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    movies = data.get('movies', [])
                    
                    # ì¤‘ë³µ ì œê±°í•˜ë©´ì„œ ì¶”ê°€
                    for movie in movies:
                        if movie.get('id') not in seen_ids:
                            all_movies.append(movie)
                            seen_ids.add(movie.get('id'))
                            category_count += 1
                
                collection_summary['sources_processed'].append(str(file_path))
            
            collection_summary['by_category'][category] = category_count
    
    collection_summary['total_unique_movies'] = len(all_movies)
    
    # í†µí•© ë°ì´í„° ì €ì¥
    consolidated_dir = Path('/opt/airflow/data/processed/weekly')
    consolidated_dir.mkdir(parents=True, exist_ok=True)
    
    week_number = collection_summary['week_number']
    output_file = consolidated_dir / f\"consolidated_week_{week_number}_{context['ds_nodash']}.json\"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            'movies': all_movies,
            'consolidation_info': collection_summary
        }, f, ensure_ascii=False, indent=2, default=str)
    
    print(f\"\\nğŸ“Š ì£¼ê°„ ë°ì´í„° í†µí•© ì™„ë£Œ\")
    print(f\"ğŸ¬ ì´ ê³ ìœ  ì˜í™”: {len(all_movies)}ê°œ\")
    for category, count in collection_summary['by_category'].items():
        print(f\"  {category}: {count}ê°œ\")
    print(f\"ğŸ“ í†µí•© íŒŒì¼: {output_file}\")
    
    return collection_summary

# ì‹œì‘ íƒœìŠ¤í¬
start_task = DummyOperator(
    task_id='start_weekly_collection',
    dag=dag
)

# ì¥ë¥´ë³„ ìˆ˜ì§‘ íƒœìŠ¤í¬ë“¤
genre_tasks = []
for genre_id, genre_name in MAJOR_GENRES.items():
    task = PythonOperator(
        task_id=f'collect_{genre_name.lower()}_movies',
        python_callable=collect_genre_movies,
        op_kwargs={'genre_id': genre_id, 'genre_name': genre_name},
        dag=dag
    )
    genre_tasks.append(task)

# í‰ì  ë†’ì€ ì˜í™” ìˆ˜ì§‘ íƒœìŠ¤í¬
top_rated_task = PythonOperator(
    task_id='collect_top_rated_movies',
    python_callable=collect_top_rated_movies,
    dag=dag
)

# ë°ì´í„° í†µí•© íƒœìŠ¤í¬
consolidate_task = PythonOperator(
    task_id='consolidate_weekly_data',
    python_callable=consolidate_weekly_data,
    dag=dag
)

# ì™„ë£Œ íƒœìŠ¤í¬
end_task = DummyOperator(
    task_id='end_weekly_collection',
    dag=dag
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
start_task >> [*genre_tasks, top_rated_task] >> consolidate_task >> end_task
EOF
"
```

---

## ğŸ¯ 1.7.3 Airflow ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### Airflow í™˜ê²½ ì‹œì‘

```bash
# Airflow ë””ë ‰í† ë¦¬ë¡œ ì´ë™ í›„ ì„œë¹„ìŠ¤ ì‹œì‘
docker exec mlops-dev bash -c "
cd airflow

# í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ë³µì‚¬
cp ../.env .env

# Airflow ì´ˆê¸°í™” ë° ì‹œì‘
echo 'Airflow ì´ˆê¸°í™” ì¤‘...'
docker-compose -f docker-compose-airflow.yml up airflow-init

echo 'Airflow ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...'
docker-compose -f docker-compose-airflow.yml up -d

echo 'Airflow ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...'
docker-compose -f docker-compose-airflow.yml ps
"
```

### DAG ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

```bash
# DAG êµ¬ë¬¸ ê²€ì¦
docker exec mlops-dev bash -c "
cd airflow

# DAG íŒŒì¼ êµ¬ë¬¸ ê²€ì‚¬
echo '=== DAG êµ¬ë¬¸ ê²€ì¦ ==='
docker-compose -f docker-compose-airflow.yml exec airflow-webserver airflow dags check tmdb_daily_collection
docker-compose -f docker-compose-airflow.yml exec airflow-webserver airflow dags check tmdb_weekly_comprehensive

# DAG ëª©ë¡ í™•ì¸
echo '=== ë“±ë¡ëœ DAG ëª©ë¡ ==='
docker-compose -f docker-compose-airflow.yml exec airflow-webserver airflow dags list

# íŠ¹ì • íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
echo '=== íƒœìŠ¤í¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ==='
docker-compose -f docker-compose-airflow.yml exec airflow-webserver airflow tasks test tmdb_daily_collection collect_popular_movies 2024-01-01
"
```

### Airflow ì›¹ UI ì ‘ì†

```bash
# ì›¹ UI ì ‘ì† ì •ë³´ ì¶œë ¥
echo "
=== Airflow ì›¹ UI ì ‘ì† ì •ë³´ ===
URL: http://localhost:8080
ì‚¬ìš©ìëª…: admin
ë¹„ë°€ë²ˆí˜¸: admin123

ë¸Œë¼ìš°ì €ì—ì„œ ìœ„ URLë¡œ ì ‘ì†í•˜ì—¬ DAGë¥¼ í™•ì¸í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"
```

---

## ğŸ¯ 1.7.4 ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹… í†µí•©

### Airflow ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```bash
# Airflow ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
docker exec mlops-dev bash -c "
cat > scripts/airflow_management.py << 'EOF'
#!/usr/bin/env python3
\"\"\"
Airflow ìš´ì˜ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸
\"\"\"

import subprocess
import sys
import json
from pathlib import Path
from datetime import datetime

class AirflowManager:
    \"\"\"Airflow ìš´ì˜ ê´€ë¦¬\"\"\"
    
    def __init__(self):
        self.airflow_dir = Path('/app/airflow')
        self.compose_file = self.airflow_dir / 'docker-compose-airflow.yml'
    
    def start_airflow(self):
        \"\"\"Airflow ì„œë¹„ìŠ¤ ì‹œì‘\"\"\"
        print(\"Airflow ì„œë¹„ìŠ¤ ì‹œì‘ ì¤‘...\")
        
        try:
            # ì„œë¹„ìŠ¤ ì‹œì‘
            result = subprocess.run([
                'docker-compose', '-f', str(self.compose_file), 'up', '-d'
            ], cwd=self.airflow_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(\"âœ… Airflow ì„œë¹„ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.\")
                self._check_services()
            else:
                print(f\"âŒ Airflow ì‹œì‘ ì‹¤íŒ¨: {result.stderr}\")
                return False
                
        except Exception as e:
            print(f\"âŒ Airflow ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}\")
            return False
        
        return True
    
    def stop_airflow(self):
        \"\"\"Airflow ì„œë¹„ìŠ¤ ì¤‘ì§€\"\"\"
        print(\"Airflow ì„œë¹„ìŠ¤ ì¤‘ì§€ ì¤‘...\")
        
        try:
            result = subprocess.run([
                'docker-compose', '-f', str(self.compose_file), 'down'
            ], cwd=self.airflow_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(\"âœ… Airflow ì„œë¹„ìŠ¤ê°€ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\")
            else:
                print(f\"âŒ Airflow ì¤‘ì§€ ì‹¤íŒ¨: {result.stderr}\")
                return False
                
        except Exception as e:
            print(f\"âŒ Airflow ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {e}\")
            return False
        
        return True
    
    def restart_airflow(self):
        \"\"\"Airflow ì„œë¹„ìŠ¤ ì¬ì‹œì‘\"\"\"
        print(\"Airflow ì„œë¹„ìŠ¤ ì¬ì‹œì‘ ì¤‘...\")
        self.stop_airflow()
        return self.start_airflow()
    
    def _check_services(self):
        \"\"\"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸\"\"\"
        try:
            result = subprocess.run([
                'docker-compose', '-f', str(self.compose_file), 'ps'
            ], cwd=self.airflow_dir, capture_output=True, text=True)
            
            print(\"\\n=== Airflow ì„œë¹„ìŠ¤ ìƒíƒœ ===\")
            print(result.stdout)
            
        except Exception as e:
            print(f\"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}\")
    
    def get_dag_status(self, dag_id=None):
        \"\"\"DAG ìƒíƒœ ì¡°íšŒ\"\"\"
        try:
            if dag_id:
                cmd = ['docker-compose', '-f', str(self.compose_file), 'exec', 'airflow-webserver', 
                       'airflow', 'dags', 'state', dag_id]
            else:
                cmd = ['docker-compose', '-f', str(self.compose_file), 'exec', 'airflow-webserver', 
                       'airflow', 'dags', 'list']
            
            result = subprocess.run(cmd, cwd=self.airflow_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(result.stdout)
            else:
                print(f\"DAG ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {result.stderr}\")
                
        except Exception as e:
            print(f\"DAG ìƒíƒœ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}\")
    
    def trigger_dag(self, dag_id, execution_date=None):
        \"\"\"DAG ìˆ˜ë™ íŠ¸ë¦¬ê±°\"\"\"
        print(f\"DAG íŠ¸ë¦¬ê±°: {dag_id}\")
        
        try:
            cmd = ['docker-compose', '-f', str(self.compose_file), 'exec', 'airflow-webserver', 
                   'airflow', 'dags', 'trigger', dag_id]
            
            if execution_date:
                cmd.extend(['-e', execution_date])
            
            result = subprocess.run(cmd, cwd=self.airflow_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                print(f\"âœ… DAG {dag_id} íŠ¸ë¦¬ê±° ì„±ê³µ\")
                print(result.stdout)
            else:
                print(f\"âŒ DAG íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {result.stderr}\")
                
        except Exception as e:
            print(f\"DAG íŠ¸ë¦¬ê±° ì¤‘ ì˜¤ë¥˜: {e}\")
    
    def backup_metadata(self):
        \"\"\"ë©”íƒ€ë°ì´í„° ë°±ì—…\"\"\"
        backup_dir = Path('/app/data/backup/airflow')
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f'airflow_metadata_backup_{timestamp}.sql'
        
        print(f\"Airflow ë©”íƒ€ë°ì´í„° ë°±ì—… ì¤‘: {backup_file}\")
        
        try:
            # PostgreSQL ë°±ì—…
            result = subprocess.run([
                'docker-compose', '-f', str(self.compose_file), 'exec', 'postgres',
                'pg_dump', '-U', 'airflow', '-d', 'airflow'
            ], cwd=self.airflow_dir, capture_output=True, text=True)
            
            if result.returncode == 0:
                with open(backup_file, 'w') as f:
                    f.write(result.stdout)
                print(f\"âœ… ë°±ì—… ì™„ë£Œ: {backup_file}\")
            else:
                print(f\"âŒ ë°±ì—… ì‹¤íŒ¨: {result.stderr}\")
                
        except Exception as e:
            print(f\"ë°±ì—… ì¤‘ ì˜¤ë¥˜: {e}\")

def main():
    if len(sys.argv) < 2:
        print(\"ì‚¬ìš©ë²•: python airflow_management.py <command> [arguments]\")
        print(\"ëª…ë ¹ì–´:\")
        print(\"  start                    - Airflow ì‹œì‘\")
        print(\"  stop                     - Airflow ì¤‘ì§€\")
        print(\"  restart                  - Airflow ì¬ì‹œì‘\")
        print(\"  status [dag_id]          - DAG ìƒíƒœ ì¡°íšŒ\")
        print(\"  trigger <dag_id> [date]  - DAG íŠ¸ë¦¬ê±°\")
        print(\"  backup                   - ë©”íƒ€ë°ì´í„° ë°±ì—…\")
        return
    
    manager = AirflowManager()
    command = sys.argv[1]
    
    if command == 'start':
        manager.start_airflow()
    elif command == 'stop':
        manager.stop_airflow()
    elif command == 'restart':
        manager.restart_airflow()
    elif command == 'status':
        dag_id = sys.argv[2] if len(sys.argv) > 2 else None
        manager.get_dag_status(dag_id)
    elif command == 'trigger':
        if len(sys.argv) < 3:
            print(\"DAG IDê°€ í•„ìš”í•©ë‹ˆë‹¤.\")
            return
        dag_id = sys.argv[2]
        execution_date = sys.argv[3] if len(sys.argv) > 3 else None
        manager.trigger_dag(dag_id, execution_date)
    elif command == 'backup':
        manager.backup_metadata()
    else:
        print(f\"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´: {command}\")

if __name__ == \"__main__\":
    main()
EOF

chmod +x scripts/airflow_management.py
"
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### 1.7.1 ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [x] Docker ê¸°ë°˜ Airflow ì„¤ì¹˜ ì™„ë£Œ âœ…
- [x] ê¸°ë³¸ DAG êµ¬í˜„ ì™„ë£Œ âœ…  
- [x] TMDB ë°ì´í„° ìˆ˜ì§‘ ì›Œí¬í”Œë¡œìš° ì‘ë™ âœ…
- [x] ì£¼ê°„ ì¢…í•© ìˆ˜ì§‘ DAG êµ¬í˜„ âœ…
- [x] DAG ê°„ ì˜ì¡´ì„± ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶• âœ…

### 1.7.2 ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [x] PostgreSQL ë°±ì—”ë“œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ë™ âœ…
- [x] LocalExecutor ê¸°ë°˜ ì‹¤í–‰ í™˜ê²½ âœ…
- [x] XComì„ í†µí•œ íƒœìŠ¤í¬ ê°„ ë°ì´í„° ì „ë‹¬ âœ…
- [x] ë¡œê¹… ì‹œìŠ¤í…œ í†µí•© âœ…
- [x] ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ âœ…

### 1.7.3 ìš´ì˜ì  ì™„ë£Œ ê¸°ì¤€
- [x] ì›¹ UIë¥¼ í†µí•œ DAG ëª¨ë‹ˆí„°ë§ âœ…
- [x] ìŠ¤ì¼€ì¤„ë§ ìë™í™” ì‹œìŠ¤í…œ âœ…
- [x] DAG ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë„êµ¬ âœ…
- [x] ë°±ì—… ë° ë³µêµ¬ ì ˆì°¨ ìˆ˜ë¦½ âœ…
- [x] ìš´ì˜ ê´€ë¦¬ ìŠ¤í¬ë¦½íŠ¸ ì™„ë¹„ âœ…

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

### 2ë‹¨ê³„ í”¼ì²˜ ìŠ¤í† ì–´ë¡œ ì—°ê³„
- Airflow DAGë¥¼ í†µí•œ í”¼ì²˜ íŒŒì´í”„ë¼ì¸ ìë™í™”
- ìˆ˜ì§‘ëœ ì›ì‹œ ë°ì´í„°ë¥¼ í”¼ì²˜ë¡œ ë³€í™˜í•˜ëŠ” ì›Œí¬í”Œë¡œìš°
- í”¼ì²˜ í’ˆì§ˆ ê²€ì¦ ë° ë²„ì „ ê´€ë¦¬ ìë™í™”

### ì „ì²´ MLOps íŒŒì´í”„ë¼ì¸ í†µí•©
- 1ë‹¨ê³„ ì™„ë£Œ í›„ 2-9ë‹¨ê³„ë¡œì˜ ìì—°ìŠ¤ëŸ¬ìš´ í™•ì¥
- ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•œ end-to-end ìë™í™”
- ë³µì¡í•œ ML íŒŒì´í”„ë¼ì¸ì˜ ì˜ì¡´ì„± ê´€ë¦¬

**ğŸ¯ ëª©í‘œ ë‹¬ì„±**: Apache Airflow ê¸°ë°˜ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!

**MLOps 1ë‹¨ê³„ ì™„ì„±**: ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì „ì²´ êµ¬ì¶• ì™„ë£Œ!
