# 8ë‹¨ê³„: ëª¨ë‹ˆí„°ë§ ë° ê´€ì¸¡ì„± ê°•í™” - ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ë‹¨ê³„ ê°œìš”

**ëª©í‘œ**: ì¢…í•©ì ì¸ ì‹œìŠ¤í…œ ë° ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì•ˆì •ì  ìš´ì˜ ë³´ì¥

**í•µì‹¬ ê°€ì¹˜**: ë‹¤ì¸µ ëª¨ë‹ˆí„°ë§ ì²´ê³„ë¡œ ì¸í”„ë¼, ì• í”Œë¦¬ì¼€ì´ì…˜, ëª¨ë¸, ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ëŠ¥ì„ ì‹¤ì‹œê°„ ì¶”ì í•˜ê³  ì´ìƒ ìƒí™© ì¡°ê¸° ê°ì§€

---

## ğŸ¯ 8.1 Prometheus ì„¤ì¹˜ ë° ë©”íŠ¸ë¦­ ìˆ˜ì§‘

### ëª©í‘œ
ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ì˜ í¬ê´„ì ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì‹œìŠ¤í…œ êµ¬ì¶•

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **8.1.1 Prometheus ì„œë²„ ì„¤ì •**
- **ì„¤ì¹˜ ë° ê¸°ë³¸ êµ¬ì„±**
  ```yaml
  # prometheus.yml
  global:
    scrape_interval: 15s
    evaluation_interval: 15s
    
  rule_files:
    - "rules/*.yml"
    
  alerting:
    alertmanagers:
      - static_configs:
          - targets:
            - alertmanager:9093
  
  scrape_configs:
    # MLOps API ì„œë²„ ëª¨ë‹ˆí„°ë§
    - job_name: 'mlops-api'
      static_configs:
        - targets: ['localhost:8000']
      metrics_path: '/metrics'
      scrape_interval: 10s
      
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
    - job_name: 'node-exporter'
      static_configs:
        - targets: ['localhost:9100']
      
    # Redis ëª¨ë‹ˆí„°ë§
    - job_name: 'redis'
      static_configs:
        - targets: ['localhost:9121']
        
    # MLflow ëª¨ë‹ˆí„°ë§
    - job_name: 'mlflow'
      static_configs:
        - targets: ['localhost:5000']
      metrics_path: '/metrics'
      
    # PostgreSQL ëª¨ë‹ˆí„°ë§
    - job_name: 'postgres'
      static_configs:
        - targets: ['localhost:9187']
  ```

- **ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê·œì¹™**
  ```yaml
  # rules/mlops_rules.yml
  groups:
    - name: mlops.rules
      rules:
        # API ì‘ë‹µ ì‹œê°„ í‰ê· 
        - record: mlops:api_response_time_avg
          expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])
          
        # ëª¨ë¸ ì˜ˆì¸¡ ì„±ê³µë¥ 
        - record: mlops:prediction_success_rate
          expr: rate(ml_predictions_total{status="success"}[5m]) / rate(ml_predictions_total[5m])
          
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
        - record: mlops:cpu_usage_percent
          expr: 100 * (1 - avg by (instance)(rate(node_cpu_seconds_total{mode="idle"}[5m])))
          
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        - record: mlops:memory_usage_percent
          expr: 100 * (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes))
  ```

#### **8.1.2 ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° ì„¤ì •**
- **Node Exporter ì„¤ì¹˜**
  ```bash
  # Node Exporter ë‹¤ìš´ë¡œë“œ ë° ì„¤ì¹˜
  wget https://github.com/prometheus/node_exporter/releases/download/v1.8.2/node_exporter-1.8.2.linux-amd64.tar.gz
  tar xvfz node_exporter-1.8.2.linux-amd64.tar.gz
  sudo mv node_exporter-1.8.2.linux-amd64/node_exporter /usr/local/bin/
  
  # ì‹œìŠ¤í…œ ì„œë¹„ìŠ¤ ë“±ë¡
  sudo tee /etc/systemd/system/node_exporter.service > /dev/null <<EOF
  [Unit]
  Description=Node Exporter
  Wants=network-online.target
  After=network-online.target
  
  [Service]
  User=prometheus
  Group=prometheus
  Type=simple
  ExecStart=/usr/local/bin/node_exporter
  
  [Install]
  WantedBy=multi-user.target
  EOF
  
  sudo systemctl daemon-reload
  sudo systemctl enable node_exporter
  sudo systemctl start node_exporter
  ```

- **Redis Exporter ì„¤ì •**
  ```yaml
  # docker-compose.ymlì— Redis Exporter ì¶”ê°€
  redis-exporter:
    image: oliver006/redis_exporter:latest
    environment:
      REDIS_ADDR: "redis:6379"
      REDIS_PASSWORD: "${REDIS_PASSWORD}"
    ports:
      - "9121:9121"
    depends_on:
      - redis
  ```

#### **8.1.3 ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ êµ¬í˜„**
- **FastAPI ë©”íŠ¸ë¦­ ë¯¸ë“¤ì›¨ì–´**
  ```python
  # src/api/middleware/monitoring.py
  from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST
  from fastapi import Request, Response
  from fastapi.responses import Response as FastAPIResponse
  import time
  import psutil
  
  # ë©”íŠ¸ë¦­ ì •ì˜
  REQUEST_COUNT = Counter(
      'http_requests_total',
      'Total number of HTTP requests',
      ['method', 'endpoint', 'status_code']
  )
  
  REQUEST_DURATION = Histogram(
      'http_request_duration_seconds',
      'HTTP request duration in seconds',
      ['method', 'endpoint']
  )
  
  PREDICTION_COUNT = Counter(
      'ml_predictions_total',
      'Total number of ML predictions',
      ['model_version', 'status']
  )
  
  PREDICTION_DURATION = Histogram(
      'ml_prediction_duration_seconds',
      'ML prediction duration in seconds',
      ['model_version']
  )
  
  ACTIVE_CONNECTIONS = Gauge(
      'active_connections',
      'Number of active connections'
  )
  
  MODEL_ACCURACY = Gauge(
      'model_accuracy',
      'Current model accuracy',
      ['model_version']
  )
  
  SYSTEM_CPU_USAGE = Gauge(
      'system_cpu_usage_percent',
      'System CPU usage percentage'
  )
  
  SYSTEM_MEMORY_USAGE = Gauge(
      'system_memory_usage_percent',
      'System memory usage percentage'
  )
  
  class MonitoringMiddleware:
      def __init__(self, app):
          self.app = app
          
      async def __call__(self, scope, receive, send):
          if scope["type"] == "http":
              request = Request(scope, receive)
              start_time = time.time()
              
              # í™œì„± ì—°ê²° ìˆ˜ ì¦ê°€
              ACTIVE_CONNECTIONS.inc()
              
              try:
                  # ì‘ë‹µ ì²˜ë¦¬
                  response = await self.process_request(request, send)
                  
                  # ë©”íŠ¸ë¦­ ê¸°ë¡
                  duration = time.time() - start_time
                  method = request.method
                  endpoint = request.url.path
                  status_code = getattr(response, 'status_code', 0)
                  
                  REQUEST_COUNT.labels(
                      method=method,
                      endpoint=endpoint,
                      status_code=status_code
                  ).inc()
                  
                  REQUEST_DURATION.labels(
                      method=method,
                      endpoint=endpoint
                  ).observe(duration)
                  
              finally:
                  # í™œì„± ì—°ê²° ìˆ˜ ê°ì†Œ
                  ACTIVE_CONNECTIONS.dec()
                  
          else:
              await self.app(scope, receive, send)
      
      async def process_request(self, request: Request, send):
          # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
          self.update_system_metrics()
          
          # ìš”ì²­ ì²˜ë¦¬
          response = await self.app(request.scope, request.receive, send)
          return response
      
      def update_system_metrics(self):
          \"\"\"ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸\"\"\"
          try:
              # CPU ì‚¬ìš©ë¥ 
              cpu_percent = psutil.cpu_percent(interval=None)
              SYSTEM_CPU_USAGE.set(cpu_percent)
              
              # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
              memory = psutil.virtual_memory()
              SYSTEM_MEMORY_USAGE.set(memory.percent)
              
          except Exception as e:
              print(f"Failed to update system metrics: {e}")
  
  # ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸
  async def metrics_endpoint():
      \"\"\"Prometheus ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸\"\"\"
      return FastAPIResponse(
          content=generate_latest(),
          media_type=CONTENT_TYPE_LATEST
      )
  
  # ëª¨ë¸ ì˜ˆì¸¡ ë©”íŠ¸ë¦­ ì¶”ì  ë°ì½”ë ˆì´í„°
  def track_prediction(func):
      \"\"\"ëª¨ë¸ ì˜ˆì¸¡ ë©”íŠ¸ë¦­ ì¶”ì  ë°ì½”ë ˆì´í„°\"\"\"
      def wrapper(*args, **kwargs):
          start_time = time.time()
          model_version = "unknown"
          status = "success"
          
          try:
              result = func(*args, **kwargs)
              
              # ëª¨ë¸ ë²„ì „ ì¶”ì¶œ
              if hasattr(result, 'model_version'):
                  model_version = result.model_version
              
              return result
              
          except Exception as e:
              status = "error"
              raise
          
          finally:
              # ë©”íŠ¸ë¦­ ê¸°ë¡
              duration = time.time() - start_time
              
              PREDICTION_COUNT.labels(
                  model_version=model_version,
                  status=status
              ).inc()
              
              if status == "success":
                  PREDICTION_DURATION.labels(
                      model_version=model_version
                  ).observe(duration)
      
      return wrapper
  ```

---

## ğŸ¯ 8.2 Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### ëª©í‘œ
ì§ê´€ì ì´ê³  ì‹¤ìš©ì ì¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œë¡œ ì¢…í•©ì ì¸ ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë‹ˆí„°ë§

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **8.2.1 Grafana ì„¤ì¹˜ ë° ì„¤ì •**
- **Docker Composeë¡œ ì„¤ì¹˜**
  ```yaml
  # docker-compose.monitoring.yml
  version: '3.8'
  
  services:
    prometheus:
      image: prom/prometheus:latest
      container_name: prometheus
      ports:
        - "9090:9090"
      volumes:
        - ./prometheus.yml:/etc/prometheus/prometheus.yml
        - ./rules:/etc/prometheus/rules
        - prometheus_data:/prometheus
      command:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--storage.tsdb.retention.time=200h'
        - '--web.enable-lifecycle'
        - '--web.enable-admin-api'
    
    grafana:
      image: grafana/grafana:latest
      container_name: grafana
      ports:
        - "3000:3000"
      volumes:
        - grafana_data:/var/lib/grafana
        - ./grafana/provisioning:/etc/grafana/provisioning
        - ./grafana/dashboards:/var/lib/grafana/dashboards
      environment:
        - GF_SECURITY_ADMIN_PASSWORD=admin123
        - GF_USERS_ALLOW_SIGN_UP=false
        - GF_SMTP_ENABLED=true
        - GF_SMTP_HOST=smtp.gmail.com:587
        - GF_SMTP_USER=${SMTP_USER}
        - GF_SMTP_PASSWORD=${SMTP_PASSWORD}
      depends_on:
        - prometheus
    
    alertmanager:
      image: prom/alertmanager:latest
      container_name: alertmanager
      ports:
        - "9093:9093"
      volumes:
        - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
        - alertmanager_data:/alertmanager
  
  volumes:
    prometheus_data:
    grafana_data:
    alertmanager_data:
  ```

- **ë°ì´í„° ì†ŒìŠ¤ ìë™ ì„¤ì •**
  ```yaml
  # grafana/provisioning/datasources/prometheus.yml
  apiVersion: 1
  
  datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://prometheus:9090
      isDefault: true
      editable: true
  ```

#### **8.2.2 MLOps ì¢…í•© ëŒ€ì‹œë³´ë“œ ì„¤ê³„**
- **ëŒ€ì‹œë³´ë“œ JSON ì„¤ì •**
  ```json
  {
    "dashboard": {
      "id": null,
      "title": "MLOps ì¢…í•© ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ",
      "tags": ["mlops", "monitoring"],
      "timezone": "Asia/Seoul",
      "panels": [
        {
          "id": 1,
          "title": "ì‹œìŠ¤í…œ ê°œìš”",
          "type": "stat",
          "gridPos": {"h": 8, "w": 24, "x": 0, "y": 0},
          "targets": [
            {
              "expr": "up{job=\"mlops-api\"}",
              "legendFormat": "API ì„œë²„ ìƒíƒœ"
            },
            {
              "expr": "mlops:prediction_success_rate",
              "legendFormat": "ì˜ˆì¸¡ ì„±ê³µë¥ "
            },
            {
              "expr": "mlops:cpu_usage_percent",
              "legendFormat": "CPU ì‚¬ìš©ë¥ "
            },
            {
              "expr": "mlops:memory_usage_percent",
              "legendFormat": "ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ "
            }
          ]
        },
        {
          "id": 2,
          "title": "API ì„±ëŠ¥ ë©”íŠ¸ë¦­",
          "type": "graph",
          "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
          "targets": [
            {
              "expr": "rate(http_requests_total[5m])",
              "legendFormat": "ìš”ì²­/ì´ˆ - {{method}} {{endpoint}}"
            },
            {
              "expr": "mlops:api_response_time_avg",
              "legendFormat": "í‰ê·  ì‘ë‹µì‹œê°„"
            }
          ]
        },
        {
          "id": 3,
          "title": "ëª¨ë¸ ì˜ˆì¸¡ ì„±ëŠ¥",
          "type": "graph",
          "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
          "targets": [
            {
              "expr": "rate(ml_predictions_total[5m])",
              "legendFormat": "ì˜ˆì¸¡/ì´ˆ - {{model_version}} ({{status}})"
            },
            {
              "expr": "histogram_quantile(0.95, rate(ml_prediction_duration_seconds_bucket[5m]))",
              "legendFormat": "95% ì˜ˆì¸¡ ì§€ì—°ì‹œê°„"
            }
          ]
        }
      ],
      "time": {
        "from": "now-1h",
        "to": "now"
      },
      "refresh": "10s"
    }
  }
  ```

#### **8.2.3 íŠ¹í™”ëœ ëŒ€ì‹œë³´ë“œë“¤**
- **ëª¨ë¸ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ**
  - ì˜ˆì¸¡ ì •í™•ë„ íŠ¸ë Œë“œ
  - ëª¨ë¸ ë²„ì „ë³„ ì„±ëŠ¥ ë¹„êµ
  - ì¶”ë¡  ì§€ì—°ì‹œê°„ ë¶„í¬
  - ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥

- **ì¸í”„ë¼ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**
  - ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ë¥ 
  - ë„¤íŠ¸ì›Œí¬ I/O
  - ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰
  - ì„œë¹„ìŠ¤ ê°€ìš©ì„±

- **ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ**
  - ì¼ì¼ ì‚¬ìš©ì ìˆ˜
  - ì¶”ì²œ í´ë¦­ë¥ 
  - ì„œë¹„ìŠ¤ ë§Œì¡±ë„ ì§€í‘œ
  - ìˆ˜ìµ ê´€ë ¨ ë©”íŠ¸ë¦­

---

## ğŸ¯ 8.3 ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•

### ëª©í‘œ
êµ¬ì¡°í™”ëœ ë¡œê¹…ìœ¼ë¡œ ì• í”Œë¦¬ì¼€ì´ì…˜ ë™ì‘ ì¶”ì  ë° ë””ë²„ê¹… ì§€ì›

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **8.3.1 êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •**
- **Python ë¡œê¹… ì„¤ì •**
  ```python
  # src/core/logging_config.py
  import logging
  import logging.config
  import json
  from datetime import datetime
  import sys
  from pathlib import Path
  
  class JSONFormatter(logging.Formatter):
      \"\"\"JSON í˜•íƒœ ë¡œê·¸ í¬ë§·í„°\"\"\"
      
      def format(self, record):
          log_data = {
              'timestamp': datetime.fromtimestamp(record.created).isoformat(),
              'level': record.levelname,
              'logger': record.name,
              'message': record.getMessage(),
              'module': record.module,
              'function': record.funcName,
              'line': record.lineno
          }
          
          # ì¶”ê°€ í•„ë“œë“¤
          if hasattr(record, 'user_id'):
              log_data['user_id'] = record.user_id
          if hasattr(record, 'request_id'):
              log_data['request_id'] = record.request_id
          if hasattr(record, 'model_version'):
              log_data['model_version'] = record.model_version
          if hasattr(record, 'execution_time'):
              log_data['execution_time'] = record.execution_time
          
          # ì˜ˆì™¸ ì •ë³´
          if record.exc_info:
              log_data['exception'] = self.formatException(record.exc_info)
          
          return json.dumps(log_data, ensure_ascii=False)
  
  def setup_logging(log_level='INFO', log_dir='logs'):
      \"\"\"ë¡œê¹… ì‹œìŠ¤í…œ ì„¤ì •\"\"\"
      
      # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
      Path(log_dir).mkdir(parents=True, exist_ok=True)
      
      logging_config = {
          'version': 1,
          'disable_existing_loggers': False,
          'formatters': {
              'json': {
                  '()': JSONFormatter,
              },
              'standard': {
                  'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
              }
          },
          'handlers': {
              'console': {
                  'level': log_level,
                  'class': 'logging.StreamHandler',
                  'formatter': 'standard',
                  'stream': sys.stdout
              },
              'file_app': {
                  'level': 'INFO',
                  'class': 'logging.handlers.RotatingFileHandler',
                  'formatter': 'json',
                  'filename': f'{log_dir}/app.log',
                  'maxBytes': 10485760,  # 10MB
                  'backupCount': 10
              },
              'file_error': {
                  'level': 'ERROR',
                  'class': 'logging.handlers.RotatingFileHandler',
                  'formatter': 'json',
                  'filename': f'{log_dir}/error.log',
                  'maxBytes': 10485760,  # 10MB
                  'backupCount': 5
              },
              'file_prediction': {
                  'level': 'INFO',
                  'class': 'logging.handlers.RotatingFileHandler',
                  'formatter': 'json',
                  'filename': f'{log_dir}/prediction.log',
                  'maxBytes': 10485760,  # 10MB
                  'backupCount': 20
              }
          },
          'loggers': {
              '': {  # root logger
                  'handlers': ['console', 'file_app'],
                  'level': log_level,
                  'propagate': False
              },
              'mlops.prediction': {
                  'handlers': ['file_prediction'],
                  'level': 'INFO',
                  'propagate': True
              },
              'mlops.error': {
                  'handlers': ['file_error'],
                  'level': 'ERROR',
                  'propagate': True
              }
          }
      }
      
      logging.config.dictConfig(logging_config)
  
  # ì»¨í…ìŠ¤íŠ¸ ë¡œê±° í´ë˜ìŠ¤
  class ContextLogger:
      \"\"\"ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë¡œê±°\"\"\"
      
      def __init__(self, logger_name: str):
          self.logger = logging.getLogger(logger_name)
          self.context = {}
      
      def set_context(self, **kwargs):
          \"\"\"ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •\"\"\"
          self.context.update(kwargs)
      
      def clear_context(self):
          \"\"\"ì»¨í…ìŠ¤íŠ¸ ì´ˆê¸°í™”\"\"\"
          self.context.clear()
      
      def _log_with_context(self, level, msg, *args, **kwargs):
          \"\"\"ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ë¡œê·¸ ê¸°ë¡\"\"\"
          extra = kwargs.pop('extra', {})
          extra.update(self.context)
          kwargs['extra'] = extra
          getattr(self.logger, level)(msg, *args, **kwargs)
      
      def info(self, msg, *args, **kwargs):
          self._log_with_context('info', msg, *args, **kwargs)
      
      def error(self, msg, *args, **kwargs):
          self._log_with_context('error', msg, *args, **kwargs)
      
      def warning(self, msg, *args, **kwargs):
          self._log_with_context('warning', msg, *args, **kwargs)
      
      def debug(self, msg, *args, **kwargs):
          self._log_with_context('debug', msg, *args, **kwargs)
  ```

#### **8.3.2 ë¡œê·¸ ìˆ˜ì§‘ ë° ì¤‘ì•™í™”**
- **ELK ìŠ¤íƒ ì„¤ì •**
  ```yaml
  # docker-compose.logging.yml
  version: '3.8'
  
  services:
    elasticsearch:
      image: docker.elastic.co/elasticsearch/elasticsearch:8.15.0
      container_name: elasticsearch
      environment:
        - node.name=elasticsearch
        - cluster.name=mlops-logs
        - discovery.type=single-node
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
        - xpack.security.enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      volumes:
        - elasticsearch_data:/usr/share/elasticsearch/data
      ports:
        - "9200:9200"
    
    logstash:
      image: docker.elastic.co/logstash/logstash:8.15.0
      container_name: logstash
      volumes:
        - ./logstash/config:/usr/share/logstash/pipeline
      ports:
        - "5044:5044"
        - "5000:5000/tcp"
        - "5000:5000/udp"
      environment:
        LS_JAVA_OPTS: "-Xmx256m -Xms256m"
      depends_on:
        - elasticsearch
    
    kibana:
      image: docker.elastic.co/kibana/kibana:8.15.0
      container_name: kibana
      ports:
        - "5601:5601"
      environment:
        ELASTICSEARCH_URL: http://elasticsearch:9200
        ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
      depends_on:
        - elasticsearch
    
    filebeat:
      image: docker.elastic.co/beats/filebeat:8.15.0
      container_name: filebeat
      user: root
      volumes:
        - ./filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
        - /var/lib/docker/containers:/var/lib/docker/containers:ro
        - /var/run/docker.sock:/var/run/docker.sock:ro
        - ../logs:/var/log/mlops:ro
      depends_on:
        - logstash
  
  volumes:
    elasticsearch_data:
  ```

- **Filebeat ì„¤ì •**
  ```yaml
  # filebeat/filebeat.yml
  filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/mlops/*.log
    json.keys_under_root: true
    json.add_error_key: true
    fields:
      service: mlops-api
      environment: production
    fields_under_root: true
    
  - type: docker
    containers.ids:
      - "*"
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"
  
  output.logstash:
    hosts: ["logstash:5044"]
  
  processors:
    - add_host_metadata:
        when.not.contains.tags: forwarded
  ```

---

## ğŸ¯ 8.4 ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ ì‹œìŠ¤í…œ

### ëª©í‘œ
ëª¨ë¸ ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ ë³€í™”ë¥¼ ì‹¤ì‹œê°„ ê°ì§€í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ì˜ˆë°©

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **8.4.1 Evidently ê¸°ë°˜ ë“œë¦¬í”„íŠ¸ ê°ì§€**
- **ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ ì„¤ì •**
  ```python
  # src/monitoring/drift_detector.py
  import pandas as pd
  import numpy as np
  from evidently import ColumnMapping
  from evidently.report import Report
  from evidently.metric_preset import DataDriftPreset, DataQualityPreset
  from evidently.metrics import DatasetDriftMetric, ColumnDriftMetric
  from datetime import datetime, timedelta
  import logging
  from typing import Dict, List, Optional, Tuple
  
  class DataDriftDetector:
      \"\"\"ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ê¸°\"\"\"
      
      def __init__(self, reference_data: pd.DataFrame, 
                   feature_columns: List[str],
                   target_column: Optional[str] = None,
                   drift_threshold: float = 0.5):
          self.reference_data = reference_data
          self.feature_columns = feature_columns
          self.target_column = target_column
          self.drift_threshold = drift_threshold
          self.logger = logging.getLogger(__name__)
          
          # ì»¬ëŸ¼ ë§¤í•‘ ì„¤ì •
          self.column_mapping = ColumnMapping(
              numerical_features=self._get_numerical_features(),
              categorical_features=self._get_categorical_features(),
              target=target_column
          )
      
      def _get_numerical_features(self) -> List[str]:
          \"\"\"ìˆ˜ì¹˜í˜• í”¼ì²˜ ì¶”ì¶œ\"\"\"
          numerical_features = []
          for col in self.feature_columns:
              if pd.api.types.is_numeric_dtype(self.reference_data[col]):
                  numerical_features.append(col)
          return numerical_features
      
      def _get_categorical_features(self) -> List[str]:
          \"\"\"ë²”ì£¼í˜• í”¼ì²˜ ì¶”ì¶œ\"\"\"
          categorical_features = []
          for col in self.feature_columns:
              if not pd.api.types.is_numeric_dtype(self.reference_data[col]):
                  categorical_features.append(col)
          return categorical_features
      
      def detect_drift(self, current_data: pd.DataFrame) -> Dict:
          \"\"\"ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€\"\"\"
          try:
              # ë°ì´í„° ë“œë¦¬í”„íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±
              report = Report(metrics=[
                  DataDriftPreset(),
                  DataQualityPreset(),
                  DatasetDriftMetric()
              ])
              
              report.run(
                  reference_data=self.reference_data,
                  current_data=current_data,
                  column_mapping=self.column_mapping
              )
              
              # ë¦¬í¬íŠ¸ ê²°ê³¼ ì¶”ì¶œ
              result = report.as_dict()
              
              # ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ë¶„ì„
              drift_info = self._analyze_drift_result(result)
              
              # ì•Œë¦¼ í•„ìš” ì—¬ë¶€ ê²°ì •
              if drift_info['dataset_drift_detected']:
                  self._send_drift_alert(drift_info)
              
              return drift_info
              
          except Exception as e:
              self.logger.error(f"ë“œë¦¬í”„íŠ¸ ê°ì§€ ì‹¤íŒ¨: {e}")
              raise
      
      def _analyze_drift_result(self, result: Dict) -> Dict:
          \"\"\"ë“œë¦¬í”„íŠ¸ ê²°ê³¼ ë¶„ì„\"\"\"
          metrics = result['metrics']
          
          # ë°ì´í„°ì…‹ ë ˆë²¨ ë“œë¦¬í”„íŠ¸
          dataset_drift = None
          for metric in metrics:
              if metric['metric'] == 'DatasetDriftMetric':
                  dataset_drift = metric['result']
                  break
          
          # ì»¬ëŸ¼ë³„ ë“œë¦¬í”„íŠ¸ ì •ë³´
          column_drifts = {}
          for metric in metrics:
              if metric['metric'] == 'ColumnDriftMetric':
                  column_name = metric['result']['column_name']
                  column_drifts[column_name] = {
                      'drift_detected': metric['result']['drift_detected'],
                      'drift_score': metric['result']['drift_score'],
                      'stattest_name': metric['result']['stattest_name']
                  }
          
          return {
              'timestamp': datetime.now().isoformat(),
              'dataset_drift_detected': dataset_drift['drift_detected'] if dataset_drift else False,
              'drift_share': dataset_drift['drift_share'] if dataset_drift else 0,
              'number_of_drifted_columns': dataset_drift['number_of_drifted_columns'] if dataset_drift else 0,
              'column_drifts': column_drifts,
              'data_quality_issues': self._extract_data_quality_issues(result)
          }
      
      def _extract_data_quality_issues(self, result: Dict) -> Dict:
          \"\"\"ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ì¶”ì¶œ\"\"\"
          issues = {
              'missing_values': {},
              'duplicate_rows': 0,
              'data_type_mismatches': []
          }
          
          # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” resultì—ì„œ ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¶”ì¶œ
          return issues
      
      def _send_drift_alert(self, drift_info: Dict):
          \"\"\"ë“œë¦¬í”„íŠ¸ ì•Œë¦¼ ì „ì†¡\"\"\"
          alert_message = f\"\"\"
          ğŸš¨ ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ë¨!
          
          ê°ì§€ ì‹œê°„: {drift_info['timestamp']}
          ë“œë¦¬í”„íŠ¸ ì»¬ëŸ¼ ìˆ˜: {drift_info['number_of_drifted_columns']}
          ë“œë¦¬í”„íŠ¸ ë¹„ìœ¨: {drift_info['drift_share']:.2%}
          
          ì˜í–¥ë°›ì€ ì»¬ëŸ¼ë“¤:
          \"\"\"
          
          for col, info in drift_info['column_drifts'].items():
              if info['drift_detected']:
                  alert_message += f\"- {col}: {info['drift_score']:.3f}\\n\"
          
          self.logger.warning(alert_message)
          
          # ì‹¤ì œ ì•Œë¦¼ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡ (Slack, Email ë“±)
          # self._send_to_slack(alert_message)
          # self._send_email_alert(alert_message)
      
      def generate_drift_report(self, current_data: pd.DataFrame, 
                              output_path: str = "drift_report.html"):
          \"\"\"ë“œë¦¬í”„íŠ¸ ë¦¬í¬íŠ¸ HTML ìƒì„±\"\"\"
          report = Report(metrics=[
              DataDriftPreset(),
              DataQualityPreset()
          ])
          
          report.run(
              reference_data=self.reference_data,
              current_data=current_data,
              column_mapping=self.column_mapping
          )
          
          report.save_html(output_path)
          self.logger.info(f"ë“œë¦¬í”„íŠ¸ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {output_path}")
  
  # ì‹¤ì‹œê°„ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
  class RealTimeDriftMonitor:
      \"\"\"ì‹¤ì‹œê°„ ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§\"\"\"
      
      def __init__(self, detector: DataDriftDetector, 
                   monitoring_window: int = 1000,
                   check_interval: int = 3600):
          self.detector = detector
          self.monitoring_window = monitoring_window
          self.check_interval = check_interval
          self.current_batch = []
          self.last_check_time = datetime.now()
          self.logger = logging.getLogger(__name__)
      
      def add_prediction_data(self, data: Dict):
          \"\"\"ì˜ˆì¸¡ ë°ì´í„° ì¶”ê°€\"\"\"
          self.current_batch.append(data)
          
          # ë°°ì¹˜ í¬ê¸° í™•ì¸
          if len(self.current_batch) >= self.monitoring_window:
              self._check_drift()
          
          # ì‹œê°„ ê¸°ë°˜ í™•ì¸
          elif (datetime.now() - self.last_check_time).seconds >= self.check_interval:
              if len(self.current_batch) > 0:
                  self._check_drift()
      
      def _check_drift(self):
          \"\"\"ë“œë¦¬í”„íŠ¸ í™•ì¸ ì‹¤í–‰\"\"\"
          try:
              # ë°°ì¹˜ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
              batch_df = pd.DataFrame(self.current_batch)
              
              # ë“œë¦¬í”„íŠ¸ ê°ì§€
              drift_result = self.detector.detect_drift(batch_df)
              
              # ê²°ê³¼ ë¡œê¹…
              self.logger.info(f"ë“œë¦¬í”„íŠ¸ ì²´í¬ ì™„ë£Œ: {drift_result['dataset_drift_detected']}")
              
              # ë°°ì¹˜ ì´ˆê¸°í™”
              self.current_batch = []
              self.last_check_time = datetime.now()
              
              return drift_result
              
          except Exception as e:
              self.logger.error(f"ë“œë¦¬í”„íŠ¸ ì²´í¬ ì‹¤íŒ¨: {e}")
              self.current_batch = []  # ì˜¤ë¥˜ ì‹œì—ë„ ë°°ì¹˜ ì´ˆê¸°í™”
  ```

#### **8.4.2 ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§**
- **ì„±ëŠ¥ ì§€í‘œ ì¶”ì **
  ```python
  # src/monitoring/model_performance.py
  import pandas as pd
  import numpy as np
  from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
  from typing import Dict, List, Optional, Tuple
  import logging
  from datetime import datetime, timedelta
  
  class ModelPerformanceMonitor:
      \"\"\"ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§\"\"\"
      
      def __init__(self, model_name: str, baseline_metrics: Dict[str, float]):
          self.model_name = model_name
          self.baseline_metrics = baseline_metrics
          self.performance_history = []
          self.logger = logging.getLogger(__name__)
      
      def log_prediction(self, user_id: int, prediction: List[int], 
                        actual_interaction: Optional[List[int]] = None,
                        feedback_score: Optional[float] = None):
          \"\"\"ì˜ˆì¸¡ ê²°ê³¼ ë¡œê¹…\"\"\"
          prediction_log = {
              'timestamp': datetime.now(),
              'user_id': user_id,
              'prediction': prediction,
              'actual_interaction': actual_interaction,
              'feedback_score': feedback_score,
              'model_name': self.model_name
          }
          
          self.performance_history.append(prediction_log)
          
          # ì„±ëŠ¥ í‰ê°€ (ì‹¤ì œ ìƒí˜¸ì‘ìš©ì´ ìˆëŠ” ê²½ìš°)
          if actual_interaction is not None:
              self._evaluate_prediction(prediction, actual_interaction, feedback_score)
      
      def _evaluate_prediction(self, prediction: List[int], 
                             actual: List[int], 
                             feedback_score: Optional[float]):
          \"\"\"ê°œë³„ ì˜ˆì¸¡ í‰ê°€\"\"\"
          # Hit Rate ê³„ì‚° (ì¶”ì²œ ëª©ë¡ì— ì‹¤ì œ ìƒí˜¸ì‘ìš©í•œ ì•„ì´í…œì´ ìˆëŠ”ì§€)
          hit_rate = len(set(prediction) & set(actual)) / len(prediction) if prediction else 0
          
          # í”¼ë“œë°± ì ìˆ˜ ì²˜ë¦¬
          satisfaction_score = feedback_score if feedback_score is not None else 0
          
          # ë©”íŠ¸ë¦­ ê¸°ë¡
          self.logger.info(
              f"ì˜ˆì¸¡ í‰ê°€ ì™„ë£Œ - Hit Rate: {hit_rate:.3f}, ë§Œì¡±ë„: {satisfaction_score:.2f}",
              extra={
                  'hit_rate': hit_rate,
                  'satisfaction_score': satisfaction_score,
                  'model_name': self.model_name
              }
          )
      
      def calculate_daily_metrics(self, date: datetime = None) -> Dict[str, float]:
          \"\"\"ì¼ì¼ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°\"\"\"
          if date is None:
              date = datetime.now().date()
          
          # í•´ë‹¹ ë‚ ì§œì˜ ë°ì´í„° í•„í„°ë§
          daily_data = [
              log for log in self.performance_history
              if log['timestamp'].date() == date and log['actual_interaction'] is not None
          ]
          
          if not daily_data:
              return {}
          
          # ë©”íŠ¸ë¦­ ê³„ì‚°
          hit_rates = []
          satisfaction_scores = []
          
          for log in daily_data:
              prediction = log['prediction']
              actual = log['actual_interaction']
              
              # Hit Rate
              hit_rate = len(set(prediction) & set(actual)) / len(prediction) if prediction else 0
              hit_rates.append(hit_rate)
              
              # ë§Œì¡±ë„ ì ìˆ˜
              if log['feedback_score'] is not None:
                  satisfaction_scores.append(log['feedback_score'])
          
          metrics = {
              'date': date.isoformat(),
              'total_predictions': len(daily_data),
              'avg_hit_rate': np.mean(hit_rates) if hit_rates else 0,
              'avg_satisfaction': np.mean(satisfaction_scores) if satisfaction_scores else 0,
              'predictions_with_feedback': len(satisfaction_scores)
          }
          
          # ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ ì„±ëŠ¥ ë³€í™”
          if 'hit_rate' in self.baseline_metrics:
              metrics['hit_rate_change'] = metrics['avg_hit_rate'] - self.baseline_metrics['hit_rate']
          
          return metrics
      
      def detect_performance_degradation(self, window_days: int = 7) -> Dict:
          \"\"\"ì„±ëŠ¥ ì €í•˜ ê°ì§€\"\"\"
          end_date = datetime.now().date()
          start_date = end_date - timedelta(days=window_days)
          
          # ê¸°ê°„ë³„ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
          daily_metrics = []
          current_date = start_date
          
          while current_date <= end_date:
              metrics = self.calculate_daily_metrics(current_date)
              if metrics:
                  daily_metrics.append(metrics)
              current_date += timedelta(days=1)
          
          if len(daily_metrics) < 2:
              return {'degradation_detected': False, 'message': 'ì¶©ë¶„í•œ ë°ì´í„° ì—†ìŒ'}
          
          # íŠ¸ë Œë“œ ë¶„ì„
          recent_performance = np.mean([m['avg_hit_rate'] for m in daily_metrics[-3:]])
          baseline_performance = self.baseline_metrics.get('hit_rate', 0)
          
          degradation_threshold = 0.05  # 5% ì„±ëŠ¥ ì €í•˜
          
          performance_drop = baseline_performance - recent_performance
          degradation_detected = performance_drop > degradation_threshold
          
          result = {
              'degradation_detected': degradation_detected,
              'performance_drop': performance_drop,
              'recent_performance': recent_performance,
              'baseline_performance': baseline_performance,
              'window_days': window_days,
              'daily_metrics': daily_metrics
          }
          
          if degradation_detected:
              self._alert_performance_degradation(result)
          
          return result
      
      def _alert_performance_degradation(self, result: Dict):
          \"\"\"ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼\"\"\"
          alert_message = f\"\"\"
          âš ï¸ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ê°ì§€!
          
          ëª¨ë¸: {self.model_name}
          í˜„ì¬ ì„±ëŠ¥: {result['recent_performance']:.3f}
          ê¸°ì¤€ ì„±ëŠ¥: {result['baseline_performance']:.3f}
          ì„±ëŠ¥ ì €í•˜: {result['performance_drop']:.3f}
          \"\"\"
          
          self.logger.warning(alert_message)
  ```

---

## ğŸ¯ 8.5 ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬ì¶•

### ëª©í‘œ
ì‹œìŠ¤í…œ ì´ìƒ ìƒí™© ë°œìƒ ì‹œ ì¦‰ì‹œ ì•Œë¦¼ìœ¼ë¡œ ë¹ ë¥¸ ëŒ€ì‘ ì§€ì›

### ìƒì„¸ êµ¬í˜„ ì‚¬í•­

#### **8.5.1 Alertmanager ì„¤ì •**
- **ì•Œë¦¼ ê·œì¹™ ì •ì˜**
  ```yaml
  # alertmanager.yml
  global:
    smtp_smarthost: 'smtp.gmail.com:587'
    smtp_from: 'mlops-alerts@company.com'
    smtp_auth_username: '${SMTP_USER}'
    smtp_auth_password: '${SMTP_PASSWORD}'
    slack_api_url: '${SLACK_WEBHOOK_URL}'
  
  route:
    group_by: ['alertname']
    group_wait: 10s
    group_interval: 10s
    repeat_interval: 1h
    receiver: 'web.hook'
    routes:
    - match:
        severity: critical
      receiver: 'critical-alerts'
    - match:
        severity: warning
      receiver: 'warning-alerts'
  
  receivers:
  - name: 'web.hook'
    webhook_configs:
    - url: 'http://localhost:5001/'
  
  - name: 'critical-alerts'
    email_configs:
    - to: 'ops-team@company.com'
      subject: '[CRITICAL] MLOps Alert: {{ .GroupLabels.alertname }}'
      body: |
        {{ range .Alerts }}
        Alert: {{ .Annotations.summary }}
        Description: {{ .Annotations.description }}
        {{ end }}
    slack_configs:
    - channel: '#mlops-alerts'
      title: 'CRITICAL MLOps Alert'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      color: 'danger'
  
  - name: 'warning-alerts'
    slack_configs:
    - channel: '#mlops-monitoring'
      title: 'MLOps Warning'
      text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
      color: 'warning'
  ```

- **Prometheus ì•Œë¦¼ ê·œì¹™**
  ```yaml
  # rules/alerts.yml
  groups:
  - name: mlops-alerts
    rules:
    - alert: APIHighErrorRate
      expr: rate(http_requests_total{status_code=~"5.."}[5m]) > 0.1
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "API ì˜¤ë¥˜ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤"
        description: "API ì˜¤ë¥˜ìœ¨ì´ {{ $value }}%ì…ë‹ˆë‹¤"
    
    - alert: ModelPredictionLatency
      expr: histogram_quantile(0.95, rate(ml_prediction_duration_seconds_bucket[5m])) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "ëª¨ë¸ ì˜ˆì¸¡ ì§€ì—°ì‹œê°„ì´ ë†’ìŠµë‹ˆë‹¤"
        description: "95% ì˜ˆì¸¡ ì§€ì—°ì‹œê°„ì´ {{ $value }}ì´ˆì…ë‹ˆë‹¤"
    
    - alert: SystemHighCPU
      expr: mlops:cpu_usage_percent > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "ì‹œìŠ¤í…œ CPU ì‚¬ìš©ë¥ ì´ ë†’ìŠµë‹ˆë‹¤"
        description: "CPU ì‚¬ìš©ë¥ ì´ {{ $value }}%ì…ë‹ˆë‹¤"
    
    - alert: ModelAccuracyDrop
      expr: model_accuracy < 0.8
      for: 10m
      labels:
        severity: critical
      annotations:
        summary: "ëª¨ë¸ ì •í™•ë„ê°€ ë–¨ì–´ì¡ŒìŠµë‹ˆë‹¤"
        description: "ëª¨ë¸ ì •í™•ë„ê°€ {{ $value }}ì…ë‹ˆë‹¤"
    
    - alert: DataDriftDetected
      expr: increase(data_drift_alerts_total[1h]) > 0
      labels:
        severity: warning
      annotations:
        summary: "ë°ì´í„° ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
        description: "ìµœê·¼ 1ì‹œê°„ ë™ì•ˆ ë°ì´í„° ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤"
  ```

#### **8.5.2 Slack í†µí•©**
- **Slack ì•Œë¦¼ ë´‡ ì„¤ì •**
  ```python
  # src/monitoring/alerting.py
  import requests
  import json
  from typing import Dict, List
  import logging
  from datetime import datetime
  
  class SlackAlerter:
      \"\"\"Slack ì•Œë¦¼ ë°œì†¡ê¸°\"\"\"
      
      def __init__(self, webhook_url: str, default_channel: str = "#mlops-alerts"):
          self.webhook_url = webhook_url
          self.default_channel = default_channel
          self.logger = logging.getLogger(__name__)
      
      def send_alert(self, title: str, message: str, 
                    severity: str = "info", 
                    channel: str = None) -> bool:
          \"\"\"Slack ì•Œë¦¼ ì „ì†¡\"\"\"
          
          color_map = {
              "critical": "danger",
              "warning": "warning", 
              "info": "good"
          }
          
          payload = {
              "channel": channel or self.default_channel,
              "username": "MLOps Monitor",
              "icon_emoji": ":robot_face:",
              "attachments": [{
                  "color": color_map.get(severity, "good"),
                  "title": title,
                  "text": message,
                  "timestamp": int(datetime.now().timestamp()),
                  "fields": [
                      {
                          "title": "Severity",
                          "value": severity.upper(),
                          "short": True
                      },
                      {
                          "title": "Time",
                          "value": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                          "short": True
                      }
                  ]
              }]
          }
          
          try:
              response = requests.post(
                  self.webhook_url,
                  data=json.dumps(payload),
                  headers={'Content-Type': 'application/json'},
                  timeout=10
              )
              
              if response.status_code == 200:
                  self.logger.info(f"Slack ì•Œë¦¼ ì „ì†¡ ì„±ê³µ: {title}")
                  return True
              else:
                  self.logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {response.status_code}")
                  return False
                  
          except Exception as e:
              self.logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì¤‘ ì˜¤ë¥˜: {e}")
              return False
      
      def send_drift_alert(self, drift_info: Dict):
          \"\"\"ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì•Œë¦¼\"\"\"
          title = "ğŸš¨ Data Drift Detected"
          message = f\"\"\"
          ë°ì´í„° ë“œë¦¬í”„íŠ¸ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!
          
          â€¢ ë“œë¦¬í”„íŠ¸ ì»¬ëŸ¼ ìˆ˜: {drift_info['number_of_drifted_columns']}
          â€¢ ë“œë¦¬í”„íŠ¸ ë¹„ìœ¨: {drift_info['drift_share']:.2%}
          â€¢ ê°ì§€ ì‹œê°„: {drift_info['timestamp']}
          
          ì¦‰ì‹œ ëª¨ë¸ ì¬í›ˆë ¨ì„ ê²€í† í•´ì£¼ì„¸ìš”.
          \"\"\"
          
          self.send_alert(title, message, severity="warning")
      
      def send_performance_alert(self, performance_info: Dict):
          \"\"\"ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ì•Œë¦¼\"\"\"
          title = "âš ï¸ Model Performance Degradation"
          message = f\"\"\"
          ëª¨ë¸ ì„±ëŠ¥ì´ ì €í•˜ë˜ì—ˆìŠµë‹ˆë‹¤!
          
          â€¢ í˜„ì¬ ì„±ëŠ¥: {performance_info['recent_performance']:.3f}
          â€¢ ê¸°ì¤€ ì„±ëŠ¥: {performance_info['baseline_performance']:.3f}
          â€¢ ì„±ëŠ¥ ì €í•˜: {performance_info['performance_drop']:.3f}
          
          ëª¨ë¸ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.
          \"\"\"
          
          self.send_alert(title, message, severity="critical")
  
  # ì¢…í•© ì•Œë¦¼ ê´€ë¦¬ì
  class AlertManager:
      \"\"\"ì•Œë¦¼ ê´€ë¦¬ì\"\"\"
      
      def __init__(self, slack_alerter: SlackAlerter):
          self.slack_alerter = slack_alerter
          self.alert_history = []
          self.logger = logging.getLogger(__name__)
      
      def send_system_alert(self, alert_type: str, details: Dict):
          \"\"\"ì‹œìŠ¤í…œ ì•Œë¦¼ ì „ì†¡\"\"\"
          
          alert_templates = {
              "high_cpu": {
                  "title": "ğŸ”¥ High CPU Usage",
                  "message": f"CPU ì‚¬ìš©ë¥ ì´ {details.get('cpu_percent', 0):.1f}%ì…ë‹ˆë‹¤",
                  "severity": "warning"
              },
              "high_memory": {
                  "title": "ğŸ’¾ High Memory Usage", 
                  "message": f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ ì´ {details.get('memory_percent', 0):.1f}%ì…ë‹ˆë‹¤",
                  "severity": "warning"
              },
              "api_error": {
                  "title": "ğŸš¨ API Error Rate High",
                  "message": f"API ì˜¤ë¥˜ìœ¨ì´ {details.get('error_rate', 0):.2%}ì…ë‹ˆë‹¤",
                  "severity": "critical"
              },
              "service_down": {
                  "title": "ğŸ’¥ Service Down",
                  "message": f"ì„œë¹„ìŠ¤ {details.get('service_name', 'Unknown')}ê°€ ë‹¤ìš´ë˜ì—ˆìŠµë‹ˆë‹¤",
                  "severity": "critical"
              }
          }
          
          template = alert_templates.get(alert_type, {
              "title": f"âš ï¸ {alert_type}",
              "message": str(details),
              "severity": "info"
          })
          
          success = self.slack_alerter.send_alert(
              title=template["title"],
              message=template["message"],
              severity=template["severity"]
          )
          
          # ì•Œë¦¼ íˆìŠ¤í† ë¦¬ ê¸°ë¡
          self.alert_history.append({
              "timestamp": datetime.now(),
              "type": alert_type,
              "details": details,
              "sent": success
          })
          
          return success
  ```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### 8.5.1 ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ì‹œìŠ¤í…œ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥ (CPU, ë©”ëª¨ë¦¬, ë„¤íŠ¸ì›Œí¬)
- [ ] ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ì´ ìë™ìœ¼ë¡œ ìˆ˜ì§‘ë˜ê³  ì‹œê°í™”ë¨
- [ ] ë°ì´í„° ë“œë¦¬í”„íŠ¸ ê°ì§€ ì‹œìŠ¤í…œì´ ì‘ë™í•˜ì—¬ ì´ìƒ ìƒí™© ê°ì§€
- [ ] ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ì¦‰ì‹œ ì•Œë¦¼ ìˆ˜ì‹  (Slack, Email)
- [ ] ì¢…í•© ëŒ€ì‹œë³´ë“œì—ì„œ ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ í•œëˆˆì— íŒŒì•… ê°€ëŠ¥

### 8.5.2 ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] Prometheus + Grafana ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ êµ¬ì¶• ì™„ë£Œ
- [ ] êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œìœ¼ë¡œ ëª¨ë“  ì´ë²¤íŠ¸ ì¶”ì 
- [ ] ELK ìŠ¤íƒìœ¼ë¡œ ë¡œê·¸ ì¤‘ì•™í™” ë° ê²€ìƒ‰ ê°€ëŠ¥
- [ ] Evidentlyë¥¼ í™œìš©í•œ ë“œë¦¬í”„íŠ¸ ê°ì§€ ìë™í™”
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œìœ¼ë¡œ ë‹¤ì–‘í•œ ì±„ë„ ì§€ì›

### 8.5.3 ìš´ì˜ì  ì™„ë£Œ ê¸°ì¤€
- [ ] 24ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì•ˆì • ìš´ì˜
- [ ] ì¥ì•  ë°œìƒ ì‹œ 5ë¶„ ì´ë‚´ ì•Œë¦¼ ìˆ˜ì‹ 
- [ ] ì„±ëŠ¥ ì €í•˜ ì¡°ê¸° ê°ì§€ë¡œ ì‚¬ì „ ëŒ€ì‘ ê°€ëŠ¥
- [ ] ëª¨ë‹ˆí„°ë§ ë°ì´í„° ê¸°ë°˜ ì‹œìŠ¤í…œ ìµœì í™” ì‹¤í–‰
- [ ] íŒ€ì› ëŒ€ìƒ ëª¨ë‹ˆí„°ë§ ìš´ì˜ êµìœ¡ ì™„ë£Œ

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„ ì¤€ë¹„

### 9ë‹¨ê³„ ì—°ê³„ ì‘ì—…
- ëª¨ë‹ˆí„°ë§ ì´ë²¤íŠ¸ë¥¼ 9ë‹¨ê³„ ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ì˜ íŠ¸ë¦¬ê±°ë¡œ í™œìš©
- ì„±ëŠ¥ ì§€í‘œ ê¸°ë°˜ ìë™ ìŠ¤ì¼€ì¼ë§ ë° ì¬í›ˆë ¨ íŠ¸ë¦¬ê±° ì—°ë™
- ì•Œë¦¼ ì‹œìŠ¤í…œì„ ì „ì²´ MLOps ë¼ì´í”„ì‚¬ì´í´ ì´ë²¤íŠ¸ë¡œ í™•ì¥

### ê³ ë„í™” ê³„íš
- ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ ë„ì…
- ì˜ˆì¸¡ì  ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¥ì•  ì‚¬ì „ ë°©ì§€
- ë¶„ì‚° íŠ¸ë ˆì´ì‹±ìœ¼ë¡œ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ëª¨ë‹ˆí„°ë§
- AI Ops ë„ì…ìœ¼ë¡œ ìë™ ì¥ì•  ëŒ€ì‘

ì´ 8ë‹¨ê³„ë¥¼ ì™„ë£Œí•˜ë©´ ì „ì²´ MLOps ì‹œìŠ¤í…œì˜ ì•ˆì •ì„±ê³¼ ê°€ìš©ì„±ì„ ë³´ì¥í•˜ëŠ” í¬ê´„ì ì¸ ê´€ì¸¡ì„± ì²´ê³„ê°€ êµ¬ì¶•ë©ë‹ˆë‹¤.
