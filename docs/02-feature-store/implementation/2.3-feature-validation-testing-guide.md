---
title: "2.3 í”¼ì²˜ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ"
description: "í”¼ì²˜ í’ˆì§ˆ ë³´ì¥ê³¼ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ê²€ì¦"
author: "MLOps Team"
created: "2025-06-05"
updated: "2025-06-05"
version: "1.0"
stage: "2.3"
category: "Feature Validation"
tags: ["í’ˆì§ˆê²€ì¦", "A/Bí…ŒìŠ¤íŠ¸", "í”¼ì²˜ì¤‘ìš”ë„", "í†µê³„ê²€ì •", "ëª¨ë¸ì„±ëŠ¥"]
prerequisites: ["2.2 í”¼ì²˜ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ", "í†µê³„í•™ ê¸°ì´ˆ", "scikit-learn"]
difficulty: "intermediate"
estimated_time: "4-6ì‹œê°„"
---

# 2.3 í”¼ì²˜ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

**ëª©í‘œ**: í”¼ì²˜ í’ˆì§ˆ ë³´ì¥ê³¼ ëª¨ë¸ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ê²€ì¦

**í•µì‹¬ ê°€ì¹˜**: ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” í”¼ì²˜ë¡œ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™”

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

### ì´ë¡ ì  ëª©í‘œ
- í”¼ì²˜ í’ˆì§ˆ í‰ê°€ ì§€í‘œì™€ ë°©ë²•ë¡  ì´í•´
- í†µê³„ì  ê²€ì •ê³¼ ê°€ì„¤ ê²€ì¦ ë°©ë²• í•™ìŠµ
- í”¼ì²˜ ì¤‘ìš”ë„ì™€ ëª¨ë¸ ì„±ëŠ¥ì˜ ê´€ê³„ íŒŒì•…

### ì‹¤ë¬´ì  ëª©í‘œ
- ìë™í™”ëœ í”¼ì²˜ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•
- A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬í˜„
- í”¼ì²˜ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ ê°œë°œ

---

## ğŸ”§ 2.3.1 ìë™í™”ëœ í’ˆì§ˆ ê²€ì¦

### ì¢…í•© í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ

```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any
from scipy import stats

class FeatureQualityValidator:
    """í”¼ì²˜ í’ˆì§ˆ ì¢…í•© ê²€ì¦ê¸°"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.validation_rules = self._load_validation_rules()
        self.quality_report = {}
        
    def validate_features(self, feature_data: pd.DataFrame) -> Dict[str, Any]:
        """ì „ì²´ í”¼ì²˜ í’ˆì§ˆ ê²€ì¦"""
        
        print("ğŸ” í”¼ì²˜ í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        # 1. ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ì²´í¬
        basic_quality = self._check_basic_quality(feature_data)
        
        # 2. í†µê³„ì  í’ˆì§ˆ ë¶„ì„
        statistical_quality = self._check_statistical_quality(feature_data)
        
        # 3. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
        business_quality = self._check_business_rules(feature_data)
        
        # 4. í”¼ì²˜ ê°„ ê´€ê³„ ê²€ì¦
        relationship_quality = self._check_feature_relationships(feature_data)
        
        # 5. ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        overall_score = self._calculate_overall_quality_score({
            'basic': basic_quality,
            'statistical': statistical_quality,
            'business': business_quality,
            'relationship': relationship_quality
        })
        
        self.quality_report = {
            'overall_score': overall_score,
            'basic_quality': basic_quality,
            'statistical_quality': statistical_quality,
            'business_quality': business_quality,
            'relationship_quality': relationship_quality,
            'recommendations': self._generate_recommendations()
        }
        
        print(f"âœ… í”¼ì²˜ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ - ì¢…í•© ì ìˆ˜: {overall_score:.2f}/100")
        
        return self.quality_report
    
    def _check_basic_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ë³¸ ë°ì´í„° í’ˆì§ˆ ì²´í¬"""
        
        results = {
            'total_features': len(df.columns),
            'total_records': len(df),
            'feature_quality': {}
        }
        
        for column in df.columns:
            series = df[column]
            
            # ê²°ì¸¡ê°’ ë¶„ì„
            missing_count = series.isnull().sum()
            missing_ratio = missing_count / len(series)
            
            # ìœ ë‹ˆí¬ ê°’ ë¶„ì„
            unique_count = series.nunique()
            unique_ratio = unique_count / len(series)
            
            # ì˜ê°’ ë¶„ì„ (ìˆ˜ì¹˜í˜•ì˜ ê²½ìš°)
            zero_count = 0
            zero_ratio = 0
            if pd.api.types.is_numeric_dtype(series):
                zero_count = (series == 0).sum()
                zero_ratio = zero_count / len(series)
            
            feature_quality = {
                'missing_count': missing_count,
                'missing_ratio': missing_ratio,
                'unique_count': unique_count,
                'unique_ratio': unique_ratio,
                'zero_count': zero_count,
                'zero_ratio': zero_ratio,
                'quality_score': self._calculate_basic_quality_score(
                    missing_ratio, unique_ratio, zero_ratio
                )
            }
            
            results['feature_quality'][column] = feature_quality
        
        # ì „ì²´ ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
        feature_scores = [
            fq['quality_score'] 
            for fq in results['feature_quality'].values()
        ]
        results['overall_basic_score'] = np.mean(feature_scores) if feature_scores else 0
        
        return results
    
    def _calculate_basic_quality_score(self, missing_ratio: float, 
                                     unique_ratio: float, zero_ratio: float) -> float:
        """ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        # ê²°ì¸¡ê°’ ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        missing_score = max(0, 100 - missing_ratio * 200)
        
        # ìœ ë‹ˆí¬ ê°’ ì ìˆ˜ (ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ìœ¼ë©´ ì•ˆ ì¢‹ìŒ)
        if unique_ratio < 0.01:  # ë„ˆë¬´ ì ì€ ìœ ë‹ˆí¬ ê°’
            unique_score = 50
        elif unique_ratio > 0.95:  # ê±°ì˜ ëª¨ë“  ê°’ì´ ìœ ë‹ˆí¬
            unique_score = 70
        else:
            unique_score = 100
        
        # ì˜ê°’ ì ìˆ˜ (ë„ˆë¬´ ë§ìœ¼ë©´ ì•ˆ ì¢‹ìŒ)
        zero_score = max(0, 100 - zero_ratio * 150)
        
        return (missing_score * 0.5 + unique_score * 0.3 + zero_score * 0.2)
```

### ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

```python
class RealTimeQualityMonitor:
    """ì‹¤ì‹œê°„ í”¼ì²˜ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, baseline_stats: Dict[str, Any]):
        self.baseline_stats = baseline_stats
        self.alert_thresholds = {
            'missing_ratio_increase': 0.1,  # 10% ì¦ê°€
            'distribution_shift': 0.05,     # p-value < 0.05
            'outlier_ratio_increase': 0.05  # 5% ì¦ê°€
        }
        
    def monitor_batch(self, new_data: pd.DataFrame) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ë°°ì¹˜ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        
        alerts = []
        monitoring_results = {}
        
        for column in new_data.columns:
            if column not in self.baseline_stats:
                continue
                
            baseline = self.baseline_stats[column]
            current_stats = self._calculate_column_stats(new_data[column])
            
            # ê²°ì¸¡ê°’ ë¹„ìœ¨ ë³€í™” ì²´í¬
            baseline_missing = baseline.get('missing_ratio', 0)
            current_missing = current_stats['missing_ratio']
            
            if current_missing - baseline_missing > self.alert_thresholds['missing_ratio_increase']:
                alerts.append({
                    'type': 'missing_ratio_increase',
                    'feature': column,
                    'baseline': baseline_missing,
                    'current': current_missing,
                    'severity': 'high' if current_missing > 0.2 else 'medium'
                })
            
            monitoring_results[column] = {
                'baseline_stats': baseline,
                'current_stats': current_stats,
                'alerts': [alert for alert in alerts if alert.get('feature') == column]
            }
        
        return {
            'timestamp': pd.Timestamp.now().isoformat(),
            'total_alerts': len(alerts),
            'alerts': alerts,
            'feature_monitoring': monitoring_results
        }
    
    def _calculate_column_stats(self, series: pd.Series) -> Dict[str, Any]:
        """ì»¬ëŸ¼ í†µê³„ ê³„ì‚°"""
        
        stats = {
            'missing_ratio': series.isnull().sum() / len(series),
            'unique_count': series.nunique(),
            'unique_ratio': series.nunique() / len(series)
        }
        
        if pd.api.types.is_numeric_dtype(series):
            stats.update({
                'mean': series.mean(),
                'std': series.std(),
                'min': series.min(),
                'max': series.max(),
                'q25': series.quantile(0.25),
                'q50': series.quantile(0.50),
                'q75': series.quantile(0.75)
            })
        
        return stats
```

---

## ğŸ”§ 2.3.2 í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„

### ì¢…í•©ì ì¸ ì¤‘ìš”ë„ ë¶„ì„ê¸°

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.inspection import permutation_importance
from sklearn.feature_selection import mutual_info_regression, f_regression

class FeatureImportanceAnalyzer:
    """í”¼ì²˜ ì¤‘ìš”ë„ ì¢…í•© ë¶„ì„ê¸°"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.importance_results = {}
        
    def analyze_feature_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, Any]:
        """ì¢…í•©ì ì¸ í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„"""
        
        print("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì‹œì‘...")
        
        # 1. Random Forest ê¸°ë°˜ ì¤‘ìš”ë„
        rf_importance = self._calculate_rf_importance(X, y)
        
        # 2. Permutation Importance
        perm_importance = self._calculate_permutation_importance(X, y)
        
        # 3. ìƒí˜¸ì •ë³´ëŸ‰ (Mutual Information)
        mi_importance = self._calculate_mutual_information(X, y)
        
        # 4. í†µê³„ì  ì¤‘ìš”ë„ (F-í†µê³„ëŸ‰)
        statistical_importance = self._calculate_statistical_importance(X, y)
        
        # 5. ì¢…í•© ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        combined_importance = self._combine_importance_scores({
            'random_forest': rf_importance,
            'permutation': perm_importance,
            'mutual_information': mi_importance,
            'statistical': statistical_importance
        })
        
        self.importance_results = {
            'combined_importance': combined_importance,
            'individual_methods': {
                'random_forest': rf_importance,
                'permutation': perm_importance,
                'mutual_information': mi_importance,
                'statistical': statistical_importance
            },
            'feature_ranking': self._create_feature_ranking(combined_importance)
        }
        
        print("âœ… í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì™„ë£Œ")
        
        return self.importance_results
    
    def _calculate_rf_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Random Forest ê¸°ë°˜ ì¤‘ìš”ë„"""
        
        rf = RandomForestRegressor(
            n_estimators=100,
            random_state=42,
            n_jobs=-1
        )
        
        rf.fit(X, y)
        
        importance_scores = {}
        for feature, importance in zip(X.columns, rf.feature_importances_):
            importance_scores[feature] = importance
            
        return importance_scores
    
    def _calculate_permutation_importance(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """Permutation Importance ê³„ì‚°"""
        
        # ê¸°ë³¸ ëª¨ë¸ í›ˆë ¨
        rf = RandomForestRegressor(n_estimators=50, random_state=42)
        rf.fit(X, y)
        
        # Permutation importance ê³„ì‚°
        perm_result = permutation_importance(
            rf, X, y, 
            n_repeats=10,
            random_state=42,
            n_jobs=-1
        )
        
        importance_scores = {}
        for feature, importance in zip(X.columns, perm_result.importances_mean):
            importance_scores[feature] = importance
            
        return importance_scores
    
    def _calculate_mutual_information(self, X: pd.DataFrame, y: pd.Series) -> Dict[str, float]:
        """ìƒí˜¸ì •ë³´ëŸ‰ ê¸°ë°˜ ì¤‘ìš”ë„"""
        
        # ìˆ˜ì¹˜í˜• í”¼ì²˜ë§Œ ì„ íƒ
        numeric_features = X.select_dtypes(include=[np.number])
        
        if len(numeric_features.columns) == 0:
            return {}
        
        mi_scores = mutual_info_regression(numeric_features, y, random_state=42)
        
        importance_scores = {}
        for feature, score in zip(numeric_features.columns, mi_scores):
            importance_scores[feature] = score
            
        # ì •ê·œí™”
        max_score = max(importance_scores.values()) if importance_scores else 1
        if max_score > 0:
            importance_scores = {
                k: v / max_score for k, v in importance_scores.items()
            }
            
        return importance_scores
    
    def _combine_importance_scores(self, method_scores: Dict[str, Dict[str, float]]) -> Dict[str, float]:
        """ì—¬ëŸ¬ ë°©ë²•ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê²°í•©"""
        
        # ëª¨ë“  í”¼ì²˜ ìˆ˜ì§‘
        all_features = set()
        for scores in method_scores.values():
            all_features.update(scores.keys())
        
        combined_scores = {}
        
        for feature in all_features:
            scores = []
            weights = []
            
            for method, method_weight in [
                ('random_forest', 0.3),
                ('permutation', 0.4),
                ('mutual_information', 0.2),
                ('statistical', 0.1)
            ]:
                if method in method_scores and feature in method_scores[method]:
                    scores.append(method_scores[method][feature])
                    weights.append(method_weight)
            
            if scores:
                # ê°€ì¤‘ í‰ê·  ê³„ì‚°
                combined_scores[feature] = np.average(scores, weights=weights)
            else:
                combined_scores[feature] = 0.0
        
        return combined_scores
```

---

## ğŸ”§ 2.3.3 A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬

### í”¼ì²˜ ì‹¤í—˜ ì„¤ê³„

```python
import hashlib
from datetime import datetime, timedelta

class FeatureABTestFramework:
    """í”¼ì²˜ A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.experiments = {}
        self.results_tracker = {}
        
    def create_experiment(self, experiment_name: str, 
                         control_features: List[str],
                         treatment_features: List[str],
                         split_ratio: float = 0.5,
                         duration_days: int = 7) -> str:
        """A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±"""
        
        experiment_id = self._generate_experiment_id(experiment_name)
        
        experiment = {
            'id': experiment_id,
            'name': experiment_name,
            'control_features': control_features,
            'treatment_features': treatment_features,
            'split_ratio': split_ratio,
            'duration_days': duration_days,
            'start_date': datetime.now(),
            'end_date': datetime.now() + timedelta(days=duration_days),
            'status': 'active',
            'participants': {'control': 0, 'treatment': 0},
            'metrics': {'control': [], 'treatment': []}
        }
        
        self.experiments[experiment_id] = experiment
        
        print(f"âœ… A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ìƒì„±: {experiment_name} (ID: {experiment_id})")
        
        return experiment_id
    
    def assign_user_to_group(self, user_id: str, experiment_id: str) -> str:
        """ì‚¬ìš©ìë¥¼ ì‹¤í—˜ ê·¸ë£¹ì— í• ë‹¹"""
        
        if experiment_id not in self.experiments:
            raise ValueError(f"ì‹¤í—˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {experiment_id}")
        
        experiment = self.experiments[experiment_id]
        
        # í•´ì‹œ ê¸°ë°˜ ì¼ê´€ëœ ê·¸ë£¹ í• ë‹¹
        hash_input = f"{user_id}_{experiment_id}".encode()
        hash_value = int(hashlib.md5(hash_input).hexdigest(), 16)
        
        split_ratio = experiment['split_ratio']
        
        if (hash_value % 100) / 100 < split_ratio:
            group = 'control'
        else:
            group = 'treatment'
        
        # ì°¸ì—¬ì ìˆ˜ ì—…ë°ì´íŠ¸
        experiment['participants'][group] += 1
        
        return group
    
    def record_metric(self, user_id: str, experiment_id: str, 
                     metric_name: str, metric_value: float):
        """ì‹¤í—˜ ë©”íŠ¸ë¦­ ê¸°ë¡"""
        
        group = self.assign_user_to_group(user_id, experiment_id)
        experiment = self.experiments[experiment_id]
        
        metric_record = {
            'user_id': user_id,
            'timestamp': datetime.now().isoformat(),
            'metric_name': metric_name,
            'metric_value': metric_value,
            'group': group
        }
        
        experiment['metrics'][group].append(metric_record)
    
    def analyze_experiment_results(self, experiment_id: str) -> Dict[str, Any]:
        """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„"""
        
        experiment = self.experiments[experiment_id]
        control_metrics = experiment['metrics']['control']
        treatment_metrics = experiment['metrics']['treatment']
        
        # ë©”íŠ¸ë¦­ë³„ ë¶„ì„
        metric_analysis = {}
        
        all_metric_names = set()
        for record in control_metrics + treatment_metrics:
            all_metric_names.add(record['metric_name'])
        
        for metric_name in all_metric_names:
            control_values = [
                r['metric_value'] for r in control_metrics 
                if r['metric_name'] == metric_name
            ]
            treatment_values = [
                r['metric_value'] for r in treatment_metrics 
                if r['metric_name'] == metric_name
            ]
            
            analysis = self._analyze_metric_difference(
                control_values, treatment_values, metric_name
            )
            metric_analysis[metric_name] = analysis
        
        return {
            'experiment_id': experiment_id,
            'experiment_name': experiment['name'],
            'participants': experiment['participants'],
            'metric_analysis': metric_analysis,
            'overall_winner': self._determine_overall_winner(metric_analysis)
        }
    
    def _analyze_metric_difference(self, control_values: List[float], 
                                 treatment_values: List[float], 
                                 metric_name: str) -> Dict[str, Any]:
        """ê°œë³„ ë©”íŠ¸ë¦­ ì°¨ì´ ë¶„ì„"""
        
        if not control_values or not treatment_values:
            return {
                'error': 'Insufficient data',
                'control_count': len(control_values),
                'treatment_count': len(treatment_values)
            }
        
        control_mean = np.mean(control_values)
        treatment_mean = np.mean(treatment_values)
        
        # íš¨ê³¼ í¬ê¸° ê³„ì‚°
        effect_size = treatment_mean - control_mean
        relative_effect = (effect_size / control_mean * 100) if control_mean != 0 else 0
        
        # í†µê³„ì  ìœ ì˜ì„± ê²€ì • (t-test)
        from scipy.stats import ttest_ind
        
        t_stat, p_value = ttest_ind(treatment_values, control_values)
        
        return {
            'metric_name': metric_name,
            'control_stats': {
                'mean': control_mean,
                'std': np.std(control_values),
                'count': len(control_values)
            },
            'treatment_stats': {
                'mean': treatment_mean,
                'std': np.std(treatment_values),
                'count': len(treatment_values)
            },
            'effect_analysis': {
                'absolute_effect': effect_size,
                'relative_effect_percent': relative_effect,
                'statistical_significance': {
                    't_statistic': t_stat,
                    'p_value': p_value,
                    'is_significant': p_value < 0.05
                }
            },
            'recommendation': self._get_metric_recommendation(
                effect_size, relative_effect, p_value
            )
        }
    
    def _generate_experiment_id(self, experiment_name: str) -> str:
        """ì‹¤í—˜ ID ìƒì„±"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        hash_input = f"{experiment_name}_{timestamp}".encode()
        hash_value = hashlib.md5(hash_input).hexdigest()[:8]
        return f"exp_{hash_value}"
```

### ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 

```python
class ModelPerformanceTracker:
    """ëª¨ë¸ ì„±ëŠ¥ ì¶”ì ê¸°"""
    
    def __init__(self):
        self.performance_history = []
        self.baseline_metrics = {}
        
    def track_model_performance(self, model_version: str, 
                              feature_set: List[str],
                              X_test: pd.DataFrame, 
                              y_test: pd.Series,
                              model) -> Dict[str, Any]:
        """ëª¨ë¸ ì„±ëŠ¥ ì¶”ì """
        
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        y_pred = model.predict(X_test[feature_set])
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'mse': mean_squared_error(y_test, y_pred),
            'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
            'mae': mean_absolute_error(y_test, y_pred),
            'r2': r2_score(y_test, y_pred)
        }
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        residuals = y_test - y_pred
        metrics.update({
            'mean_residual': np.mean(residuals),
            'std_residual': np.std(residuals),
            'max_residual': np.max(np.abs(residuals))
        })
        
        # ì„±ëŠ¥ ê¸°ë¡
        performance_record = {
            'timestamp': datetime.now().isoformat(),
            'model_version': model_version,
            'feature_set': feature_set,
            'feature_count': len(feature_set),
            'test_samples': len(y_test),
            'metrics': metrics
        }
        
        self.performance_history.append(performance_record)
        
        # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµ
        comparison = self._compare_with_baseline(metrics, model_version)
        
        return {
            'performance_record': performance_record,
            'baseline_comparison': comparison,
            'improvement_summary': self._summarize_improvements(comparison)
        }
    
    def _compare_with_baseline(self, current_metrics: Dict[str, float], 
                             model_version: str) -> Dict[str, Any]:
        """ë² ì´ìŠ¤ë¼ì¸ê³¼ ì„±ëŠ¥ ë¹„êµ"""
        
        if not self.baseline_metrics:
            # ì²« ë²ˆì§¸ ëª¨ë¸ì„ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì„¤ì •
            self.baseline_metrics = current_metrics.copy()
            return {'status': 'baseline_set', 'baseline_metrics': self.baseline_metrics}
        
        comparison = {}
        for metric_name, current_value in current_metrics.items():
            baseline_value = self.baseline_metrics.get(metric_name, 0)
            
            if baseline_value != 0:
                improvement_ratio = (baseline_value - current_value) / baseline_value
                # ë‚®ì€ ê°’ì´ ì¢‹ì€ ë©”íŠ¸ë¦­ë“¤ (MSE, RMSE, MAE)
                if metric_name in ['mse', 'rmse', 'mae']:
                    improvement_ratio = improvement_ratio  # ê°ì†Œê°€ ê°œì„ 
                else:  # ë†’ì€ ê°’ì´ ì¢‹ì€ ë©”íŠ¸ë¦­ë“¤ (R2)
                    improvement_ratio = -improvement_ratio  # ì¦ê°€ê°€ ê°œì„ 
            else:
                improvement_ratio = 0
            
            comparison[metric_name] = {
                'baseline': baseline_value,
                'current': current_value,
                'improvement_ratio': improvement_ratio,
                'improvement_percent': improvement_ratio * 100
            }
        
        return comparison
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ìë™í™”ëœ í”¼ì²˜ í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] 5ê°€ì§€ ì´ìƒ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚° ë°©ë²• êµ¬í˜„
- [ ] A/B í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ êµ¬ì¶•
- [ ] ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ëª¨ë¸ ì„±ëŠ¥ ì¶”ì  ì‹œìŠ¤í…œ êµ¬í˜„

### ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] í’ˆì§ˆ ê²€ì¦ ì²˜ë¦¬ ì‹œê°„ 1ë¶„ ì´ë‚´
- [ ] í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ì •í™•ë„ 95% ì´ìƒ
- [ ] A/B í…ŒìŠ¤íŠ¸ í†µê³„ì  ìœ ì˜ì„± ê²€ì • êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìë™ ìˆ˜ì§‘ ë° ì €ì¥

### í’ˆì§ˆ ì™„ë£Œ ê¸°ì¤€
- [ ] ëª¨ë“  ê²€ì¦ ë¡œì§ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] í†µê³„ì  ê²€ì • ë°©ë²•ë¡  ë¬¸ì„œí™”
- [ ] í’ˆì§ˆ ì„ê³„ê°’ ë° ì•Œë¦¼ ê¸°ì¤€ ìˆ˜ë¦½
- [ ] A/B í…ŒìŠ¤íŠ¸ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ê°€ì´ë“œ ì‘ì„±
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° SLA ì •ì˜

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì™„ë£Œ í›„ [2.4 í”¼ì²˜ ë©”íƒ€ë°ì´í„° ê´€ë¦¬](./2.4-feature-metadata-management-guide.md)ë¡œ ì§„í–‰í•˜ì—¬ í”¼ì²˜ì˜ ìƒì„± ê³¼ì •ê³¼ í’ˆì§ˆ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¬¸ì„œí™”í•©ë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Statistical Methods for A/B Testing](https://exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf)
- [Feature Selection Techniques](https://scikit-learn.org/stable/modules/feature_selection.html)
- [Model Validation Strategies](https://scikit-learn.org/stable/modules/cross_validation.html)
- [Data Quality Assessment](https://www.kaggle.com/learn/data-cleaning)
