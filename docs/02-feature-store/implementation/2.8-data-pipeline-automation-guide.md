---
title: "2.8 ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™” ê°€ì´ë“œ"
description: "Apache Airflowë¥¼ í™œìš©í•œ TMDB ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ì „ ìë™í™”"
author: "MLOps Team"
created: "2025-06-06"
updated: "2025-06-06"
version: "1.0"
stage: "2.8"
category: "Pipeline Automation"
tags: ["Airflow", "ìë™í™”", "ìŠ¤ì¼€ì¤„ë§", "ë°ì´í„°íŒŒì´í”„ë¼ì¸", "ëª¨ë‹ˆí„°ë§"]
prerequisites: ["2.7 Feast í†µí•© ì™„ë£Œ", "Apache Airflow", "TMDB API"]
difficulty: "advanced"
estimated_time: "8-10ì‹œê°„"
improvement_type: "automation"
priority: "high"
---

# 2.8 ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™” ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

**ëª©í‘œ**: í˜„ì¬ êµ¬í˜„ëœ TMDB í¬ë¡¤ëŸ¬ì™€ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ Apache Airflowë¡œ ì™„ì „ ìë™í™”

**ê°œì„  ë°°ê²½**: ìˆ˜ë™ ì‹¤í–‰ì—ì„œ ë²—ì–´ë‚˜ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•

---

## ğŸ¯ í˜„ì¬ ìƒíƒœ ë¶„ì„

### êµ¬í˜„ëœ ì»´í¬ë„ŒíŠ¸
```python
# í˜„ì¬ src/data_processing/tmdb_crawler.pyì— êµ¬í˜„ë¨
class TMDBCrawler:
    - get_popular_movies_bulk()      # âœ… êµ¬í˜„ë¨
    - get_movies_by_genre()          # âœ… êµ¬í˜„ë¨  
    - get_trending_movies()          # âœ… êµ¬í˜„ë¨
    - collect_comprehensive_dataset() # âœ… êµ¬í˜„ë¨
    
# í˜„ì¬ src/features/engineering/tmdb_processor.pyì— êµ¬í˜„ë¨
class AdvancedTMDBPreProcessor:
    - extract_all_features()         # âœ… êµ¬í˜„ë¨
    - extract_temporal_features()    # âœ… êµ¬í˜„ë¨
    - extract_statistical_features() # âœ… êµ¬í˜„ë¨
```

### ë¶€ì¡±í•œ ë¶€ë¶„
- âŒ ìë™ ìŠ¤ì¼€ì¤„ë§ ì—†ìŒ
- âŒ ì—ëŸ¬ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ ë¶€ì¡±
- âŒ ë°ì´í„° í’ˆì§ˆ ì²´í¬ ìë™í™” ì—†ìŒ
- âŒ íŒŒì´í”„ë¼ì¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ì—†ìŒ

---

## ğŸ”§ Airflow DAG êµ¬í˜„

### ë©”ì¸ ë°ì´í„° ìˆ˜ì§‘ DAG

```python
# airflow/dags/tmdb_data_collection_dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.data_processing.tmdb_crawler import TMDBCrawler
from src.features.engineering.tmdb_processor import AdvancedTMDBPreProcessor
from src.features.store.feature_store import SimpleFeatureStore

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': days_ago(1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'max_active_runs': 1,
}

dag = DAG(
    'tmdb_data_collection_pipeline',
    default_args=default_args,
    description='TMDB ë°ì´í„° ìˆ˜ì§‘ ë° í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ì˜¤ì „ 2ì‹œ
    catchup=False,
    tags=['tmdb', 'data-collection', 'features'],
)

def collect_tmdb_data(**context):
    """TMDB ë°ì´í„° ìˆ˜ì§‘"""
    crawler = TMDBCrawler()
    
    try:
        # ì¢…í•© ë°ì´í„°ì…‹ ìˆ˜ì§‘
        results = crawler.collect_comprehensive_dataset()
        
        # ìˆ˜ì§‘ í†µê³„ ë¡œê¹…
        stats = crawler.get_collection_stats()
        context['task_instance'].xcom_push(
            key='collection_stats', 
            value=stats
        )
        
        return results['collection_summary']
        
    except Exception as e:
        context['task_instance'].log.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        raise
    finally:
        crawler.close()

def validate_collected_data(**context):
    """ìˆ˜ì§‘ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    from src.data_processing.quality_validator import QualityValidator
    
    validator = QualityValidator()
    
    # ìµœì‹  ìˆ˜ì§‘ íŒŒì¼ ê²€ì¦
    latest_files = validator.get_latest_collection_files()
    validation_results = {}
    
    for file_path in latest_files:
        result = validator.validate_movie_data_file(file_path)
        validation_results[file_path] = result
        
        # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ì‹¤íŒ¨
        if result['quality_score'] < 0.8:
            raise ValueError(f"ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬: {file_path}")
    
    return validation_results

def process_features(**context):
    """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰"""
    from src.data_processing.quality_validator import QualityValidator
    
    # ê²€ì¦ëœ ë°ì´í„° ë¡œë“œ
    validator = QualityValidator()
    movies_data = validator.load_validated_movies()
    
    # í”¼ì²˜ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = AdvancedTMDBPreProcessor(movies_data)
    
    # ëª¨ë“  í”¼ì²˜ ìƒì„±
    features = processor.extract_all_features()
    
    # í”¼ì²˜ ê²€ì¦
    validation_results = processor.validate_features(features)
    
    # ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬
    for category, results in validation_results.items():
        if not all(results.values()):
            raise ValueError(f"í”¼ì²˜ ê²€ì¦ ì‹¤íŒ¨: {category}")
    
    context['task_instance'].xcom_push(
        key='feature_stats',
        value=validation_results
    )
    
    return features

def save_to_feature_store(**context):
    """í”¼ì²˜ ìŠ¤í† ì–´ì— ì €ì¥"""
    # ì´ì „ íƒœìŠ¤í¬ì—ì„œ í”¼ì²˜ ë°ì´í„° ë°›ê¸°
    features = context['task_instance'].xcom_pull(
        task_ids='process_features'
    )
    
    # í”¼ì²˜ ìŠ¤í† ì–´ ì´ˆê¸°í™”
    store = SimpleFeatureStore()
    
    # í”¼ì²˜ë³„ë¡œ ì €ì¥
    saved_paths = {}
    
    for category, df in features.items():
        if category != 'metadata':
            paths = store.save_features(
                feature_group='processed_movies',
                features_data={category: df}
            )
            saved_paths.update(paths)
    
    return saved_paths

def generate_quality_report(**context):
    """ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±"""
    from src.data_processing.quality_reporter import QualityReporter
    
    reporter = QualityReporter()
    
    # ìˆ˜ì§‘ í†µê³„ ê°€ì ¸ì˜¤ê¸°
    collection_stats = context['task_instance'].xcom_pull(
        task_ids='collect_data',
        key='collection_stats'
    )
    
    # í”¼ì²˜ í†µê³„ ê°€ì ¸ì˜¤ê¸°
    feature_stats = context['task_instance'].xcom_pull(
        task_ids='process_features',
        key='feature_stats'
    )
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = reporter.generate_daily_report(
        collection_stats=collection_stats,
        feature_stats=feature_stats,
        date=context['ds']
    )
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_path = reporter.save_report(report, context['ds'])
    
    return report_path

# íƒœìŠ¤í¬ ì •ì˜
collect_data = PythonOperator(
    task_id='collect_data',
    python_callable=collect_tmdb_data,
    dag=dag,
)

validate_data = PythonOperator(
    task_id='validate_data',
    python_callable=validate_collected_data,
    dag=dag,
)

process_features = PythonOperator(
    task_id='process_features',
    python_callable=process_features,
    dag=dag,
)

save_features = PythonOperator(
    task_id='save_features',
    python_callable=save_to_feature_store,
    dag=dag,
)

quality_report = PythonOperator(
    task_id='quality_report',
    python_callable=generate_quality_report,
    dag=dag,
)

# ë°ì´í„° ë°±ì—…
backup_data = BashOperator(
    task_id='backup_data',
    bash_command='python scripts/backup_automation.py --type daily',
    dag=dag,
)

# ì˜ì¡´ì„± ì„¤ì •
collect_data >> validate_data >> process_features >> save_features >> quality_report >> backup_data
```

### ì£¼ê°„ ëª¨ë¸ ì¬í›ˆë ¨ DAG

```python
# airflow/dags/weekly_model_retrain_dag.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator

default_args = {
    'owner': 'mlops-team',
    'depends_on_past': False,
    'start_date': datetime(2025, 6, 1),
    'email_on_failure': True,
    'retries': 2,
    'retry_delay': timedelta(minutes=10),
}

dag = DAG(
    'weekly_model_retrain_pipeline',
    default_args=default_args,
    description='ì£¼ê°„ ëª¨ë¸ ì¬í›ˆë ¨ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 3 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ ì˜¤ì „ 3ì‹œ
    catchup=False,
    tags=['model', 'training', 'weekly'],
)

def prepare_training_data(**context):
    """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
    from src.features.store.feature_store import SimpleFeatureStore
    
    store = SimpleFeatureStore()
    
    # ìµœê·¼ ì¼ì£¼ì¼ í”¼ì²˜ ë°ì´í„° ì¡°íšŒ
    end_date = context['ds']
    start_date = (datetime.strptime(end_date, '%Y-%m-%d') - timedelta(days=7)).strftime('%Y-%m-%d')
    
    features = store.get_features_by_date_range(
        feature_group='processed_movies',
        start_date=start_date,
        end_date=end_date
    )
    
    return len(features)

def train_recommendation_model(**context):
    """ì¶”ì²œ ëª¨ë¸ í›ˆë ¨"""
    # ê¸°ì¡´ my-mlopsì˜ MoviePredictor í™œìš©
    from models.movie_predictor import MoviePredictor
    from src.features.store.feature_store import SimpleFeatureStore
    
    store = SimpleFeatureStore()
    
    # í›ˆë ¨ ë°ì´í„° ë¡œë“œ
    features = store.get_latest_features('processed_movies')
    
    # ëª¨ë¸ ì´ˆê¸°í™” ë° í›ˆë ¨
    model = MoviePredictor()
    training_results = model.train(features)
    
    # MLflowì— ì‹¤í—˜ ê¸°ë¡
    import mlflow
    
    with mlflow.start_run():
        mlflow.log_params(model.get_params())
        mlflow.log_metrics(training_results['metrics'])
        mlflow.sklearn.log_model(model, "recommendation_model")
    
    return training_results

def evaluate_model_performance(**context):
    """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
    from models.model_evaluator import ModelEvaluator
    
    evaluator = ModelEvaluator()
    
    # ìµœì‹  ëª¨ë¸ ë¡œë“œ
    latest_model = evaluator.load_latest_model()
    
    # ì„±ëŠ¥ í‰ê°€
    evaluation_results = evaluator.evaluate_model(latest_model)
    
    # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
    if evaluation_results['accuracy'] < 0.75:
        raise ValueError("ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ì¤€ì— ë¯¸ë‹¬")
    
    return evaluation_results

def deploy_model(**context):
    """ëª¨ë¸ ë°°í¬"""
    from models.model_deployer import ModelDeployer
    
    deployer = ModelDeployer()
    
    # ëª¨ë¸ ë°°í¬
    deployment_result = deployer.deploy_to_production(
        model_version=context['ds']
    )
    
    return deployment_result

# íƒœìŠ¤í¬ ì •ì˜
prepare_data = PythonOperator(
    task_id='prepare_data',
    python_callable=prepare_training_data,
    dag=dag,
)

train_model = PythonOperator(
    task_id='train_model',
    python_callable=train_recommendation_model,
    dag=dag,
)

evaluate_model = PythonOperator(
    task_id='evaluate_model',
    python_callable=evaluate_model_performance,
    dag=dag,
)

deploy_model = PythonOperator(
    task_id='deploy_model',
    python_callable=deploy_model,
    dag=dag,
)

# ì˜ì¡´ì„± ì„¤ì •
prepare_data >> train_model >> evaluate_model >> deploy_model
```

---

## ğŸ”§ ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬ ì‹œìŠ¤í…œ

### ìë™ ì¬ì‹œë„ ë° ì•Œë¦¼

```python
# src/data_processing/error_handler.py
import logging
from typing import Dict, Any, Optional
from datetime import datetime
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class PipelineErrorHandler:
    """íŒŒì´í”„ë¼ì¸ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger("PipelineErrorHandler")
        
    def handle_data_collection_error(self, error: Exception, context: Dict[str, Any]):
        """ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬ ì²˜ë¦¬"""
        
        # ì—ëŸ¬ ë¡œê¹…
        self.logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {error}")
        
        # ë°±ì—… ë°ì´í„° ì†ŒìŠ¤ ì‹œë„
        if self._should_retry_with_backup(error):
            return self._retry_with_backup_source(context)
        
        # ë¶€ë¶„ ìˆ˜ì§‘ ì‹œë„
        if self._should_try_partial_collection(error):
            return self._partial_collection_fallback(context)
        
        # ìµœì¢… ì‹¤íŒ¨ ì²˜ë¦¬
        self._send_failure_notification(error, context)
        raise error
    
    def handle_feature_processing_error(self, error: Exception, context: Dict[str, Any]):
        """í”¼ì²˜ ì²˜ë¦¬ ì—ëŸ¬ ì²˜ë¦¬"""
        
        # ì´ì „ ì„±ê³µí•œ í”¼ì²˜ ì‚¬ìš©
        if self._has_previous_features():
            self.logger.warning("ì´ì „ í”¼ì²˜ ì‚¬ìš©ìœ¼ë¡œ í´ë°±")
            return self._load_previous_features()
        
        # ê¸°ë³¸ í”¼ì²˜ë§Œ ìƒì„±
        return self._generate_basic_features_only(context)
    
    def _should_retry_with_backup(self, error: Exception) -> bool:
        """ë°±ì—… ì†ŒìŠ¤ ì¬ì‹œë„ ì—¬ë¶€ ê²°ì •"""
        # API ì œí•œì´ë‚˜ ì„œë²„ ì˜¤ë¥˜ì¸ ê²½ìš°
        return "429" in str(error) or "500" in str(error)
    
    def _send_failure_notification(self, error: Exception, context: Dict[str, Any]):
        """ì‹¤íŒ¨ ì•Œë¦¼ ì „ì†¡"""
        if not self.config.get('email_notifications'):
            return
            
        subject = f"MLOps íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì•Œë¦¼ - {datetime.now().strftime('%Y-%m-%d %H:%M')}"
        
        body = f"""
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.
        
        ì—ëŸ¬: {str(error)}
        ì‹œê°„: {datetime.now().isoformat()}
        ì»¨í…ìŠ¤íŠ¸: {context}
        
        ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.
        """
        
        self._send_email(subject, body)
    
    def _send_email(self, subject: str, body: str):
        """ì´ë©”ì¼ ì „ì†¡"""
        try:
            msg = MIMEMultipart()
            msg['From'] = self.config['email_from']
            msg['To'] = self.config['email_to']
            msg['Subject'] = subject
            
            msg.attach(MIMEText(body, 'plain'))
            
            server = smtplib.SMTP(self.config['smtp_server'], self.config['smtp_port'])
            server.starttls()
            server.login(self.config['email_from'], self.config['email_password'])
            
            text = msg.as_string()
            server.sendmail(self.config['email_from'], self.config['email_to'], text)
            server.quit()
            
        except Exception as e:
            self.logger.error(f"ì´ë©”ì¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
```

---

## ğŸ”§ ë°ì´í„° í’ˆì§ˆ ìë™ ì²´í¬

### ì‹¤ì‹œê°„ ë°ì´í„° ê²€ì¦

```python
# src/data_processing/quality_validator.py í™•ì¥
class EnhancedQualityValidator:
    """ê°•í™”ëœ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger("QualityValidator")
        
        # í’ˆì§ˆ ê¸°ì¤€ ì •ì˜
        self.quality_thresholds = {
            'missing_data_ratio': 0.1,      # ê²°ì¸¡ê°’ 10% ì´í•˜
            'duplicate_ratio': 0.05,        # ì¤‘ë³µ 5% ì´í•˜
            'outlier_ratio': 0.15,          # ì´ìƒì¹˜ 15% ì´í•˜
            'schema_compliance': 0.95,      # ìŠ¤í‚¤ë§ˆ ì¤€ìˆ˜ìœ¨ 95% ì´ìƒ
            'data_freshness_hours': 24,     # ë°ì´í„° ì‹ ì„ ë„ 24ì‹œê°„ ì´ë‚´
        }
    
    def validate_pipeline_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ë°ì´í„° ì¢…í•© ê²€ì¦"""
        
        validation_results = {
            'timestamp': datetime.now().isoformat(),
            'overall_score': 0.0,
            'passed': False,
            'checks': {}
        }
        
        checks = [
            self._check_data_completeness,
            self._check_data_quality,
            self._check_schema_compliance,
            self._check_data_freshness,
            self._check_business_rules
        ]
        
        total_score = 0
        
        for check in checks:
            result = check(data)
            validation_results['checks'][check.__name__] = result
            total_score += result['score']
        
        validation_results['overall_score'] = total_score / len(checks)
        validation_results['passed'] = validation_results['overall_score'] >= 0.8
        
        # í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ ì‹œ ì•Œë¦¼
        if not validation_results['passed']:
            self._send_quality_alert(validation_results)
        
        return validation_results
    
    def _check_data_completeness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì™„ì „ì„± ê²€ì‚¬"""
        
        required_fields = ['id', 'title', 'vote_average', 'popularity', 'release_date']
        missing_counts = {}
        
        for field in required_fields:
            if field in data:
                missing_count = sum(1 for item in data[field] if item is None or item == '')
                missing_counts[field] = missing_count / len(data[field])
        
        avg_missing_ratio = sum(missing_counts.values()) / len(missing_counts)
        
        return {
            'score': 1.0 - avg_missing_ratio,
            'passed': avg_missing_ratio <= self.quality_thresholds['missing_data_ratio'],
            'details': missing_counts
        }
    
    def _check_data_freshness(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ë°ì´í„° ì‹ ì„ ë„ ê²€ì‚¬"""
        
        if 'collection_timestamp' in data:
            collection_time = datetime.fromisoformat(data['collection_timestamp'])
            hours_old = (datetime.now() - collection_time).total_seconds() / 3600
            
            freshness_score = max(0, 1 - (hours_old / self.quality_thresholds['data_freshness_hours']))
            
            return {
                'score': freshness_score,
                'passed': hours_old <= self.quality_thresholds['data_freshness_hours'],
                'hours_old': hours_old
            }
        
        return {'score': 0.5, 'passed': False, 'message': 'No timestamp found'}
    
    def _send_quality_alert(self, validation_results: Dict[str, Any]):
        """í’ˆì§ˆ ì•Œë¦¼ ì „ì†¡"""
        self.logger.warning(f"ë°ì´í„° í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬: {validation_results['overall_score']:.2f}")
        
        # Slack ì›¹í›… ì „ì†¡ (ì‹¤ì œ í™˜ê²½ì—ì„œ êµ¬í˜„)
        webhook_url = os.getenv('SLACK_WEBHOOK_URL')
        if webhook_url:
            self._send_slack_notification(webhook_url, validation_results)
    
    def _send_slack_notification(self, webhook_url: str, results: Dict[str, Any]):
        """Slack ì•Œë¦¼ ì „ì†¡"""
        import requests
        
        message = {
            "text": f"ğŸš¨ ë°ì´í„° í’ˆì§ˆ ì•Œë¦¼",
            "attachments": [
                {
                    "color": "danger",
                    "fields": [
                        {"title": "í’ˆì§ˆ ì ìˆ˜", "value": f"{results['overall_score']:.2f}", "short": True},
                        {"title": "í†µê³¼ ì—¬ë¶€", "value": "âŒ ì‹¤íŒ¨" if not results['passed'] else "âœ… í†µê³¼", "short": True}
                    ]
                }
            ]
        }
        
        try:
            requests.post(webhook_url, json=message)
        except Exception as e:
            self.logger.error(f"Slack ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] Airflow DAG ì„¤ì • ë° ì‹¤í–‰ í™•ì¸
- [ ] ìë™ ìŠ¤ì¼€ì¤„ë§ ë™ì‘ ê²€ì¦ (ë§¤ì¼/ì£¼ê°„)
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ìë™ ë³µêµ¬ ë™ì‘ í™•ì¸
- [ ] ë°ì´í„° í’ˆì§ˆ ì²´í¬ ìë™í™” ë™ì‘ ê²€ì¦
- [ ] ì•Œë¦¼ ì‹œìŠ¤í…œ (ì´ë©”ì¼/Slack) ë™ì‘ í™•ì¸

### ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] DAG ì‹¤í–‰ ì„±ê³µë¥  95% ì´ìƒ
- [ ] ì—ëŸ¬ ë³µêµ¬ ì„±ê³µë¥  80% ì´ìƒ
- [ ] ë°ì´í„° í’ˆì§ˆ ì²´í¬ ìë™í™” 100% ì ìš©
- [ ] íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹œê°„ 2ì‹œê°„ ì´ë‚´
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### ìš´ì˜ ì™„ë£Œ ê¸°ì¤€
- [ ] ì£¼ 7ì¼ ë¬´ì¸ ìš´ì˜ ê°€ëŠ¥
- [ ] ì¥ì•  ë°œìƒ ì‹œ 5ë¶„ ì´ë‚´ ì•Œë¦¼
- [ ] ë°ì´í„° í’ˆì§ˆ SLA 95% ì´ìƒ ìœ ì§€
- [ ] ìš´ì˜ ë¬¸ì„œ ë° ê°€ì´ë“œ ì‘ì„± ì™„ë£Œ
- [ ] íŒ€ì› êµìœ¡ ë° ì¸ìˆ˜ì¸ê³„ ì™„ë£Œ

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì™„ë£Œ í›„ [2.9 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•](./2.9-monitoring-system-guide.md)ìœ¼ë¡œ ì§„í–‰í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ìƒíƒœë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ì‹œìŠ¤í…œì„ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [MLOps Pipeline Best Practices](https://ml-ops.org/)
- [Data Quality Monitoring](https://greatexpectations.io/)
- [Pipeline Automation Patterns](https://martinfowler.com/articles/cd4ml.html)
