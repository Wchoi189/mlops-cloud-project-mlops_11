---
title: "2.9 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ"
description: "Prometheus, Grafana, ELK Stackì„ í™œìš©í•œ ì¢…í•© ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"
author: "MLOps Team"
created: "2025-06-06"
updated: "2025-06-06"
version: "1.0"
stage: "2.9"
category: "Monitoring & Observability"
tags: ["Prometheus", "Grafana", "ELK", "ëª¨ë‹ˆí„°ë§", "ì•Œë¦¼", "ëŒ€ì‹œë³´ë“œ"]
prerequisites: ["2.8 íŒŒì´í”„ë¼ì¸ ìë™í™” ì™„ë£Œ", "Docker", "ì‹œê³„ì—´ ë°ì´í„°ë² ì´ìŠ¤"]
difficulty: "advanced"
estimated_time: "10-12ì‹œê°„"
improvement_type: "monitoring"
priority: "high"
---

# 2.9 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶• ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

**ëª©í‘œ**: í˜„ì¬ êµ¬í˜„ëœ MLOps ì‹œìŠ¤í…œì— ì¢…í•©ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ê´€ì¸¡ì„± ì‹œìŠ¤í…œ ì¶”ê°€

**ê°œì„  ë°°ê²½**: í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•ˆì •ì  ìš´ì˜ì„ ìœ„í•œ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í•„ìš”

---

## ğŸ”§ ELK Stack ë¡œê·¸ ê´€ë¦¬

### Logstash íŒŒì´í”„ë¼ì¸ ì„¤ì •

```ruby
# config/logstash/pipeline/mlops-logs.conf
input {
  beats {
    port => 5044
  }
  
  file {
    path => "/var/log/mlops/*.log"
    start_position => "beginning"
    sincedb_path => "/dev/null"
  }
}

filter {
  # TMDB í¬ë¡¤ëŸ¬ ë¡œê·¸ íŒŒì‹±
  if [source] =~ "tmdb_crawler" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:message}"
      }
    }
    
    # ìˆ˜ì§‘ í†µê³„ ì¶”ì¶œ
    if [message] =~ "ìˆ˜ì§‘ ì™„ë£Œ" {
      grok {
        match => {
          "message" => "ìˆ˜ì§‘ ì™„ë£Œ: %{NUMBER:movies_collected:int}ê°œ"
        }
      }
    }
  }
  
  # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œê·¸ íŒŒì‹±
  if [source] =~ "tmdb_processor" {
    grok {
      match => {
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} - %{DATA:feature_type} í”¼ì²˜ %{NUMBER:feature_count:int}ê°œ ìƒì„±"
      }
    }
  }
  
  # API ì„œë²„ ë¡œê·¸ íŒŒì‹±
  if [source] =~ "api_server" {
    grok {
      match => {
        "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent} %{NUMBER:request_time:float}"
      }
    }
  }
  
  # ì—ëŸ¬ ë¡œê·¸ íŠ¹ë³„ ì²˜ë¦¬
  if [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
      add_field => { "alert_required" => "true" }
    }
  }
  
  # ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
  if [message] =~ "duration" {
    grok {
      match => {
        "message" => "duration: %{NUMBER:duration:float}ms"
      }
    }
  }
  
  # ë‚ ì§œ íŒŒì‹±
  date {
    match => [ "timestamp", "ISO8601" ]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "mlops-logs-%{+YYYY.MM.dd}"
  }
  
  # ì—ëŸ¬ ë¡œê·¸ëŠ” ë³„ë„ ì¸ë±ìŠ¤ë¡œ
  if "error" in [tags] {
    elasticsearch {
      hosts => ["elasticsearch:9200"]
      index => "mlops-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # ì‹¤ì‹œê°„ ì•Œë¦¼ì„ ìœ„í•œ ì¶œë ¥
  if [alert_required] == "true" {
    http {
      url => "http://alertmanager:9093/api/v1/alerts"
      http_method => "post"
      content_type => "application/json"
      format => "json"
    }
  }
  
  stdout { codec => rubydebug }
}
```

### Elasticsearch ì¸ë±ìŠ¤ í…œí”Œë¦¿

```json
{
  "index_patterns": ["mlops-logs-*"],
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.refresh_interval": "5s"
    },
    "mappings": {
      "properties": {
        "@timestamp": { "type": "date" },
        "level": { "type": "keyword" },
        "logger": { "type": "keyword" },
        "message": { "type": "text" },
        "source": { "type": "keyword" },
        "movies_collected": { "type": "integer" },
        "feature_count": { "type": "integer" },
        "feature_type": { "type": "keyword" },
        "duration": { "type": "float" },
        "response": { "type": "integer" },
        "request_time": { "type": "float" },
        "clientip": { "type": "ip" },
        "verb": { "type": "keyword" },
        "request": { "type": "text" }
      }
    }
  }
}
```

### Kibana ëŒ€ì‹œë³´ë“œ

```json
{
  "version": "8.0.0",
  "objects": [
    {
      "id": "mlops-overview-dashboard",
      "type": "dashboard",
      "attributes": {
        "title": "MLOps ë¡œê·¸ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
        "panelsJSON": "[{\"version\":\"8.0.0\",\"gridData\":{\"x\":0,\"y\":0,\"w\":24,\"h\":15},\"panelIndex\":\"1\",\"embeddableConfig\":{},\"panelRefName\":\"panel_1\"}]",
        "optionsJSON": "{\"useMargins\":true,\"syncColors\":false,\"hidePanelTitles\":false}",
        "timeRestore": false,
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"query\":{\"query\":\"\",\"language\":\"kuery\"},\"filter\":[]}"
        }
      }
    },
    {
      "id": "error-analysis-visualization",
      "type": "visualization",
      "attributes": {
        "title": "ì—ëŸ¬ ë¶„ì„",
        "visState": "{\"title\":\"ì—ëŸ¬ ë¶„ì„\",\"type\":\"histogram\",\"params\":{\"grid\":{\"categoryLines\":false,\"style\":{\"color\":\"#eee\"}},\"categoryAxes\":[{\"id\":\"CategoryAxis-1\",\"type\":\"category\",\"position\":\"bottom\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\"},\"labels\":{\"show\":true,\"truncate\":100},\"title\":{}}],\"valueAxes\":[{\"id\":\"ValueAxis-1\",\"name\":\"LeftAxis-1\",\"type\":\"value\",\"position\":\"left\",\"show\":true,\"style\":{},\"scale\":{\"type\":\"linear\",\"mode\":\"normal\"},\"labels\":{\"show\":true,\"rotate\":0,\"filter\":false,\"truncate\":100},\"title\":{\"text\":\"Count\"}}],\"seriesParams\":[{\"show\":\"true\",\"type\":\"histogram\",\"mode\":\"stacked\",\"data\":{\"label\":\"Count\",\"id\":\"1\"},\"valueAxis\":\"ValueAxis-1\",\"drawLinesBetweenPoints\":true,\"showCircles\":true}],\"addTooltip\":true,\"addLegend\":true,\"legendPosition\":\"right\",\"times\":[],\"addTimeMarker\":false},\"aggs\":[{\"id\":\"1\",\"enabled\":true,\"type\":\"count\",\"schema\":\"metric\",\"params\":{}},{\"id\":\"2\",\"enabled\":true,\"type\":\"terms\",\"schema\":\"segment\",\"params\":{\"field\":\"level\",\"size\":5,\"order\":\"desc\",\"orderBy\":\"1\"}}]}",
        "uiStateJSON": "{}",
        "kibanaSavedObjectMeta": {
          "searchSourceJSON": "{\"index\":\"mlops-logs-*\",\"query\":{\"match\":{\"level\":\"ERROR\"}},\"filter\":[]}"
        }
      }
    }
  ]
}
```

---

## ğŸ”§ ì§€ëŠ¥í˜• ì•Œë¦¼ ì‹œìŠ¤í…œ

### Alertmanager ì„¤ì •

```yaml
# config/alertmanager/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'mlops-alerts@company.com'
  smtp_auth_username: 'mlops-alerts@company.com'
  smtp_auth_password: 'your_password'

# ì•Œë¦¼ ë¼ìš°íŒ… ê·œì¹™
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'
  - match:
      team: data-science
    receiver: 'data-team'

# ì•Œë¦¼ ìˆ˜ì‹ ì ì„¤ì •
receivers:
- name: 'default'
  email_configs:
  - to: 'mlops-team@company.com'
    subject: 'MLOps Alert: {{ .GroupLabels.alertname }}'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Time: {{ .StartsAt }}
      {{ end }}

- name: 'critical-alerts'
  email_configs:
  - to: 'oncall@company.com'
    subject: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#mlops-alerts'
    title: 'ğŸš¨ Critical MLOps Alert'
    text: |
      {{ range .Alerts }}
      *Alert:* {{ .Annotations.summary }}
      *Description:* {{ .Annotations.description }}
      *Severity:* {{ .Labels.severity }}
      {{ end }}

- name: 'data-team'
  slack_configs:
  - api_url: 'https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK'
    channel: '#data-science'
    title: 'Data Pipeline Alert'

# ì•Œë¦¼ ì–µì œ ê·œì¹™
inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
```

### Prometheus ì•Œë¦¼ ê·œì¹™

```yaml
# config/prometheus/alert_rules.yml
groups:
- name: mlops.rules
  rules:
  
  # ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•Œë¦¼
  - alert: DataCollectionFailure
    expr: increase(tmdb_collection_errors_total[1h]) > 3
    for: 5m
    labels:
      severity: critical
      team: data-science
    annotations:
      summary: "TMDB ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ì¦ê°€"
      description: "ì§€ë‚œ 1ì‹œê°„ ë™ì•ˆ {{ $value }}íšŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨"
      
  - alert: FeatureGenerationSlow
    expr: histogram_quantile(0.95, rate(feature_generation_duration_seconds_bucket[5m])) > 300
    for: 10m
    labels:
      severity: warning
      team: data-science
    annotations:
      summary: "í”¼ì²˜ ìƒì„± ì„±ëŠ¥ ì €í•˜"
      description: "í”¼ì²˜ ìƒì„± ì‹œê°„ì´ 5ë¶„ì„ ì´ˆê³¼í•¨"
      
  - alert: DataQualityDegraded
    expr: data_completeness_ratio < 0.9
    for: 5m
    labels:
      severity: warning
      team: data-science
    annotations:
      summary: "ë°ì´í„° í’ˆì§ˆ ì €í•˜"
      description: "ë°ì´í„° ì™„ì „ì„±ì´ {{ $value }}ë¡œ ì €í•˜ë¨"
      
  # ëª¨ë¸ ì„±ëŠ¥ ì•Œë¦¼
  - alert: ModelAccuracyDrop
    expr: model_accuracy_score < 0.75
    for: 15m
    labels:
      severity: critical
      team: ml-engineering
    annotations:
      summary: "ëª¨ë¸ ì •í™•ë„ ê¸‰ë½"
      description: "{{ $labels.model_name }} ëª¨ë¸ ì •í™•ë„ê°€ {{ $value }}ë¡œ í•˜ë½"
      
  - alert: ModelDriftDetected
    expr: feature_drift_score > 0.1
    for: 5m
    labels:
      severity: warning
      team: ml-engineering
    annotations:
      summary: "í”¼ì²˜ ë“œë¦¬í”„íŠ¸ ê°ì§€"
      description: "{{ $labels.feature_name }}ì—ì„œ ë“œë¦¬í”„íŠ¸ ê°ì§€ (ì ìˆ˜: {{ $value }})"
      
  # ì‹œìŠ¤í…œ ì„±ëŠ¥ ì•Œë¦¼
  - alert: HighAPILatency
    expr: histogram_quantile(0.95, rate(api_request_duration_seconds_bucket[5m])) > 1
    for: 5m
    labels:
      severity: warning
      team: platform
    annotations:
      summary: "API ì‘ë‹µ ì§€ì—°"
      description: "{{ $labels.endpoint }} API ì‘ë‹µì‹œê°„ì´ 1ì´ˆ ì´ˆê³¼"
      
  - alert: LowCacheHitRate
    expr: feature_store_cache_hits_total / (feature_store_cache_hits_total + feature_store_cache_misses_total) < 0.7
    for: 10m
    labels:
      severity: warning
      team: platform
    annotations:
      summary: "ìºì‹œ ì ì¤‘ë¥  ì €í•˜"
      description: "í”¼ì²˜ ìŠ¤í† ì–´ ìºì‹œ ì ì¤‘ë¥ ì´ {{ $value }}ë¡œ ì €í•˜"
      
  - alert: HighMemoryUsage
    expr: memory_usage_bytes / (1024*1024*1024) > 8
    for: 5m
    labels:
      severity: critical
      team: platform
    annotations:
      summary: "ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„ê³„ì¹˜ ì´ˆê³¼"
      description: "{{ $labels.service_name }} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {{ $value }}GB"
      
  # ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­ ì•Œë¦¼
  - alert: RecommendationPerformanceDrop
    expr: recommendation_click_rate < 0.05
    for: 30m
    labels:
      severity: warning
      team: product
    annotations:
      summary: "ì¶”ì²œ ì„±ê³¼ ì €í•˜"
      description: "ì¶”ì²œ í´ë¦­ë¥ ì´ {{ $value }}ë¡œ í•˜ë½"
      
  - alert: ConversionRateDrop
    expr: conversion_rate < 0.02
    for: 1h
    labels:
      severity: warning
      team: product
    annotations:
      summary: "ì „í™˜ìœ¨ ì €í•˜"
      description: "{{ $labels.channel }} ì±„ë„ ì „í™˜ìœ¨ì´ {{ $value }}ë¡œ í•˜ë½"
```

---

## ğŸ”§ ìë™í™”ëœ ì´ìƒ íƒì§€

### ê¸°ê³„í•™ìŠµ ê¸°ë°˜ ì´ìƒ íƒì§€

```python
# src/monitoring/anomaly_detector.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from typing import Dict, List, Any, Tuple
import logging
from datetime import datetime, timedelta

class MLOpsAnomalyDetector:
    """MLOps ë©”íŠ¸ë¦­ ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, metrics_collector: MLOpsMetricsCollector):
        self.metrics_collector = metrics_collector
        self.logger = logging.getLogger("AnomalyDetector")
        
        # ì´ìƒ íƒì§€ ëª¨ë¸ë“¤
        self.models = {
            'feature_metrics': IsolationForest(contamination=0.1, random_state=42),
            'model_metrics': IsolationForest(contamination=0.05, random_state=42),
            'system_metrics': IsolationForest(contamination=0.1, random_state=42)
        }
        
        self.scalers = {
            'feature_metrics': StandardScaler(),
            'model_metrics': StandardScaler(),
            'system_metrics': StandardScaler()
        }
        
        # í•™ìŠµ ë°ì´í„° ì €ì¥
        self.training_data = {}
        self.is_trained = False
    
    def collect_training_data(self, days: int = 7) -> None:
        """ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ ë°ì´í„° ìˆ˜ì§‘"""
        self.logger.info(f"ì´ìƒ íƒì§€ í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {days}ì¼")
        
        # Prometheusì—ì„œ ë©”íŠ¸ë¦­ ë°ì´í„° ìˆ˜ì§‘
        from prometheus_api_client import PrometheusConnect
        
        prom = PrometheusConnect(url="http://prometheus:9090", disable_ssl=True)
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)
        
        # í”¼ì²˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        feature_queries = [
            'feature_generation_duration_seconds',
            'feature_drift_score',
            'feature_staleness_hours'
        ]
        
        feature_data = []
        for query in feature_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                feature_data.extend(values)
        
        if feature_data:
            self.training_data['feature_metrics'] = np.array(feature_data).reshape(-1, 1)
        
        # ëª¨ë¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        model_queries = [
            'model_prediction_latency_seconds',
            'model_accuracy_score',
            'prediction_confidence_score'
        ]
        
        model_data = []
        for query in model_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                model_data.extend(values)
        
        if model_data:
            self.training_data['model_metrics'] = np.array(model_data).reshape(-1, 1)
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        system_queries = [
            'cpu_usage_percent',
            'memory_usage_bytes',
            'api_request_duration_seconds'
        ]
        
        system_data = []
        for query in system_queries:
            data = prom.get_metric_range_data(
                metric_name=query,
                start_time=start_time,
                end_time=end_time,
                step='5m'
            )
            if data:
                values = [float(point[1]) for point in data[0]['values']]
                system_data.extend(values)
        
        if system_data:
            self.training_data['system_metrics'] = np.array(system_data).reshape(-1, 1)
        
        self.logger.info(f"í›ˆë ¨ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(self.training_data)} ì¹´í…Œê³ ë¦¬")
    
    def train_models(self) -> None:
        """ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨"""
        if not self.training_data:
            raise ValueError("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. collect_training_data()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        self.logger.info("ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        
        for category, data in self.training_data.items():
            if len(data) < 100:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
                self.logger.warning(f"{category}: í›ˆë ¨ ë°ì´í„° ë¶€ì¡± ({len(data)}ê°œ)")
                continue
            
            # ë°ì´í„° ì •ê·œí™”
            scaled_data = self.scalers[category].fit_transform(data)
            
            # ëª¨ë¸ í›ˆë ¨
            self.models[category].fit(scaled_data)
            
            self.logger.info(f"{category} ì´ìƒ íƒì§€ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
        self.is_trained = True
    
    def detect_anomalies(self, current_metrics: Dict[str, float]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì´ìƒ íƒì§€"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_models()ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        anomalies = {}
        
        for category, metrics in current_metrics.items():
            if category not in self.models:
                continue
            
            # ë©”íŠ¸ë¦­ ë°°ì—´ë¡œ ë³€í™˜
            metric_array = np.array(list(metrics.values())).reshape(1, -1)
            
            # ì •ê·œí™”
            scaled_metrics = self.scalers[category].transform(metric_array)
            
            # ì´ìƒ íƒì§€
            anomaly_score = self.models[category].decision_function(scaled_metrics)[0]
            is_anomaly = self.models[category].predict(scaled_metrics)[0] == -1
            
            anomalies[category] = {
                'is_anomaly': is_anomaly,
                'anomaly_score': anomaly_score,
                'metrics': metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            if is_anomaly:
                self.logger.warning(f"ì´ìƒ íƒì§€: {category} (ì ìˆ˜: {anomaly_score:.3f})")
                self._send_anomaly_alert(category, anomalies[category])
        
        return anomalies
    
    def _send_anomaly_alert(self, category: str, anomaly_info: Dict[str, Any]):
        """ì´ìƒ íƒì§€ ì•Œë¦¼ ì „ì†¡"""
        message = f"""
        ğŸš¨ MLOps ì´ìƒ íƒì§€!
        
        ì¹´í…Œê³ ë¦¬: {category}
        ì´ìƒ ì ìˆ˜: {anomaly_info['anomaly_score']:.3f}
        ê°ì§€ ì‹œê°„: {anomaly_info['timestamp']}
        
        ê´€ë ¨ ë©”íŠ¸ë¦­:
        {anomaly_info['metrics']}
        
        ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
        """
        
        # Slackìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡ (ì‹¤ì œ í™˜ê²½ì—ì„œ êµ¬í˜„)
        self.logger.critical(message)


class HealthChecker:
    """ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ì²´í¬"""
    
    def __init__(self):
        self.logger = logging.getLogger("HealthChecker")
        
        # ê±´ê°• ìƒíƒœ ê¸°ì¤€
        self.health_thresholds = {
            'api_response_time': 1.0,      # 1ì´ˆ ì´í•˜
            'cache_hit_rate': 0.8,         # 80% ì´ìƒ
            'data_freshness_hours': 2,     # 2ì‹œê°„ ì´ë‚´
            'error_rate': 0.01,            # 1% ì´í•˜
            'memory_usage_gb': 8,          # 8GB ì´í•˜
            'cpu_usage_percent': 80        # 80% ì´í•˜
        }
    
    def check_system_health(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì „ì²´ ê±´ê°• ìƒíƒœ ì²´í¬"""
        
        health_status = {
            'timestamp': datetime.now().isoformat(),
            'overall_health': 'healthy',
            'components': {},
            'issues': []
        }
        
        # ê° ì»´í¬ë„ŒíŠ¸ ê±´ê°• ìƒíƒœ ì²´í¬
        components = [
            ('api_server', self._check_api_health),
            ('feature_store', self._check_feature_store_health),
            ('data_pipeline', self._check_data_pipeline_health),
            ('model_serving', self._check_model_serving_health)
        ]
        
        unhealthy_count = 0
        
        for component_name, check_func in components:
            try:
                component_health = check_func()
                health_status['components'][component_name] = component_health
                
                if component_health['status'] != 'healthy':
                    unhealthy_count += 1
                    health_status['issues'].extend(component_health.get('issues', []))
                    
            except Exception as e:
                health_status['components'][component_name] = {
                    'status': 'error',
                    'error': str(e)
                }
                unhealthy_count += 1
        
        # ì „ì²´ ê±´ê°• ìƒíƒœ ê²°ì •
        if unhealthy_count == 0:
            health_status['overall_health'] = 'healthy'
        elif unhealthy_count <= 2:
            health_status['overall_health'] = 'degraded'
        else:
            health_status['overall_health'] = 'unhealthy'
        
        # ì‹¬ê°í•œ ìƒíƒœì¼ ë•Œ ì•Œë¦¼
        if health_status['overall_health'] == 'unhealthy':
            self._send_health_alert(health_status)
        
        return health_status
    
    def _check_api_health(self) -> Dict[str, Any]:
        """API ì„œë²„ ê±´ê°• ìƒíƒœ ì²´í¬"""
        import requests
        
        try:
            # API ì‘ë‹µ ì‹œê°„ ì²´í¬
            start_time = time.time()
            response = requests.get('http://localhost:8001/health', timeout=5)
            response_time = time.time() - start_time
            
            status = 'healthy'
            issues = []
            
            if response.status_code != 200:
                status = 'unhealthy'
                issues.append(f"API ìƒíƒœ ì½”ë“œ: {response.status_code}")
            
            if response_time > self.health_thresholds['api_response_time']:
                status = 'degraded'
                issues.append(f"ì‘ë‹µ ì‹œê°„ ì´ˆê³¼: {response_time:.2f}ì´ˆ")
            
            return {
                'status': status,
                'response_time': response_time,
                'status_code': response.status_code,
                'issues': issues
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'issues': ['API ì„œë²„ ì—°ê²° ì‹¤íŒ¨']
            }
    
    def _check_feature_store_health(self) -> Dict[str, Any]:
        """í”¼ì²˜ ìŠ¤í† ì–´ ê±´ê°• ìƒíƒœ ì²´í¬"""
        from src.features.store.feature_store import SimpleFeatureStore
        
        try:
            store = SimpleFeatureStore()
            stats = store.get_store_stats()
            
            status = 'healthy'
            issues = []
            
            # ìºì‹œ ì ì¤‘ë¥  ì²´í¬
            cache_stats = stats.get('cache_stats', {})
            hit_rate = cache_stats.get('hit_ratio', 0)
            
            if hit_rate < self.health_thresholds['cache_hit_rate']:
                status = 'degraded'
                issues.append(f"ìºì‹œ ì ì¤‘ë¥  ì €í•˜: {hit_rate:.2f}")
            
            return {
                'status': status,
                'cache_hit_rate': hit_rate,
                'total_features': stats.get('total_features', 0),
                'issues': issues
            }
            
        except Exception as e:
            return {
                'status': 'unhealthy',
                'error': str(e),
                'issues': ['í”¼ì²˜ ìŠ¤í† ì–´ ì—°ê²° ì‹¤íŒ¨']
            }
    
    def _send_health_alert(self, health_status: Dict[str, Any]):
        """ê±´ê°• ìƒíƒœ ì•Œë¦¼ ì „ì†¡"""
        message = f"""
        ğŸš¨ ì‹œìŠ¤í…œ ê±´ê°• ìƒíƒœ ê²½ê³ !
        
        ì „ì²´ ìƒíƒœ: {health_status['overall_health']}
        ê°ì§€ ì‹œê°„: {health_status['timestamp']}
        
        ë¬¸ì œì :
        {chr(10).join(health_status['issues'])}
        
        ì¦‰ì‹œ ì ê²€ì´ í•„ìš”í•©ë‹ˆë‹¤.
        """
        
        self.logger.critical(message)
```

---

## ğŸ”§ Docker Compose í†µí•© ì„¤ì •

### ëª¨ë‹ˆí„°ë§ ìŠ¤íƒ ë°°í¬

```yaml
# docker-compose.monitoring.yml
version: '3.8'

services:
  # Prometheus
  prometheus:
    image: prom/prometheus:v2.40.0
    container_name: mlops-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus:/etc/prometheus
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - mlops-network

  # Grafana
  grafana:
    image: grafana/grafana:9.3.0
    container_name: mlops-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources
    networks:
      - mlops-network
    depends_on:
      - prometheus

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.0
    container_name: mlops-elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - mlops-network

  # Logstash
  logstash:
    image: docker.elastic.co/logstash/logstash:8.5.0
    container_name: mlops-logstash
    ports:
      - "5044:5044"
      - "9600:9600"
    volumes:
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline
      - ./config/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml
      - /var/log/mlops:/var/log/mlops
    networks:
      - mlops-network
    depends_on:
      - elasticsearch

  # Kibana
  kibana:
    image: docker.elastic.co/kibana/kibana:8.5.0
    container_name: mlops-kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - mlops-network
    depends_on:
      - elasticsearch

  # Alertmanager
  alertmanager:
    image: prom/alertmanager:v0.25.0
    container_name: mlops-alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./config/alertmanager:/etc/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
    networks:
      - mlops-network

  # Node Exporter (ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­)
  node-exporter:
    image: prom/node-exporter:v1.5.0
    container_name: mlops-node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - mlops-network

networks:
  mlops-network:
    driver: bridge

volumes:
  prometheus_data:
  grafana_data:
  elasticsearch_data:
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] Prometheus ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ì •ìƒ ë™ì‘ í™•ì¸
- [ ] Grafana ëŒ€ì‹œë³´ë“œ êµ¬ì¶• ë° ì‹œê°í™” ì™„ë£Œ
- [ ] ELK Stack ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„ êµ¬í˜„
- [ ] ì§€ëŠ¥í˜• ì•Œë¦¼ ì‹œìŠ¤í…œ ë™ì‘ í™•ì¸
- [ ] ì´ìƒ íƒì§€ ì‹œìŠ¤í…œ êµ¬í˜„ ë° í…ŒìŠ¤íŠ¸

### ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ê°„ê²© 5ë¶„ ì´í•˜
- [ ] ì•Œë¦¼ ì „ì†¡ ì‹œê°„ 1ë¶„ ì´ë‚´
- [ ] ëŒ€ì‹œë³´ë“œ ì‘ë‹µ ì‹œê°„ 3ì´ˆ ì´í•˜
- [ ] ë¡œê·¸ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™”
- [ ] ì´ìƒ íƒì§€ ì •í™•ë„ 90% ì´ìƒ

### ìš´ì˜ ì™„ë£Œ ê¸°ì¤€
- [ ] 24/7 ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ìš´ì˜
- [ ] ì¥ì•  ì˜ˆì¸¡ ë° ì‚¬ì „ ëŒ€ì‘ ì²´ê³„ êµ¬ì¶•
- [ ] ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµìœ¡ ì™„ë£Œ
- [ ] ìš´ì˜ ê°€ì´ë“œ ë° ë¬¸ì„œ ì‘ì„±
- [ ] ë°±ì—… ë° ë³µêµ¬ ì ˆì°¨ ê²€ì¦

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì™„ë£Œ í›„ [2.10 MLflow í†µí•© ê°•í™”](./2.10-mlflow-integration-enhancement.md)ë¡œ ì§„í–‰í•˜ì—¬ ì‹¤í—˜ ì¶”ì ê³¼ ëª¨ë¸ ê´€ë¦¬ë¥¼ ê³ ë„í™”í•©ë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Prometheus Monitoring Guide](https://prometheus.io/docs/introduction/overview/)
- [Grafana Dashboard Best Practices](https://grafana.com/docs/)
- [ELK Stack Configuration](https://www.elastic.co/guide/)
- [MLOps Monitoring Patterns](https://ml-ops.org/content/monitoring-ml-systems)
