---
title: "2.2 í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê°€ì´ë“œ"
description: "ìžë™í™”ëœ í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¼ê´€ì„±ê³¼ ìž¬í˜„ì„± í™•ë³´"
author: "MLOps Team"
created: "2025-06-05"
updated: "2025-06-05"
version: "1.0"
stage: "2.2"
category: "Feature Pipeline"
tags: ["íŒŒì´í”„ë¼ì¸", "ìžë™í™”", "ë³‘ë ¬ì²˜ë¦¬", "YAMLì„¤ì •", "ì¦ë¶„ì²˜ë¦¬"]
prerequisites: ["2.1 í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¡œì§ ì™„ë£Œ", "Python ë©€í‹°í”„ë¡œì„¸ì‹±", "YAML"]
difficulty: "intermediate"
estimated_time: "6-8ì‹œê°„"
---

# 2.2 í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê°€ì´ë“œ

## ðŸ“‹ ê°œìš”

**ëª©í‘œ**: ìžë™í™”ëœ í”¼ì²˜ ìƒì„±ìœ¼ë¡œ ì¼ê´€ì„±ê³¼ ìž¬í˜„ì„± í™•ë³´

**í•µì‹¬ ê°€ì¹˜**: ìˆ˜ë™ ìž‘ì—… ì—†ì´ ì‹ ë¢°í•  ìˆ˜ ìžˆëŠ” í”¼ì²˜ë¥¼ ì§€ì†ì ìœ¼ë¡œ ìƒì„±

---

## ðŸŽ¯ í•™ìŠµ ëª©í‘œ

### ì´ë¡ ì  ëª©í‘œ
- íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì›ì¹™ê³¼ íŒ¨í„´ ì´í•´
- ETL/ELT í”„ë¡œì„¸ìŠ¤ì™€ í”¼ì²˜ íŒŒì´í”„ë¼ì¸ì˜ ì°¨ì´ì  í•™ìŠµ
- ë³‘ë ¬ ì²˜ë¦¬ì™€ ìµœì í™” ì „ëžµ ìŠµë“

### ì‹¤ë¬´ì  ëª©í‘œ
- ë‹¨ê³„ë³„ í”¼ì²˜ íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- YAML ê¸°ë°˜ ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- ì¦ë¶„ ì²˜ë¦¬ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°œë°œ

---

## ðŸ”§ 2.2.1 íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### í•µì‹¬ ì„¤ê³„ ì›ì¹™

#### **ë‹¨ê³„ë³„ ë¶„ë¦¬ (Separation of Concerns)**
```python
from abc import ABC, abstractmethod
from typing import Any, Dict, List
import logging
import time

class PipelineStage(ABC):
    """íŒŒì´í”„ë¼ì¸ ìŠ¤í…Œì´ì§€ ë² ì´ìŠ¤ í´ëž˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.metrics = {}
    
    @abstractmethod
    def process(self, data: Any) -> Any:
        """ë°ì´í„° ì²˜ë¦¬ ë¡œì§"""
        pass
    
    def validate_input(self, data: Any) -> bool:
        """ìž…ë ¥ ë°ì´í„° ê²€ì¦"""
        return True
    
    def validate_output(self, data: Any) -> bool:
        """ì¶œë ¥ ë°ì´í„° ê²€ì¦"""
        return True
    
    def get_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        return self.metrics

class FeaturePipeline:
    """í”¼ì²˜ ìƒì„± ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(self.__class__.__name__)
        self.stages = self._initialize_stages()
        self.metrics_collector = MetricsCollector()
    
    def run(self, input_data: Any) -> Any:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        start_time = time.time()
        current_data = input_data
        
        self.logger.info(f"íŒŒì´í”„ë¼ì¸ ì‹œìž‘: {len(self.stages)}ê°œ ìŠ¤í…Œì´ì§€")
        
        try:
            for i, stage in enumerate(self.stages):
                stage_start = time.time()
                
                self.logger.info(f"ìŠ¤í…Œì´ì§€ {i+1}/{len(self.stages)} ì‹œìž‘: {stage.__class__.__name__}")
                
                # ìž…ë ¥ ê²€ì¦
                if not stage.validate_input(current_data):
                    raise ValueError(f"ìŠ¤í…Œì´ì§€ {stage.__class__.__name__} ìž…ë ¥ ê²€ì¦ ì‹¤íŒ¨")
                
                # ë°ì´í„° ì²˜ë¦¬
                current_data = stage.process(current_data)
                
                # ì¶œë ¥ ê²€ì¦
                if not stage.validate_output(current_data):
                    raise ValueError(f"ìŠ¤í…Œì´ì§€ {stage.__class__.__name__} ì¶œë ¥ ê²€ì¦ ì‹¤íŒ¨")
                
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                stage_metrics = stage.get_metrics()
                stage_metrics['stage_name'] = stage.__class__.__name__
                stage_metrics['stage_duration'] = time.time() - stage_start
                
                self.metrics_collector.add_stage_metrics(stage_metrics)
                
                self.logger.info(
                    f"ìŠ¤í…Œì´ì§€ {i+1} ì™„ë£Œ: {stage_metrics.get('processing_time', 0):.2f}ì´ˆ"
                )
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­
            total_duration = time.time() - start_time
            self.metrics_collector.set_pipeline_metrics({
                'total_duration': total_duration,
                'total_stages': len(self.stages),
                'status': 'success'
            })
            
            self.logger.info(f"íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {total_duration:.2f}ì´ˆ")
            
            return current_data
            
        except Exception as e:
            self.logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.metrics_collector.set_pipeline_metrics({
                'total_duration': time.time() - start_time,
                'status': 'failed',
                'error': str(e)
            })
            raise
```

---

## ðŸ”§ 2.2.2 YAML ê¸°ë°˜ êµ¬ì„± ê´€ë¦¬

### íŒŒì´í”„ë¼ì¸ ì„¤ì • ìŠ¤í‚¤ë§ˆ

#### **ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì„¤ì •**
```yaml
# config/feature_pipeline.yaml

feature_pipeline:
  name: "movie_recommendation_features"
  version: "1.0"
  description: "ì˜í™” ì¶”ì²œì„ ìœ„í•œ í”¼ì²˜ ìƒì„± íŒŒì´í”„ë¼ì¸"
  
  # ìž…ë ¥ ì„¤ì •
  input:
    source_type: "json"
    source_path: "data/raw/movies"
    file_pattern: "*.json"
    encoding: "utf-8"
  
  # ì¶œë ¥ ì„¤ì •
  output:
    destination_type: "parquet"
    destination_path: "data/features"
    compression: "snappy"
    partition_by: ["release_year"]
  
  # ì²˜ë¦¬ ì„¤ì •
  processing:
    parallel_processing: true
    max_workers: 4
    chunk_size: 1000
    memory_limit: "4GB"
  
  # ìŠ¤í…Œì´ì§€ ì •ì˜
  stages:
    - name: "data_validation"
      type: "DataValidationStage"
      params:
        required_fields: ["id", "title", "vote_average", "popularity"]
        type_checks:
          id: "int"
          vote_average: "float"
          popularity: "float"
        range_checks:
          vote_average: [0, 10]
          popularity: [0, 1000]
    
    - name: "content_feature_extraction"
      type: "FeatureExtractionStage"
      params:
        extractors:
          - name: "genre_features"
            type: "GenreFeatureExtractor"
            params:
              encoding_type: "one_hot"
              max_genres: 20
          
          - name: "temporal_features"
            type: "TemporalFeatureExtractor"
            params:
              include_era: true
              include_decade: true
              include_age: true

# ì‹¤í—˜ ì„¤ì •
experiments:
  baseline:
    feature_pipeline:
      stages:
        # ê¸°ë³¸ í”¼ì²˜ë§Œ ì‚¬ìš©
        - "data_validation"
        - "content_feature_extraction"
        - "feature_storage"
  
  advanced:
    feature_pipeline:
      stages:
        # ëª¨ë“  í”¼ì²˜ ì‚¬ìš©
        - "data_validation"
        - "content_feature_extraction"
        - "interaction_feature_generation"
        - "feature_validation"
        - "feature_storage"
```

### ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ

```python
import yaml
from typing import Dict, Any
import os

class ConfigManager:
    """YAML ê¸°ë°˜ ì„¤ì • ê´€ë¦¬ìž"""
    
    def __init__(self, base_config_path: str, environment: str = "development"):
        self.base_config_path = base_config_path
        self.environment = environment
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ ë° ë³‘í•©"""
        
        # ê¸°ë³¸ ì„¤ì • ë¡œë“œ
        with open(self.base_config_path, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        
        # í™˜ê²½ë³„ ì„¤ì • ë¡œë“œ ë° ë³‘í•©
        env_config_path = f"config/{self.environment}.yaml"
        if os.path.exists(env_config_path):
            with open(env_config_path, 'r', encoding='utf-8') as f:
                env_config = yaml.safe_load(f)
            
            # ë”¥ ë³‘í•©
            merged_config = self._deep_merge(base_config, env_config)
        else:
            merged_config = base_config
        
        # í™˜ê²½ ë³€ìˆ˜ë¡œ ì˜¤ë²„ë¼ì´ë“œ
        return self._apply_env_overrides(merged_config)
    
    def get_pipeline_config(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ì„¤ì • ë°˜í™˜"""
        return self.config.get('feature_pipeline', {})
```

---

## ðŸ”§ 2.2.3 ì¦ë¶„ ì²˜ë¦¬ ì‹œìŠ¤í…œ

### ë³€ê²½ ë¶„ ê°ì§€

#### **ë°ì´í„° ë³€ê²½ ì¶”ì ê¸°**
```python
import hashlib
import json
from datetime import datetime
from pathlib import Path

class DataChangeTracker:
    """ë°ì´í„° ë³€ê²½ ì‚¬í•­ ì¶”ì """
    
    def __init__(self, tracking_file: str = "data/.change_tracking.json"):
        self.tracking_file = Path(tracking_file)
        self.tracking_data = self._load_tracking_data()
    
    def has_changed(self, file_path: str) -> bool:
        """íŒŒì¼ ë³€ê²½ ì—¬ë¶€ í™•ì¸"""
        file_path = str(Path(file_path).resolve())
        
        if not Path(file_path).exists():
            return False
        
        current_signature = self.calculate_file_signature(file_path)
        stored_signature = self.tracking_data.get(file_path, {}).get('signature')
        
        return current_signature != stored_signature
    
    def get_changed_files(self, file_pattern: str) -> List[str]:
        """ë³€ê²½ëœ íŒŒì¼ ëª©ë¡ ë°˜í™˜"""
        from glob import glob
        
        changed_files = []
        for file_path in glob(file_pattern):
            if self.has_changed(file_path):
                changed_files.append(file_path)
        
        return changed_files

class IncrementalFeaturePipeline:
    """ì¦ë¶„ ì²˜ë¦¬ ì§€ì› íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.change_tracker = DataChangeTracker()
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def run_incremental(self, force_full_rebuild: bool = False) -> Dict[str, Any]:
        """ì¦ë¶„ ì²˜ë¦¬ ì‹¤í–‰"""
        
        if force_full_rebuild:
            return self._run_full_pipeline()
        
        # ë³€ê²½ëœ ìž…ë ¥ íŒŒì¼ ê°ì§€
        input_pattern = self.config.get('input', {}).get('file_pattern', '*.json')
        changed_files = self.change_tracker.get_changed_files(input_pattern)
        
        if not changed_files:
            self.logger.info("ë³€ê²½ëœ íŒŒì¼ ì—†ìŒ - ìŠ¤í‚µ")
            return {'status': 'skipped', 'reason': 'no_changes'}
        
        self.logger.info(f"ë³€ê²½ëœ íŒŒì¼ {len(changed_files)}ê°œ ê°ì§€")
        
        # ë³€ê²½ëœ íŒŒì¼ë§Œ ì²˜ë¦¬
        result = self._process_changed_files(changed_files)
        
        # ë³€ê²½ ì¶”ì  ì—…ë°ì´íŠ¸
        for file_path in changed_files:
            self.change_tracker.mark_processed(file_path)
        
        return result
```

---

## ðŸ”§ 2.2.4 ë³‘ë ¬ ì²˜ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”

### ë©€í‹°í”„ë¡œì„¸ì‹± êµ¬í˜„

```python
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp

class ParallelFeatureProcessor:
    """ë³‘ë ¬ í”¼ì²˜ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.max_workers = config.get('max_workers', mp.cpu_count())
        self.chunk_size = config.get('chunk_size', 1000)
        
    def process_parallel(self, data: List[Dict], processor_func: callable) -> List[Dict]:
        """ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰"""
        
        if len(data) < self.chunk_size:
            # ìž‘ì€ ë°ì´í„°ëŠ” ìˆœì°¨ ì²˜ë¦¬
            return processor_func(data)
        
        # ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        chunks = [
            data[i:i + self.chunk_size] 
            for i in range(0, len(data), self.chunk_size)
        ]
        
        results = []
        
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ê° ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
            future_to_chunk = {
                executor.submit(processor_func, chunk): chunk 
                for chunk in chunks
            }
            
            for future in as_completed(future_to_chunk):
                try:
                    chunk_result = future.result()
                    results.extend(chunk_result)
                except Exception as e:
                    self.logger.error(f"ì²­í¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ìˆœì°¨ ì²˜ë¦¬ë¡œ ìž¬ì‹œë„
                    chunk = future_to_chunk[future]
                    fallback_result = processor_func(chunk)
                    results.extend(fallback_result)
        
        return results
```

### ë©”ëª¨ë¦¬ ìµœì í™”

```python
class MemoryEfficientProcessor:
    """ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ê¸°"""
    
    def __init__(self, memory_limit: str = "4GB"):
        self.memory_limit = self._parse_memory_limit(memory_limit)
        
    def process_with_memory_control(self, data_iterator, processor_func):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì œì–´í•˜ë©° ì²˜ë¦¬"""
        
        import psutil
        import gc
        
        results = []
        batch = []
        batch_size = 100
        
        for item in data_iterator:
            batch.append(item)
            
            if len(batch) >= batch_size:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                memory_usage = psutil.virtual_memory().percent
                
                if memory_usage > 80:  # 80% ì´ˆê³¼ ì‹œ
                    # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
                    gc.collect()
                    
                    # ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
                    batch_size = max(10, batch_size // 2)
                
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_result = processor_func(batch)
                results.extend(batch_result)
                
                batch = []
        
        # ë‚¨ì€ ë°ì´í„° ì²˜ë¦¬
        if batch:
            batch_result = processor_func(batch)
            results.extend(batch_result)
        
        return results
```

---

## ðŸ”§ 2.2.5 ëª¨ë‹ˆí„°ë§ ë° ì§„í–‰ë¥  ì¶”ì 

### ì‹¤ì‹œê°„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§

```python
from tqdm import tqdm
import time

class ProgressMonitor:
    """ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, total_items: int, description: str = "Processing"):
        self.total_items = total_items
        self.description = description
        self.progress_bar = tqdm(total=total_items, desc=description)
        self.start_time = time.time()
        
    def update(self, count: int = 1):
        """ì§„í–‰ë¥  ì—…ë°ì´íŠ¸"""
        self.progress_bar.update(count)
        
    def set_postfix(self, **kwargs):
        """ì¶”ê°€ ì •ë³´ í‘œì‹œ"""
        self.progress_bar.set_postfix(**kwargs)
        
    def close(self):
        """ì§„í–‰ë¥  ë°” ì¢…ë£Œ"""
        self.progress_bar.close()
        
        elapsed_time = time.time() - self.start_time
        print(f"\nâœ… {self.description} ì™„ë£Œ: {elapsed_time:.2f}ì´ˆ")

class MetricsCollector:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.stage_metrics = []
        self.pipeline_metrics = {}
        
    def add_stage_metrics(self, metrics: Dict[str, Any]):
        """ìŠ¤í…Œì´ì§€ ë©”íŠ¸ë¦­ ì¶”ê°€"""
        metrics['timestamp'] = datetime.now().isoformat()
        self.stage_metrics.append(metrics)
        
    def set_pipeline_metrics(self, metrics: Dict[str, Any]):
        """íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ì„¤ì •"""
        self.pipeline_metrics = metrics
        self.pipeline_metrics['timestamp'] = datetime.now().isoformat()
        
    def get_summary(self) -> Dict[str, Any]:
        """ë©”íŠ¸ë¦­ ìš”ì•½ ë°˜í™˜"""
        return {
            'pipeline': self.pipeline_metrics,
            'stages': self.stage_metrics,
            'total_stages': len(self.stage_metrics),
            'total_processing_time': sum(
                stage.get('processing_time', 0) 
                for stage in self.stage_metrics
            )
        }
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹

```python
class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.benchmarks = {}
        
    def benchmark_stage(self, stage_name: str, data_size: int, 
                       processing_time: float) -> Dict[str, float]:
        """ìŠ¤í…Œì´ì§€ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
        
        throughput = data_size / processing_time if processing_time > 0 else 0
        
        benchmark = {
            'data_size': data_size,
            'processing_time': processing_time,
            'throughput': throughput,  # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰
            'avg_time_per_item': processing_time / data_size if data_size > 0 else 0
        }
        
        self.benchmarks[stage_name] = benchmark
        return benchmark
        
    def compare_with_baseline(self, baseline_file: str) -> Dict[str, Any]:
        """ê¸°ì¤€ì„ ê³¼ ë¹„êµ"""
        
        if not os.path.exists(baseline_file):
            # ê¸°ì¤€ì„  íŒŒì¼ ìƒì„±
            with open(baseline_file, 'w') as f:
                json.dump(self.benchmarks, f, indent=2)
            return {'status': 'baseline_created'}
        
        # ê¸°ì¤€ì„  ë¡œë“œ
        with open(baseline_file, 'r') as f:
            baseline = json.load(f)
        
        comparison = {}
        for stage_name, current in self.benchmarks.items():
            if stage_name in baseline:
                baseline_throughput = baseline[stage_name]['throughput']
                current_throughput = current['throughput']
                
                improvement = (
                    (current_throughput - baseline_throughput) / baseline_throughput 
                    if baseline_throughput > 0 else 0
                )
                
                comparison[stage_name] = {
                    'current_throughput': current_throughput,
                    'baseline_throughput': baseline_throughput,
                    'improvement_ratio': improvement,
                    'status': 'improved' if improvement > 0.1 else 
                             'degraded' if improvement < -0.1 else 'stable'
                }
        
        return comparison
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ë‹¨ê³„ë³„ íŒŒì´í”„ë¼ì¸ êµ¬ì¡° êµ¬í˜„ ì™„ë£Œ
- [ ] YAML ê¸°ë°˜ ì„¤ì • ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•
- [ ] ë³‘ë ¬ ì²˜ë¦¬ ì§€ì› (4ê°œ ì´ìƒ ì›Œì»¤)
- [ ] ì¦ë¶„ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ì§„í–‰ë¥  ëª¨ë‹ˆí„°ë§ êµ¬í˜„

### ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 80% ì´í•˜ ìœ ì§€
- [ ] ì²˜ë¦¬ ì†ë„ ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ 2ë°° ì´ìƒ ê°œì„ 
- [ ] ì—ëŸ¬ ë°œìƒ ì‹œ ìžë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- [ ] ì„¤ì • ë³€ê²½ ì‹œ ìž¬ì‹œìž‘ ì—†ì´ ì ìš©
- [ ] ì˜ì¡´ì„± ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ìž¬ê³„ì‚°

### í’ˆì§ˆ ì™„ë£Œ ê¸°ì¤€
- [ ] ëª¨ë“  ìŠ¤í…Œì´ì§€ì— ëŒ€í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ìž‘ì„±
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìˆ˜ë¦½ ë° ê¸°ì¤€ì„  ì„¤ì •
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹… ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] íŒŒì´í”„ë¼ì¸ ë¬¸ì„œí™” ì™„ë£Œ
- [ ] ì„¤ì • ìŠ¤í‚¤ë§ˆ ê²€ì¦ êµ¬í˜„

---

## ðŸš€ ë‹¤ìŒ ë‹¨ê³„

ì™„ë£Œ í›„ [2.3 í”¼ì²˜ ê²€ì¦ ë° í…ŒìŠ¤íŠ¸](./2.3-feature-validation-testing-guide.md)ë¡œ ì§„í–‰í•˜ì—¬ ìƒì„±ëœ í”¼ì²˜ë“¤ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.

---

## ðŸ“š ì°¸ê³  ìžë£Œ

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Python Multiprocessing Guide](https://docs.python.org/3/library/multiprocessing.html)
- [YAML Best Practices](https://yaml.org/spec/1.2/spec.html)
- [Pipeline Design Patterns](https://martinfowler.com/articles/data-monolith-to-mesh.html)
