---
title: "2.5 ê°„ë‹¨í•œ í”¼ì²˜ ìŠ¤í† ì–´ êµ¬í˜„ ê°€ì´ë“œ"
description: "ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜ì˜ ê²½ëŸ‰ í”¼ì²˜ ìŠ¤í† ì–´ êµ¬ì¶•"
author: "MLOps Team"
created: "2025-06-05"
updated: "2025-06-05"
version: "1.0"
stage: "2.5"
category: "Feature Store"
tags: ["í”¼ì²˜ìŠ¤í† ì–´", "íŒŒì¼ì‹œìŠ¤í…œ", "APIì¸í„°í˜ì´ìŠ¤", "ìºì‹±", "ì„±ëŠ¥ìµœì í™”"]
prerequisites: ["2.4 ë©”íƒ€ë°ì´í„° ê´€ë¦¬ ì™„ë£Œ", "Parquet", "pandas", "API ì„¤ê³„"]
difficulty: "intermediate"
estimated_time: "6-8ì‹œê°„"
---

# 2.5 ê°„ë‹¨í•œ í”¼ì²˜ ìŠ¤í† ì–´ êµ¬í˜„ ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

**ëª©í‘œ**: ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œ ê¸°ë°˜ì˜ ê²½ëŸ‰ í”¼ì²˜ ìŠ¤í† ì–´ êµ¬ì¶•

**í•µì‹¬ ê°€ì¹˜**: ë³µì¡í•œ ì¸í”„ë¼ ì—†ì´ë„ íš¨ìœ¨ì ì¸ í”¼ì²˜ ê´€ë¦¬ì™€ ì„œë¹™ ì‹œìŠ¤í…œ ì œê³µ

---

## ğŸ¯ í•™ìŠµ ëª©í‘œ

### ì´ë¡ ì  ëª©í‘œ
- í”¼ì²˜ ìŠ¤í† ì–´ ì•„í‚¤í…ì²˜ íŒ¨í„´ ì´í•´
- ì˜¨ë¼ì¸/ì˜¤í”„ë¼ì¸ ì„œë¹™ì˜ ì°¨ì´ì  í•™ìŠµ
- ìºì‹± ì „ëµê³¼ ì„±ëŠ¥ ìµœì í™” ë°©ë²• ìŠµë“

### ì‹¤ë¬´ì  ëª©í‘œ
- íŒŒì¼ ê¸°ë°˜ í”¼ì²˜ ìŠ¤í† ì–´ ì„¤ê³„ ë° êµ¬í˜„
- RESTful API ì¸í„°í˜ì´ìŠ¤ ê°œë°œ
- ìºì‹± ì‹œìŠ¤í…œ ë° ì„±ëŠ¥ ìµœì í™” êµ¬í˜„

---

## ğŸ”§ 2.5.1 íŒŒì¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ì„¤ê³„

### ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
feature_store/
â”œâ”€â”€ features/                           # í”¼ì²˜ ë°ì´í„° ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ content/                        # ì»¨í…ì¸  ê´€ë ¨ í”¼ì²˜
â”‚   â”‚   â”œâ”€â”€ movie_genres.parquet        # ì˜í™” ì¥ë¥´ í”¼ì²˜
â”‚   â”‚   â”œâ”€â”€ movie_ratings.parquet       # ì˜í™” í‰ì  í”¼ì²˜
â”‚   â”‚   â””â”€â”€ movie_popularity.parquet    # ì˜í™” ì¸ê¸°ë„ í”¼ì²˜
â”‚   â”œâ”€â”€ user/                           # ì‚¬ìš©ì ê´€ë ¨ í”¼ì²˜
â”‚   â”‚   â”œâ”€â”€ user_preferences.parquet    # ì‚¬ìš©ì ì„ í˜¸ë„ í”¼ì²˜
â”‚   â”‚   â””â”€â”€ user_demographics.parquet   # ì‚¬ìš©ì ì¸êµ¬í†µê³„ í”¼ì²˜
â”‚   â””â”€â”€ interaction/                    # ìƒí˜¸ì‘ìš© í”¼ì²˜
â”‚       â””â”€â”€ user_movie_interactions.parquet
â”œâ”€â”€ metadata/                           # ë©”íƒ€ë°ì´í„° ì €ì¥ì†Œ
â”œâ”€â”€ registry/                           # í”¼ì²˜ ë ˆì§€ìŠ¤íŠ¸ë¦¬
â”œâ”€â”€ cache/                              # ìºì‹œ ì €ì¥ì†Œ
â””â”€â”€ config/                             # ì„¤ì • íŒŒì¼
```

### ìµœì í™”ëœ ìŠ¤í† ë¦¬ì§€ êµ¬í˜„

```python
import pandas as pd
import pyarrow.parquet as pq
from pathlib import Path
from typing import Dict, List, Any, Optional

class OptimizedStorage:
    """ìµœì í™”ëœ íŒŒì¼ ì €ì¥ ê´€ë¦¬ì"""
    
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.compression_config = {
            'engine': 'pyarrow',
            'compression': 'snappy',
            'index': False
        }
        
    def save_feature_data(self, data: pd.DataFrame, 
                         feature_group: str, 
                         feature_name: str,
                         partition_cols: Optional[List[str]] = None) -> str:
        """í”¼ì²˜ ë°ì´í„° ì €ì¥ (Parquet ìµœì í™”)"""
        
        # ì €ì¥ ê²½ë¡œ ìƒì„±
        storage_path = self.base_path / "features" / feature_group
        storage_path.mkdir(parents=True, exist_ok=True)
        
        file_path = storage_path / f"{feature_name}.parquet"
        
        # ë°ì´í„° íƒ€ì… ìµœì í™”
        optimized_data = self._optimize_dtypes(data)
        
        # ë°ì´í„° ì €ì¥
        optimized_data.to_parquet(file_path, **self.compression_config)
        
        return str(file_path)
    
    def load_feature_data(self, feature_group: str, 
                         feature_name: str,
                         columns: Optional[List[str]] = None,
                         filters: Optional[List] = None) -> pd.DataFrame:
        """í”¼ì²˜ ë°ì´í„° ë¡œë“œ (ì„ íƒì  ë¡œë”©)"""
        
        file_path = self.base_path / "features" / feature_group / f"{feature_name}.parquet"
        
        try:
            return pd.read_parquet(
                file_path,
                columns=columns,
                filters=filters,
                engine='pyarrow'
            )
        except FileNotFoundError:
            print(f"í”¼ì²˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {feature_name}")
            return pd.DataFrame()
    
    def _optimize_dtypes(self, df: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        
        optimized_df = df.copy()
        
        for col in optimized_df.columns:
            col_data = optimized_df[col]
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ìµœì í™”
            if pd.api.types.is_integer_dtype(col_data):
                col_min = col_data.min()
                col_max = col_data.max()
                
                if col_min >= 0:  # ìŒìˆ˜ê°€ ì—†ëŠ” ê²½ìš°
                    if col_max < 255:
                        optimized_df[col] = col_data.astype('uint8')
                    elif col_max < 65535:
                        optimized_df[col] = col_data.astype('uint16')
                else:  # ìŒìˆ˜ê°€ ìˆëŠ” ê²½ìš°
                    if col_min > -128 and col_max < 127:
                        optimized_df[col] = col_data.astype('int8')
                    elif col_min > -32768 and col_max < 32767:
                        optimized_df[col] = col_data.astype('int16')
            
            elif pd.api.types.is_float_dtype(col_data):
                # ë¶€ë™ì†Œìˆ˜ì  ìµœì í™”
                if col_data.between(-3.4e38, 3.4e38).all():
                    optimized_df[col] = col_data.astype('float32')
            
            elif pd.api.types.is_object_dtype(col_data):
                # ë¬¸ìì—´ ìµœì í™”
                unique_count = col_data.nunique()
                total_count = len(col_data)
                
                # ì¹´í…Œê³ ë¦¬í˜•ìœ¼ë¡œ ë³€í™˜í• ì§€ ê²°ì •
                if unique_count / total_count < 0.5:
                    optimized_df[col] = col_data.astype('category')
        
        return optimized_df
    
    def get_storage_info(self, feature_group: str, 
                        feature_name: str) -> Dict[str, Any]:
        """ìŠ¤í† ë¦¬ì§€ ì •ë³´ ì¡°íšŒ"""
        
        file_path = self.base_path / "features" / feature_group / f"{feature_name}.parquet"
        
        info = {
            'feature_group': feature_group,
            'feature_name': feature_name,
            'exists': file_path.exists(),
            'file_size_mb': 0,
            'row_count': 0,
            'column_count': 0
        }
        
        if file_path.exists():
            info['file_size_mb'] = file_path.stat().st_size / 1024 / 1024
            
            try:
                parquet_file = pq.ParquetFile(file_path)
                info['row_count'] = parquet_file.metadata.num_rows
                info['column_count'] = len(parquet_file.schema)
            except Exception as e:
                info['error'] = str(e)
        
        return info
```

---

## ğŸ”§ 2.5.2 API ì¸í„°í˜ì´ìŠ¤ ì„¤ê³„

### FastAPI ê¸°ë°˜ RESTful API

```python
from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime

class FeatureRequest(BaseModel):
    """í”¼ì²˜ ìš”ì²­ ëª¨ë¸"""
    feature_names: List[str]
    entity_ids: Optional[List[str]] = None
    timestamp: Optional[str] = None

class SimpleFeatureStoreAPI:
    """ê°„ë‹¨í•œ í”¼ì²˜ ìŠ¤í† ì–´ API"""
    
    def __init__(self, storage: OptimizedStorage):
        self.storage = storage
        self.app = FastAPI(title="Simple Feature Store API", version="1.0.0")
        self._setup_routes()
    
    def _setup_routes(self):
        """API ë¼ìš°íŠ¸ ì„¤ì •"""
        
        @self.app.get("/health")
        async def health_check():
            """í—¬ìŠ¤ ì²´í¬"""
            return {"status": "healthy", "timestamp": datetime.now().isoformat()}
        
        @self.app.get("/features/{feature_group}/{feature_name}")
        async def get_feature_data(
            feature_group: str,
            feature_name: str,
            columns: Optional[str] = Query(None, description="Comma-separated column names"),
            limit: Optional[int] = Query(None, description="Limit number of rows"),
            offset: Optional[int] = Query(0, description="Offset for pagination")
        ):
            """í”¼ì²˜ ë°ì´í„° ì¡°íšŒ"""
            
            try:
                # ì»¬ëŸ¼ íŒŒì‹±
                selected_columns = None
                if columns:
                    selected_columns = [col.strip() for col in columns.split(',')]
                
                # ë°ì´í„° ë¡œë“œ
                data = self.storage.load_feature_data(
                    feature_group=feature_group,
                    feature_name=feature_name,
                    columns=selected_columns
                )
                
                if data.empty:
                    raise HTTPException(status_code=404, detail="Feature data not found")
                
                # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
                if limit:
                    data = data.iloc[offset:offset+limit]
                
                # JSON ë³€í™˜
                result = {
                    "feature_group": feature_group,
                    "feature_name": feature_name,
                    "data": data.to_dict('records'),
                    "metadata": {
                        "total_rows": len(data),
                        "columns": list(data.columns),
                        "dtypes": {col: str(dtype) for col, dtype in data.dtypes.items()}
                    }
                }
                
                return result
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/features/{feature_group}/{feature_name}")
        async def save_feature_data(
            feature_group: str,
            feature_name: str,
            data: Dict[str, Any]
        ):
            """í”¼ì²˜ ë°ì´í„° ì €ì¥"""
            
            try:
                # DataFrame ë³€í™˜
                df = pd.DataFrame(data['records'])
                
                # ë°ì´í„° ì €ì¥
                file_path = self.storage.save_feature_data(
                    data=df,
                    feature_group=feature_group,
                    feature_name=feature_name
                )
                
                return {
                    "message": "Feature data saved successfully",
                    "file_path": file_path,
                    "rows_saved": len(df)
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.get("/features/{feature_group}/{feature_name}/info")
        async def get_feature_info(feature_group: str, feature_name: str):
            """í”¼ì²˜ ì •ë³´ ì¡°íšŒ"""
            
            try:
                storage_info = self.storage.get_storage_info(feature_group, feature_name)
                return {"storage_info": storage_info}
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
        
        @self.app.post("/features/batch")
        async def get_batch_features(request: FeatureRequest):
            """ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ"""
            
            try:
                result = {}
                
                for feature_name in request.feature_names:
                    # í”¼ì²˜ ê·¸ë£¹ ì¶”ì •
                    feature_group = self._infer_feature_group(feature_name)
                    
                    data = self.storage.load_feature_data(
                        feature_group=feature_group,
                        feature_name=feature_name
                    )
                    
                    # ì—”í‹°í‹° ID í•„í„°ë§
                    if request.entity_ids and 'entity_id' in data.columns:
                        data = data[data['entity_id'].isin(request.entity_ids)]
                    
                    result[feature_name] = data.to_dict('records')
                
                return {
                    "features": result,
                    "metadata": {
                        "request_timestamp": request.timestamp or datetime.now().isoformat(),
                        "feature_count": len(request.feature_names)
                    },
                    "timestamp": datetime.now().isoformat()
                }
                
            except Exception as e:
                raise HTTPException(status_code=500, detail=str(e))
    
    def _infer_feature_group(self, feature_name: str) -> str:
        """í”¼ì²˜ ì´ë¦„ì—ì„œ ê·¸ë£¹ ì¶”ì •"""
        
        if 'user' in feature_name.lower():
            return 'user'
        elif 'movie' in feature_name.lower() or 'content' in feature_name.lower():
            return 'content'
        elif 'interaction' in feature_name.lower():
            return 'interaction'
        else:
            return 'misc'
    
    def run(self, host: str = "0.0.0.0", port: int = 8001):
        """API ì„œë²„ ì‹¤í–‰"""
        uvicorn.run(self.app, host=host, port=port)
```

### í´ë¼ì´ì–¸íŠ¸ SDK

```python
import requests
from typing import List, Dict, Any, Optional

class FeatureStoreClient:
    """í”¼ì²˜ ìŠ¤í† ì–´ í´ë¼ì´ì–¸íŠ¸ SDK"""
    
    def __init__(self, base_url: str = "http://localhost:8001"):
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
    
    def get_feature(self, feature_group: str, feature_name: str,
                   columns: Optional[List[str]] = None,
                   limit: Optional[int] = None,
                   offset: int = 0) -> Dict[str, Any]:
        """ë‹¨ì¼ í”¼ì²˜ ë°ì´í„° ì¡°íšŒ"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}"
        
        params = {'offset': offset}
        if columns:
            params['columns'] = ','.join(columns)
        if limit:
            params['limit'] = limit
        
        response = self.session.get(url, params=params)
        response.raise_for_status()
        
        return response.json()
    
    def save_feature(self, feature_group: str, feature_name: str,
                    data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """í”¼ì²˜ ë°ì´í„° ì €ì¥"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}"
        
        payload = {'records': data}
        
        response = self.session.post(url, json=payload)
        response.raise_for_status()
        
        return response.json()
    
    def get_batch_features(self, feature_names: List[str],
                          entity_ids: Optional[List[str]] = None) -> Dict[str, Any]:
        """ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ"""
        
        url = f"{self.base_url}/features/batch"
        
        payload = {
            'feature_names': feature_names,
            'entity_ids': entity_ids
        }
        
        response = self.session.post(url, json=payload)
        response.raise_for_status()
        
        return response.json()
    
    def get_feature_info(self, feature_group: str, feature_name: str) -> Dict[str, Any]:
        """í”¼ì²˜ ì •ë³´ ì¡°íšŒ"""
        
        url = f"{self.base_url}/features/{feature_group}/{feature_name}/info"
        
        response = self.session.get(url)
        response.raise_for_status()
        
        return response.json()
```

---

## ğŸ”§ 2.5.3 ìºì‹± ì „ëµ

### ë©”ëª¨ë¦¬ ìºì‹œ êµ¬í˜„

```python
import time
from typing import Any, Optional
from collections import OrderedDict

class LRUCache:
    """LRU (Least Recently Used) ìºì‹œ"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.timestamps = {}
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        
        if key not in self.cache:
            return None
        
        # TTL í™•ì¸
        if time.time() - self.timestamps[key] > self.ttl_seconds:
            self.delete(key)
            return None
        
        # LRU ì—…ë°ì´íŠ¸ (ê°€ì¥ ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™)
        self.cache.move_to_end(key)
        return self.cache[key]
    
    def set(self, key: str, value: Any):
        """ìºì‹œì— ê°’ ì €ì¥"""
        
        if key in self.cache:
            # ê¸°ì¡´ ê°’ ì—…ë°ì´íŠ¸
            self.cache.move_to_end(key)
        else:
            # ìƒˆ ê°’ ì¶”ê°€
            if len(self.cache) >= self.max_size:
                # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = next(iter(self.cache))
                self.delete(oldest_key)
        
        self.cache[key] = value
        self.timestamps[key] = time.time()
    
    def delete(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ì œê±°"""
        
        if key in self.cache:
            del self.cache[key]
            del self.timestamps[key]
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache.clear()
        self.timestamps.clear()
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        
        current_time = time.time()
        expired_count = sum(
            1 for timestamp in self.timestamps.values()
            if current_time - timestamp > self.ttl_seconds
        )
        
        return {
            'total_items': len(self.cache),
            'max_size': self.max_size,
            'expired_items': expired_count,
            'cache_usage_ratio': len(self.cache) / self.max_size,
            'average_age_seconds': (
                sum(current_time - ts for ts in self.timestamps.values()) / len(self.timestamps)
                if self.timestamps else 0
            )
        }

class CachedFeatureStore:
    """ìºì‹œ ê¸°ëŠ¥ì´ ìˆëŠ” í”¼ì²˜ ìŠ¤í† ì–´"""
    
    def __init__(self, storage: OptimizedStorage, 
                 cache_size: int = 1000, cache_ttl: int = 3600):
        self.storage = storage
        self.cache = LRUCache(max_size=cache_size, ttl_seconds=cache_ttl)
        self.cache_hits = 0
        self.cache_misses = 0
    
    def get_feature_data(self, feature_group: str, feature_name: str,
                        columns: Optional[List[str]] = None) -> pd.DataFrame:
        """ìºì‹œë¥¼ í™œìš©í•œ í”¼ì²˜ ë°ì´í„° ì¡°íšŒ"""
        
        # ìºì‹œ í‚¤ ìƒì„±
        cache_key = self._generate_cache_key(feature_group, feature_name, columns)
        
        # ìºì‹œì—ì„œ ì¡°íšŒ
        cached_data = self.cache.get(cache_key)
        
        if cached_data is not None:
            self.cache_hits += 1
            return cached_data
        
        # ìºì‹œ ë¯¸ìŠ¤ - ìŠ¤í† ë¦¬ì§€ì—ì„œ ë¡œë“œ
        self.cache_misses += 1
        data = self.storage.load_feature_data(
            feature_group=feature_group,
            feature_name=feature_name,
            columns=columns
        )
        
        # ìºì‹œì— ì €ì¥ (ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
        if not data.empty:
            self.cache.set(cache_key, data)
        
        return data
    
    def invalidate_cache(self, feature_group: str, feature_name: str):
        """íŠ¹ì • í”¼ì²˜ì˜ ìºì‹œ ë¬´íš¨í™”"""
        
        # í•´ë‹¹ í”¼ì²˜ì™€ ê´€ë ¨ëœ ëª¨ë“  ìºì‹œ í‚¤ ì°¾ê¸°
        keys_to_remove = []
        for key in self.cache.cache.keys():
            if f"{feature_group}:{feature_name}" in key:
                keys_to_remove.append(key)
        
        # ìºì‹œì—ì„œ ì œê±°
        for key in keys_to_remove:
            self.cache.delete(key)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0
        
        cache_stats = self.cache.get_stats()
        cache_stats.update({
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'total_requests': total_requests
        })
        
        return cache_stats
    
    def _generate_cache_key(self, feature_group: str, feature_name: str,
                           columns: Optional[List[str]]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        
        key_parts = [feature_group, feature_name]
        
        if columns:
            key_parts.append(','.join(sorted(columns)))
        
        return ':'.join(key_parts)
```

### ë””ìŠ¤í¬ ìºì‹œ êµ¬í˜„

```python
import pickle
import hashlib
from pathlib import Path

class DiskCache:
    """ë””ìŠ¤í¬ ê¸°ë°˜ ìºì‹œ"""
    
    def __init__(self, cache_dir: str = "cache/disk", max_size_mb: int = 1000):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.max_size_mb = max_size_mb
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        
        cache_file = self._get_cache_file(key)
        
        if not cache_file.exists():
            return None
        
        try:
            with open(cache_file, 'rb') as f:
                return pickle.load(f)
        except Exception:
            # ì†ìƒëœ ìºì‹œ íŒŒì¼ ì œê±°
            cache_file.unlink(missing_ok=True)
            return None
    
    def set(self, key: str, value: Any):
        """ìºì‹œì— ê°’ ì €ì¥"""
        
        cache_file = self._get_cache_file(key)
        
        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(value, f)
            
            # ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬
            self._cleanup_if_needed()
            
        except Exception as e:
            print(f"ë””ìŠ¤í¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def delete(self, key: str):
        """ìºì‹œì—ì„œ ê°’ ì œê±°"""
        
        cache_file = self._get_cache_file(key)
        cache_file.unlink(missing_ok=True)
    
    def clear(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        
        for cache_file in self.cache_dir.glob("*.cache"):
            cache_file.unlink()
    
    def get_cache_size_mb(self) -> float:
        """ìºì‹œ í¬ê¸° ì¡°íšŒ (MB)"""
        
        total_size = sum(
            f.stat().st_size for f in self.cache_dir.glob("*.cache")
        )
        return total_size / 1024 / 1024
    
    def _get_cache_file(self, key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±"""
        
        # í‚¤ë¥¼ í•´ì‹œí•˜ì—¬ íŒŒì¼ëª… ìƒì„±
        key_hash = hashlib.md5(key.encode()).hexdigest()
        return self.cache_dir / f"{key_hash}.cache"
    
    def _cleanup_if_needed(self):
        """í•„ìš”ì‹œ ìºì‹œ ì •ë¦¬"""
        
        current_size = self.get_cache_size_mb()
        
        if current_size > self.max_size_mb:
            # ì˜¤ë˜ëœ íŒŒì¼ë¶€í„° ì œê±°
            cache_files = list(self.cache_dir.glob("*.cache"))
            cache_files.sort(key=lambda f: f.stat().st_mtime)
            
            target_size = self.max_size_mb * 0.8  # 80%ê¹Œì§€ ì •ë¦¬
            
            for cache_file in cache_files:
                cache_file.unlink()
                current_size = self.get_cache_size_mb()
                
                if current_size <= target_size:
                    break
```

---

## ğŸ”§ 2.5.4 ì„±ëŠ¥ ìµœì í™”

### í†µí•© í”¼ì²˜ ìŠ¤í† ì–´ êµ¬í˜„

```python
class SimpleFeatureStore:
    """í†µí•© í”¼ì²˜ ìŠ¤í† ì–´"""
    
    def __init__(self, base_path: str, 
                 memory_cache_size: int = 1000,
                 disk_cache_size_mb: int = 1000):
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.storage = OptimizedStorage(base_path)
        self.cached_store = CachedFeatureStore(
            self.storage, 
            cache_size=memory_cache_size
        )
        self.disk_cache = DiskCache(
            cache_dir=f"{base_path}/cache/disk",
            max_size_mb=disk_cache_size_mb
        )
        
        # API ì„œë²„
        self.api = SimpleFeatureStoreAPI(self.storage)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.performance_metrics = {
            'total_requests': 0,
            'avg_response_time': 0,
            'error_count': 0
        }
    
    def get_features(self, feature_names: List[str],
                    entity_ids: Optional[List[str]] = None,
                    use_cache: bool = True) -> Dict[str, pd.DataFrame]:
        """ë‹¤ì¤‘ í”¼ì²˜ ì¡°íšŒ (ìµœì í™”ëœ ë²„ì „)"""
        
        start_time = time.time()
        results = {}
        
        try:
            for feature_name in feature_names:
                feature_group = self._infer_feature_group(feature_name)
                
                if use_cache:
                    # ìºì‹œ ìš°ì„  ì¡°íšŒ
                    data = self.cached_store.get_feature_data(
                        feature_group, feature_name
                    )
                else:
                    # ì§ì ‘ ìŠ¤í† ë¦¬ì§€ ì¡°íšŒ
                    data = self.storage.load_feature_data(
                        feature_group, feature_name
                    )
                
                # ì—”í‹°í‹° í•„í„°ë§
                if entity_ids and 'entity_id' in data.columns:
                    data = data[data['entity_id'].isin(entity_ids)]
                
                results[feature_name] = data
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            response_time = time.time() - start_time
            self._update_metrics(response_time, success=True)
            
        except Exception as e:
            self._update_metrics(time.time() - start_time, success=False)
            raise
        
        return results
    
    def save_features(self, feature_group: str, 
                     features_data: Dict[str, pd.DataFrame]) -> Dict[str, str]:
        """ë‹¤ì¤‘ í”¼ì²˜ ì €ì¥"""
        
        results = {}
        
        for feature_name, data in features_data.items():
            try:
                # ìŠ¤í† ë¦¬ì§€ì— ì €ì¥
                file_path = self.storage.save_feature_data(
                    data, feature_group, feature_name
                )
                
                # ìºì‹œ ë¬´íš¨í™”
                self.cached_store.invalidate_cache(feature_group, feature_name)
                
                results[feature_name] = file_path
                
            except Exception as e:
                results[feature_name] = f"Error: {e}"
        
        return results
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        cache_stats = self.cached_store.get_cache_stats()
        
        return {
            'performance_metrics': self.performance_metrics,
            'memory_cache_stats': cache_stats,
            'disk_cache_size_mb': self.disk_cache.get_cache_size_mb(),
            'storage_info': self._get_storage_summary()
        }
    
    def start_api_server(self, host: str = "0.0.0.0", port: int = 8001):
        """API ì„œë²„ ì‹œì‘"""
        self.api.run(host=host, port=port)
    
    def _infer_feature_group(self, feature_name: str) -> str:
        """í”¼ì²˜ ê·¸ë£¹ ì¶”ì •"""
        
        if 'user' in feature_name.lower():
            return 'user'
        elif 'movie' in feature_name.lower() or 'content' in feature_name.lower():
            return 'content'
        elif 'interaction' in feature_name.lower():
            return 'interaction'
        else:
            return 'misc'
    
    def _update_metrics(self, response_time: float, success: bool):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        
        self.performance_metrics['total_requests'] += 1
        
        if success:
            # ì´ë™ í‰ê· ìœ¼ë¡œ ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            total_requests = self.performance_metrics['total_requests']
            current_avg = self.performance_metrics['avg_response_time']
            new_avg = (current_avg * (total_requests - 1) + response_time) / total_requests
            self.performance_metrics['avg_response_time'] = new_avg
        else:
            self.performance_metrics['error_count'] += 1
    
    def _get_storage_summary(self) -> Dict[str, Any]:
        """ìŠ¤í† ë¦¬ì§€ ìš”ì•½ ì •ë³´"""
        
        features_path = self.storage.base_path / "features"
        
        summary = {
            'total_feature_groups': 0,
            'total_features': 0,
            'total_size_mb': 0
        }
        
        if features_path.exists():
            feature_groups = [d for d in features_path.iterdir() if d.is_dir()]
            summary['total_feature_groups'] = len(feature_groups)
            
            for group_dir in feature_groups:
                feature_files = list(group_dir.glob("*.parquet"))
                summary['total_features'] += len(feature_files)
                
                for file_path in feature_files:
                    summary['total_size_mb'] += file_path.stat().st_size / 1024 / 1024
        
        return summary
```

---

## âœ… ì™„ë£Œ ê¸°ì¤€

### ê¸°ëŠ¥ì  ì™„ë£Œ ê¸°ì¤€
- [ ] íŒŒì¼ ê¸°ë°˜ ìŠ¤í† ë¦¬ì§€ ì‹œìŠ¤í…œ êµ¬í˜„ ì™„ë£Œ
- [ ] RESTful API ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- [ ] í´ë¼ì´ì–¸íŠ¸ SDK ê°œë°œ
- [ ] ë©”ëª¨ë¦¬ ë° ë””ìŠ¤í¬ ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ë°°ì¹˜ í”¼ì²˜ ì¡°íšŒ ê¸°ëŠ¥ êµ¬í˜„

### ê¸°ìˆ ì  ì™„ë£Œ ê¸°ì¤€
- [ ] Parquet íŒŒì¼ ìµœì í™” (ì••ì¶•ë¥  50% ì´ìƒ)
- [ ] API ì‘ë‹µ ì‹œê°„ 100ms ì´í•˜ (ìºì‹œ ì ì¤‘ ì‹œ)
- [ ] ìºì‹œ ì ì¤‘ë¥  80% ì´ìƒ
- [ ] ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ (100 RPS ì´ìƒ)
- [ ] ìŠ¤í† ë¦¬ì§€ ê³µê°„ íš¨ìœ¨ì„± ê²€ì¦

### í’ˆì§ˆ ì™„ë£Œ ê¸°ì¤€
- [ ] API ë¬¸ì„œí™” (OpenAPI/Swagger) ì™„ë£Œ
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ë° í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„±
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„
- [ ] ì‚¬ìš©ì ê°€ì´ë“œ ë° ì˜ˆì œ ì½”ë“œ ì‘ì„±

---

## ğŸš€ ë‹¤ìŒ ë‹¨ê³„

ì™„ë£Œ í›„ [2.6 í”¼ì²˜ ë²„ì „ ê´€ë¦¬](./2.6-feature-version-control-guide.md)ë¡œ ì§„í–‰í•˜ì—¬ í”¼ì²˜ì˜ ì§„í™”ì™€ í˜¸í™˜ì„±ì„ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [Apache Parquet Documentation](https://parquet.apache.org/docs/)
- [FastAPI User Guide](https://fastapi.tiangolo.com/)
- [Caching Strategies](https://aws.amazon.com/caching/)
- [Feature Store Design Patterns](https://www.featurestore.org/what-is-a-feature-store)
